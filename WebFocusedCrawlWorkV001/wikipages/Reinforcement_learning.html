<!DOCTYPE html>
<html class="client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-enabled vector-feature-main-menu-pinned-disabled vector-feature-limited-width-enabled vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>Reinforcement learning - Wikipedia</title>
<script>document.documentElement.className="client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-enabled vector-feature-main-menu-pinned-disabled vector-feature-limited-width-enabled vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled";(function(){var cookie=document.cookie.match(/(?:^|; )enwikimwclientprefs=([^;]+)/);if(cookie){var featureName=cookie[1];document.documentElement.className=document.documentElement.className.replace(featureName+'-enabled',featureName+'-disabled');}}());RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"a57794b8-8645-44f0-9397-66c99ee4b7ec","wgCSPNonce":false,
"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Reinforcement_learning","wgTitle":"Reinforcement learning","wgCurRevisionId":1161565586,"wgRevisionId":1161565586,"wgArticleId":66294,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 maint: url-status","Articles with short description","Short description matches Wikidata","Wikipedia articles needing clarification from January 2020","Articles needing additional references from October 2022","All articles needing additional references","Webarchive template wayback links","Reinforcement learning","Markov models","Belief revision"],"wgPageViewLanguage":"en","wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Reinforcement_learning","wgRelevantArticleId":66294,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{
"tags":{"status":{"levels":1}}},"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":true,"watchlist":true,"tagline":false,"nearby":true},"wgWMESchemaEditAttemptStepOversample":false,"wgWMEPageLength":50000,"wgNoticeProject":"wikipedia","wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsFlags":10,"wgULSCurrentAutonym":"English","wgEditSubmitButtonLabelPublish":true,"wgCentralAuthMobileDomain":false,"wgULSPosition":"interlanguage","wgULSisCompactLinksEnabled":true,"wgULSisLanguageSelectorEmpty":false,"wgWikibaseItemId":"Q830687","GEHomepageSuggestedEditsEnableTopics":true,"wgGETopicsMatchModeEnabled":false,"wgGEStructuredTaskRejectionReasonTextInputEnabled":false,"wgGELevelingUpEnabledForUser":false};RLSTATE={"skins.vector.user.styles":"ready","ext.globalCssJs.user.styles":"ready","site.styles":"ready","user.styles":"ready","skins.vector.user":"ready","ext.globalCssJs.user":
"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","codex-search-styles":"ready","skins.vector.styles":"ready","skins.vector.icons":"ready","jquery.tablesorter.styles":"ready","jquery.makeCollapsible.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.wikimediaBadges":"ready","ext.uls.interlanguage":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","mediawiki.page.media","ext.scribunto.logs","site","mediawiki.page.ready","jquery.tablesorter","jquery.makeCollapsible","mediawiki.toc","skins.vector.js","mmv.head","mmv.bootstrap.autostart","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.extra-toolbar-buttons",
"ext.gadget.switcher","ext.centralauth.centralautologin","ext.popups","ext.echo.centralauth","ext.uls.compactlinks","ext.uls.interface","ext.cx.uls.quick.actions","wikibase.client.vector-2022","ext.growthExperiments.SuggestedEditSession"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@12s5i",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=codex-search-styles%7Cext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cjquery.tablesorter.styles%7Cskins.vector.icons%2Cstyles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector-2022">
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector-2022"></script>
<meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector-2022">
<meta name="generator" content="MediaWiki 1.41.0-wmf.16">
<meta name="referrer" content="origin">
<meta name="referrer" content="origin-when-crossorigin">
<meta name="referrer" content="origin-when-cross-origin">
<meta name="robots" content="max-image-preview:standard">
<meta name="format-detection" content="telephone=no">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/1200px-Kernel_Machine.svg.png">
<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="546">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/800px-Kernel_Machine.svg.png">
<meta property="og:image:width" content="800">
<meta property="og:image:height" content="364">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/640px-Kernel_Machine.svg.png">
<meta property="og:image:width" content="640">
<meta property="og:image:height" content="291">
<meta name="viewport" content="width=1000">
<meta property="og:title" content="Reinforcement learning - Wikipedia">
<meta property="og:type" content="website">
<link rel="preconnect" href="//upload.wikimedia.org">
<link rel="alternate" media="only screen and (max-width: 720px)" href="//en.m.wikipedia.org/wiki/Reinforcement_learning">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Reinforcement_learning&amp;action=edit">
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png">
<link rel="icon" href="/static/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd">
<link rel="canonical" href="https://en.wikipedia.org/wiki/Reinforcement_learning">
<link rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom">
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<link rel="dns-prefetch" href="//login.wikimedia.org">
</head>
<body class="skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Reinforcement_learning rootpage-Reinforcement_learning skin-vector-2022 action-view"><a class="mw-jump-link" href="#bodyContent">Jump to content</a>
<div class="vector-header-container">
	<header class="vector-header mw-header">
		<div class="vector-header-start">
			<nav class="vector-main-menu-landmark" aria-label="Site" role="navigation">
				
<div id="vector-main-menu-dropdown" class="vector-dropdown vector-main-menu-dropdown vector-button-flush-left vector-button-flush-right"  >
	<input type="checkbox"
		id="vector-main-menu-dropdown-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-vector-main-menu-dropdown"
		class="vector-dropdown-checkbox "
		
		aria-label="Main menu"
		
	/>
	<label
		id="vector-main-menu-dropdown-label"
		for="vector-main-menu-dropdown-checkbox"
		class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only "
		aria-hidden="true"
		
	>
		<span class="vector-icon mw-ui-icon-menu mw-ui-icon-wikimedia-menu"></span>


		<span class="vector-dropdown-label-text">Main menu</span>
	</label>
	<div class="vector-dropdown-content">


				<div id="vector-main-menu-unpinned-container" class="vector-unpinned-container">
		
<div id="vector-main-menu" class="vector-main-menu vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-main-menu-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="main-menu-pinned"
	data-pinnable-element-id="vector-main-menu"
	data-pinned-container-id="vector-main-menu-pinned-container"
	data-unpinned-container-id="vector-main-menu-unpinned-container"
>
	<div class="vector-pinnable-header-label">Main menu</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-main-menu.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-main-menu.unpin">hide</button>
</div>

	
<div id="p-navigation" class="vector-menu mw-portlet mw-portlet-navigation"  >
	<div class="vector-menu-heading">
		Navigation
	</div>
	
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="n-mainpage-description" class="mw-list-item"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"><span>Main page</span></a></li><li id="n-contents" class="mw-list-item"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia"><span>Contents</span></a></li><li id="n-currentevents" class="mw-list-item"><a href="/wiki/Portal:Current_events" title="Articles related to current events"><span>Current events</span></a></li><li id="n-randompage" class="mw-list-item"><a href="/wiki/Special:Random" title="Visit a randomly selected article [x]" accesskey="x"><span>Random article</span></a></li><li id="n-aboutsite" class="mw-list-item"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works"><span>About Wikipedia</span></a></li><li id="n-contactpage" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia"><span>Contact us</span></a></li><li id="n-sitesupport" class="mw-list-item"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us by donating to the Wikimedia Foundation"><span>Donate</span></a></li></ul>
		
	</div>
</div>

	
	
<div id="p-interaction" class="vector-menu mw-portlet mw-portlet-interaction"  >
	<div class="vector-menu-heading">
		Contribute
	</div>
	
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="n-help" class="mw-list-item"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia"><span>Help</span></a></li><li id="n-introduction" class="mw-list-item"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia"><span>Learn to edit</span></a></li><li id="n-portal" class="mw-list-item"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors"><span>Community portal</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]" accesskey="r"><span>Recent changes</span></a></li><li id="n-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_upload_wizard" title="Add images or other media for use on Wikipedia"><span>Upload file</span></a></li></ul>
		
	</div>
</div>

	
<div class="vector-main-menu-action vector-main-menu-action-lang-alert">
	<div class="vector-main-menu-action-item">
		<div class="vector-main-menu-action-heading vector-menu-heading">Languages</div>
		<div class="vector-main-menu-action-content vector-menu-content">
			<div class="mw-message-box cdx-message cdx-message--block mw-message-box-notice cdx-message--notice vector-language-sidebar-alert"><span class="cdx-message__icon"></span><div class="cdx-message__content">Language links are at the top of the page across from the title.</div></div>
		</div>
	</div>
</div>

</div>

				</div>

	</div>
</div>
		</nav>
			
<a href="/wiki/Main_Page" class="mw-logo">
	<img class="mw-logo-icon" src="/static/images/icons/wikipedia.png" alt=""
		aria-hidden="true" height="50" width="50">
	<span class="mw-logo-container">
		<img class="mw-logo-wordmark" alt="Wikipedia"
			src="/static/images/mobile/copyright/wikipedia-wordmark-en.svg" style="width: 7.5em; height: 1.125em;">
		<img class="mw-logo-tagline"
			alt="The Free Encyclopedia"
			src="/static/images/mobile/copyright/wikipedia-tagline-en.svg" width="117" height="13" style="width: 7.3125em; height: 0.8125em;">
	</span>
</a>

		</div>
		<div class="vector-header-end">
			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-collapses vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<a href="/wiki/Special:Search"
	
		id=""
		class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only search-toggle"
		title="Search Wikipedia [f]"accesskey="f"><span class="vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search"></span>

<span>Search</span>
	</a>
	
	<div class="vector-typeahead-search-container">
		<div class="cdx-typeahead-search cdx-typeahead-search--show-thumbnail cdx-typeahead-search--auto-expand-width">
			<form action="/w/index.php" id="searchform" class="cdx-search-input cdx-search-input--has-end-button">
				<div id="simpleSearch" class="cdx-search-input__input-wrapper"  data-search-loc="header-moved">
					<div class="cdx-text-input cdx-text-input--has-start-icon">
						<input
							class="cdx-text-input__input"
							 type="search" name="search" placeholder="Search Wikipedia" aria-label="Search Wikipedia" autocapitalize="sentences" title="Search Wikipedia [f]" accesskey="f" id="searchInput"
							
						>
						<span class="cdx-text-input__icon cdx-text-input__start-icon"></span>
					</div>
					<input type="hidden" name="title" value="Special:Search">
				</div>
				<button class="cdx-button cdx-search-input__end-button">Search</button>
			</form>
		</div>
	</div>
</div>

			<nav class="vector-user-links" aria-label="Personal tools" role="navigation" >
	
<div id="p-vector-user-menu-overflow" class="vector-menu mw-portlet mw-portlet-vector-user-menu-overflow"  >
	
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="pt-createaccount-2" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Reinforcement+learning" title="You are encouraged to create an account and log in; however, it is not mandatory"><span>Create account</span></a></li><li id="pt-login-2" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Reinforcement+learning" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o"><span>Log in</span></a></li></ul>
		
	</div>
</div>

	
<div id="vector-user-links-dropdown" class="vector-dropdown vector-user-menu vector-button-flush-right vector-user-menu-logged-out"  title="Log in and more options" >
	<input type="checkbox"
		id="vector-user-links-dropdown-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-vector-user-links-dropdown"
		class="vector-dropdown-checkbox "
		
		aria-label="Personal tools"
		
	/>
	<label
		id="vector-user-links-dropdown-label"
		for="vector-user-links-dropdown-checkbox"
		class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only "
		aria-hidden="true"
		
	>
		<span class="vector-icon mw-ui-icon-ellipsis mw-ui-icon-wikimedia-ellipsis"></span>


		<span class="vector-dropdown-label-text">Personal tools</span>
	</label>
	<div class="vector-dropdown-content">


		
<div id="p-personal" class="vector-menu mw-portlet mw-portlet-personal user-links-collapsible-item"  title="User menu" >
	
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="pt-createaccount" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Reinforcement+learning" title="You are encouraged to create an account and log in; however, it is not mandatory"><span class="vector-icon mw-ui-icon-userAdd mw-ui-icon-wikimedia-userAdd"></span> <span>Create account</span></a></li><li id="pt-login" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Reinforcement+learning" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o"><span class="vector-icon mw-ui-icon-logIn mw-ui-icon-wikimedia-logIn"></span> <span>Log in</span></a></li></ul>
		
	</div>
</div>

<div id="p-user-menu-anon-editor" class="vector-menu mw-portlet mw-portlet-user-menu-anon-editor"  >
	<div class="vector-menu-heading">Pages for logged out editors <a href="/wiki/Help:Introduction" aria-label="Learn more about editing"><span>learn more</span></a></div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="pt-anoncontribs" class="mw-list-item"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y"><span>Contributions</span></a></li><li id="pt-anontalk" class="mw-list-item"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n"><span>Talk</span></a></li></ul>
		
	</div>
</div>

	
	</div>
</div>
</nav>

		</div>
	</header>
</div>
<div class="mw-page-container">
	<div class="mw-page-container-inner">
		<div class="vector-main-menu-container ">
			<div id="mw-navigation">
				<nav id="mw-panel" class="vector-main-menu-landmark" aria-label="Site" role="navigation">
					<div id="vector-main-menu-pinned-container" class="vector-pinned-container">
				
					</div>
		</nav>
			</div>
		</div>
		<div class="vector-sitenotice-container">
			<div id="siteNotice"><!-- CentralNotice --></div>
		</div>
		<input type="checkbox" id="vector-toc-collapsed-checkbox" class="vector-menu-checkbox">
		<nav id="mw-panel-toc" role="navigation" aria-label="Contents" data-event-name="ui.sidebar-toc" class="mw-table-of-contents-container vector-toc-landmark vector-sticky-pinned-container">
			<div id="vector-toc-pinned-container" class="vector-pinned-container">
					<div id="vector-toc" class="vector-toc vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-toc-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="toc-pinned"
	data-pinnable-element-id="vector-toc"
	
	
>
	<h2 class="vector-pinnable-header-label">Contents</h2>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-toc.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-toc.unpin">hide</button>
</div>


	<ul class="vector-toc-contents" id="mw-panel-toc-list">
		<li id="toc-mw-content-text"
			class="vector-toc-list-item vector-toc-level-1">
			<a href="#" class="vector-toc-link">
				<div class="vector-toc-text">(Top)</div>
			</a>
		</li>
		<li id="toc-Introduction"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Introduction">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">1</span>Introduction</div>
		</a>
		
		<ul id="toc-Introduction-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Exploration"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Exploration">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">2</span>Exploration</div>
		</a>
		
		<ul id="toc-Exploration-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Algorithms_for_control_learning"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Algorithms_for_control_learning">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">3</span>Algorithms for control learning</div>
		</a>
		
			<button aria-controls="toc-Algorithms_for_control_learning-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon vector-icon--x-small mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Algorithms for control learning subsection</span>
			</button>
		
		<ul id="toc-Algorithms_for_control_learning-sublist" class="vector-toc-list">
			<li id="toc-Criterion_of_optimality"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Criterion_of_optimality">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">3.1</span>Criterion of optimality</div>
			</a>
			
			<ul id="toc-Criterion_of_optimality-sublist" class="vector-toc-list">
				<li id="toc-Policy"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Policy">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">3.1.1</span>Policy</div>
			</a>
			
			<ul id="toc-Policy-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-State-value_function"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#State-value_function">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">3.1.2</span>State-value function</div>
			</a>
			
			<ul id="toc-State-value_function-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
		</li>
		<li id="toc-Brute_force"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Brute_force">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">3.2</span>Brute force</div>
			</a>
			
			<ul id="toc-Brute_force-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Value_function"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Value_function">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">3.3</span>Value function</div>
			</a>
			
			<ul id="toc-Value_function-sublist" class="vector-toc-list">
				<li id="toc-Monte_Carlo_methods"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Monte_Carlo_methods">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">3.3.1</span>Monte Carlo methods</div>
			</a>
			
			<ul id="toc-Monte_Carlo_methods-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Temporal_difference_methods"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Temporal_difference_methods">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">3.3.2</span>Temporal difference methods</div>
			</a>
			
			<ul id="toc-Temporal_difference_methods-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Function_approximation_methods"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Function_approximation_methods">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">3.3.3</span>Function approximation methods</div>
			</a>
			
			<ul id="toc-Function_approximation_methods-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
		</li>
		<li id="toc-Direct_policy_search"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Direct_policy_search">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">3.4</span>Direct policy search</div>
			</a>
			
			<ul id="toc-Direct_policy_search-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Model-based_algorithms"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Model-based_algorithms">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">3.5</span>Model-based algorithms</div>
			</a>
			
			<ul id="toc-Model-based_algorithms-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Theory"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Theory">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">4</span>Theory</div>
		</a>
		
		<ul id="toc-Theory-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Research"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Research">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">5</span>Research</div>
		</a>
		
		<ul id="toc-Research-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Comparison_of_reinforcement_learning_algorithms"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Comparison_of_reinforcement_learning_algorithms">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">6</span>Comparison of reinforcement learning algorithms</div>
		</a>
		
			<button aria-controls="toc-Comparison_of_reinforcement_learning_algorithms-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon vector-icon--x-small mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Comparison of reinforcement learning algorithms subsection</span>
			</button>
		
		<ul id="toc-Comparison_of_reinforcement_learning_algorithms-sublist" class="vector-toc-list">
			<li id="toc-Associative_reinforcement_learning"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Associative_reinforcement_learning">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">6.1</span>Associative reinforcement learning</div>
			</a>
			
			<ul id="toc-Associative_reinforcement_learning-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Deep_reinforcement_learning"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Deep_reinforcement_learning">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">6.2</span>Deep reinforcement learning</div>
			</a>
			
			<ul id="toc-Deep_reinforcement_learning-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Adversarial_deep_reinforcement_learning"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Adversarial_deep_reinforcement_learning">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">6.3</span>Adversarial deep reinforcement learning</div>
			</a>
			
			<ul id="toc-Adversarial_deep_reinforcement_learning-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Fuzzy_reinforcement_learning"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Fuzzy_reinforcement_learning">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">6.4</span>Fuzzy reinforcement learning</div>
			</a>
			
			<ul id="toc-Fuzzy_reinforcement_learning-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Inverse_reinforcement_learning"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Inverse_reinforcement_learning">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">6.5</span>Inverse reinforcement learning</div>
			</a>
			
			<ul id="toc-Inverse_reinforcement_learning-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Safe_reinforcement_learning"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Safe_reinforcement_learning">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">6.6</span>Safe reinforcement learning</div>
			</a>
			
			<ul id="toc-Safe_reinforcement_learning-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-See_also"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#See_also">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">7</span>See also</div>
		</a>
		
		<ul id="toc-See_also-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-References"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#References">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">8</span>References</div>
		</a>
		
		<ul id="toc-References-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Sources"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Sources">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">9</span>Sources</div>
		</a>
		
		<ul id="toc-Sources-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Further_reading"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Further_reading">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">10</span>Further reading</div>
		</a>
		
		<ul id="toc-Further_reading-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-External_links"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#External_links">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">11</span>External links</div>
		</a>
		
		<ul id="toc-External_links-sublist" class="vector-toc-list">
		</ul>
	</li>
</ul>
</div>

			</div>
		</nav>
		<div class="mw-content-container">
			<main id="content" class="mw-body" role="main">
				<header class="mw-body-header vector-page-titlebar">
					<label
						id="vector-toc-collapsed-button"
						class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet vector-button-flush-left cdx-button--icon-only"
						for="vector-toc-collapsed-checkbox"
						role="button"
						aria-controls="vector-toc"
						tabindex="0"
						title="Table of Contents">
						<span class="vector-icon mw-ui-icon-wikimedia-listBullet"></span>
						<span>Toggle the table of contents</span>
					</label>
				
					<nav role="navigation" aria-label="Contents" class="vector-toc-landmark">
						
<div id="vector-page-titlebar-toc" class="vector-dropdown vector-page-titlebar-toc vector-button-flush-left"  >
	<input type="checkbox"
		id="vector-page-titlebar-toc-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-vector-page-titlebar-toc"
		class="vector-dropdown-checkbox "
		
		aria-label="Toggle the table of contents"
		
	/>
	<label
		id="vector-page-titlebar-toc-label"
		for="vector-page-titlebar-toc-checkbox"
		class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only "
		aria-hidden="true"
		
	>
		<span class="vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet"></span>


		<span class="vector-dropdown-label-text">Toggle the table of contents</span>
	</label>
	<div class="vector-dropdown-content">


							<div id="vector-page-titlebar-toc-unpinned-container" class="vector-unpinned-container">
			</div>
		
	</div>
</div>
					</nav>
					<h1 id="firstHeading" class="firstHeading mw-first-heading"><span class="mw-page-title-main">Reinforcement learning</span></h1>
				
							
<div id="p-lang-btn" class="vector-dropdown mw-portlet mw-portlet-lang"  >
	<input type="checkbox"
		id="p-lang-btn-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-lang-btn"
		class="vector-dropdown-checkbox mw-interlanguage-selector"
		aria-label="Go to an article in another language. Available in 35 languages"
		
		
	/>
	<label
		id="p-lang-btn-label"
		for="p-lang-btn-checkbox"
		class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive mw-portlet-lang-heading-35"
		aria-hidden="true"
		
	>
		<span class="vector-icon mw-ui-icon-language-progressive mw-ui-icon-wikimedia-language-progressive"></span>


		<span class="vector-dropdown-label-text">35 languages</span>
	</label>
	<div class="vector-dropdown-content">

		<div class="vector-menu-content">
			
			<ul class="vector-menu-content-list"><li class="interlanguage-link interwiki-ar mw-list-item"><a href="https://ar.wikipedia.org/wiki/%D8%AA%D8%B9%D9%84%D9%8A%D9%85_%D9%85%D8%AF%D8%B9%D9%88%D9%85" title="تعليم مدعوم – Arabic" lang="ar" hreflang="ar" class="interlanguage-link-target"><span>العربية</span></a></li><li class="interlanguage-link interwiki-bn mw-list-item"><a href="https://bn.wikipedia.org/wiki/%E0%A6%AC%E0%A6%B2%E0%A6%AC%E0%A6%B0%E0%A7%8D%E0%A6%A7%E0%A6%A8%E0%A6%AE%E0%A7%82%E0%A6%B2%E0%A6%95_%E0%A6%B6%E0%A6%BF%E0%A6%96%E0%A6%A8" title="বলবর্ধনমূলক শিখন – Bangla" lang="bn" hreflang="bn" class="interlanguage-link-target"><span>বাংলা</span></a></li><li class="interlanguage-link interwiki-bg mw-list-item"><a href="https://bg.wikipedia.org/wiki/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%D1%81_%D1%83%D1%82%D0%B2%D1%8A%D1%80%D0%B6%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5" title="Обучение с утвърждение – Bulgarian" lang="bg" hreflang="bg" class="interlanguage-link-target"><span>Български</span></a></li><li class="interlanguage-link interwiki-bs mw-list-item"><a href="https://bs.wikipedia.org/wiki/Podr%C5%BEano_u%C4%8Denje" title="Podržano učenje – Bosnian" lang="bs" hreflang="bs" class="interlanguage-link-target"><span>Bosanski</span></a></li><li class="interlanguage-link interwiki-ca mw-list-item"><a href="https://ca.wikipedia.org/wiki/Aprenentatge_per_refor%C3%A7" title="Aprenentatge per reforç – Catalan" lang="ca" hreflang="ca" class="interlanguage-link-target"><span>Català</span></a></li><li class="interlanguage-link interwiki-cs mw-list-item"><a href="https://cs.wikipedia.org/wiki/Zp%C4%9Btnovazebn%C3%AD_u%C4%8Den%C3%AD" title="Zpětnovazební učení – Czech" lang="cs" hreflang="cs" class="interlanguage-link-target"><span>Čeština</span></a></li><li class="interlanguage-link interwiki-de mw-list-item"><a href="https://de.wikipedia.org/wiki/Best%C3%A4rkendes_Lernen" title="Bestärkendes Lernen – German" lang="de" hreflang="de" class="interlanguage-link-target"><span>Deutsch</span></a></li><li class="interlanguage-link interwiki-et mw-list-item"><a href="https://et.wikipedia.org/wiki/Stiimul%C3%B5pe" title="Stiimulõpe – Estonian" lang="et" hreflang="et" class="interlanguage-link-target"><span>Eesti</span></a></li><li class="interlanguage-link interwiki-el mw-list-item"><a href="https://el.wikipedia.org/wiki/%CE%95%CE%BD%CE%B9%CF%83%CF%87%CF%85%CF%84%CE%B9%CE%BA%CE%AE_%CE%BC%CE%AC%CE%B8%CE%B7%CF%83%CE%B7" title="Ενισχυτική μάθηση – Greek" lang="el" hreflang="el" class="interlanguage-link-target"><span>Ελληνικά</span></a></li><li class="interlanguage-link interwiki-es mw-list-item"><a href="https://es.wikipedia.org/wiki/Aprendizaje_por_refuerzo" title="Aprendizaje por refuerzo – Spanish" lang="es" hreflang="es" class="interlanguage-link-target"><span>Español</span></a></li><li class="interlanguage-link interwiki-fa mw-list-item"><a href="https://fa.wikipedia.org/wiki/%DB%8C%D8%A7%D8%AF%DA%AF%DB%8C%D8%B1%DB%8C_%D8%AA%D9%82%D9%88%DB%8C%D8%AA%DB%8C" title="یادگیری تقویتی – Persian" lang="fa" hreflang="fa" class="interlanguage-link-target"><span>فارسی</span></a></li><li class="interlanguage-link interwiki-fr mw-list-item"><a href="https://fr.wikipedia.org/wiki/Apprentissage_par_renforcement" title="Apprentissage par renforcement – French" lang="fr" hreflang="fr" class="interlanguage-link-target"><span>Français</span></a></li><li class="interlanguage-link interwiki-ko mw-list-item"><a href="https://ko.wikipedia.org/wiki/%EA%B0%95%ED%99%94_%ED%95%99%EC%8A%B5" title="강화 학습 – Korean" lang="ko" hreflang="ko" class="interlanguage-link-target"><span>한국어</span></a></li><li class="interlanguage-link interwiki-hy mw-list-item"><a href="https://hy.wikipedia.org/wiki/%D4%B1%D5%B4%D6%80%D5%A1%D5%BA%D5%B6%D5%A4%D5%B4%D5%A1%D5%B6_%D5%B8%D6%82%D5%BD%D5%B8%D6%82%D6%81%D5%B8%D6%82%D5%B4" title="Ամրապնդման ուսուցում – Armenian" lang="hy" hreflang="hy" class="interlanguage-link-target"><span>Հայերեն</span></a></li><li class="interlanguage-link interwiki-it mw-list-item"><a href="https://it.wikipedia.org/wiki/Apprendimento_per_rinforzo" title="Apprendimento per rinforzo – Italian" lang="it" hreflang="it" class="interlanguage-link-target"><span>Italiano</span></a></li><li class="interlanguage-link interwiki-he mw-list-item"><a href="https://he.wikipedia.org/wiki/%D7%9C%D7%9E%D7%99%D7%93%D7%AA_%D7%97%D7%99%D7%96%D7%95%D7%A7" title="למידת חיזוק – Hebrew" lang="he" hreflang="he" class="interlanguage-link-target"><span>עברית</span></a></li><li class="interlanguage-link interwiki-ms mw-list-item"><a href="https://ms.wikipedia.org/wiki/Pembelajaran_pengukuhan" title="Pembelajaran pengukuhan – Malay" lang="ms" hreflang="ms" class="interlanguage-link-target"><span>Bahasa Melayu</span></a></li><li class="interlanguage-link interwiki-nl mw-list-item"><a href="https://nl.wikipedia.org/wiki/Reinforcement_learning" title="Reinforcement learning – Dutch" lang="nl" hreflang="nl" class="interlanguage-link-target"><span>Nederlands</span></a></li><li class="interlanguage-link interwiki-ja mw-list-item"><a href="https://ja.wikipedia.org/wiki/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92" title="強化学習 – Japanese" lang="ja" hreflang="ja" class="interlanguage-link-target"><span>日本語</span></a></li><li class="interlanguage-link interwiki-no mw-list-item"><a href="https://no.wikipedia.org/wiki/Forsterkende_l%C3%A6ring" title="Forsterkende læring – Norwegian Bokmål" lang="nb" hreflang="nb" class="interlanguage-link-target"><span>Norsk bokmål</span></a></li><li class="interlanguage-link interwiki-or mw-list-item"><a href="https://or.wikipedia.org/wiki/%E0%AC%B0%E0%AC%BF%E0%AC%8F%E0%AC%A8%E0%AD%8D%E0%AC%AB%E0%AD%8B%E0%AC%B0%E0%AD%8D%E0%AC%B8%E0%AC%AE%E0%AD%87%E0%AC%A3%E0%AD%8D%E0%AC%9F_%E0%AC%B2%E0%AC%B0%E0%AD%8D%E0%AC%A3%E0%AD%8D%E0%AC%A3%E0%AC%BF%E0%AC%99%E0%AD%8D%E0%AC%97%E0%AD%8D" title="ରିଏନ୍ଫୋର୍ସମେଣ୍ଟ ଲର୍ଣ୍ଣିଙ୍ଗ୍ – Odia" lang="or" hreflang="or" class="interlanguage-link-target"><span>ଓଡ଼ିଆ</span></a></li><li class="interlanguage-link interwiki-pl mw-list-item"><a href="https://pl.wikipedia.org/wiki/Uczenie_przez_wzmacnianie" title="Uczenie przez wzmacnianie – Polish" lang="pl" hreflang="pl" class="interlanguage-link-target"><span>Polski</span></a></li><li class="interlanguage-link interwiki-qu mw-list-item"><a href="https://qu.wikipedia.org/wiki/Kallpanchana_yachay" title="Kallpanchana yachay – Quechua" lang="qu" hreflang="qu" class="interlanguage-link-target"><span>Runa Simi</span></a></li><li class="interlanguage-link interwiki-ru mw-list-item"><a href="https://ru.wikipedia.org/wiki/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%D1%81_%D0%BF%D0%BE%D0%B4%D0%BA%D1%80%D0%B5%D0%BF%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%D0%BC" title="Обучение с подкреплением – Russian" lang="ru" hreflang="ru" class="interlanguage-link-target"><span>Русский</span></a></li><li class="interlanguage-link interwiki-simple mw-list-item"><a href="https://simple.wikipedia.org/wiki/Reinforcement_learning" title="Reinforcement learning – Simple English" lang="en-simple" hreflang="en-simple" class="interlanguage-link-target"><span>Simple English</span></a></li><li class="interlanguage-link interwiki-sl mw-list-item"><a href="https://sl.wikipedia.org/wiki/Spodbujevano_u%C4%8Denje" title="Spodbujevano učenje – Slovenian" lang="sl" hreflang="sl" class="interlanguage-link-target"><span>Slovenščina</span></a></li><li class="interlanguage-link interwiki-ckb mw-list-item"><a href="https://ckb.wikipedia.org/wiki/%D9%81%DB%8E%D8%B1%D8%A8%D9%88%D9%88%D9%86%DB%8C_%D8%A8%DB%95%DA%BE%DB%8E%D8%B2%DA%A9%D8%B1%D8%AF%D9%86%DB%95%D9%88%DB%95" title="فێربوونی بەھێزکردنەوە – Central Kurdish" lang="ckb" hreflang="ckb" class="interlanguage-link-target"><span>کوردی</span></a></li><li class="interlanguage-link interwiki-fi mw-list-item"><a href="https://fi.wikipedia.org/wiki/Vahvistusoppiminen" title="Vahvistusoppiminen – Finnish" lang="fi" hreflang="fi" class="interlanguage-link-target"><span>Suomi</span></a></li><li class="interlanguage-link interwiki-sv mw-list-item"><a href="https://sv.wikipedia.org/wiki/F%C3%B6rst%C3%A4rkningsinl%C3%A4rning" title="Förstärkningsinlärning – Swedish" lang="sv" hreflang="sv" class="interlanguage-link-target"><span>Svenska</span></a></li><li class="interlanguage-link interwiki-tr mw-list-item"><a href="https://tr.wikipedia.org/wiki/Peki%C5%9Ftirmeli_%C3%B6%C4%9Frenme" title="Pekiştirmeli öğrenme – Turkish" lang="tr" hreflang="tr" class="interlanguage-link-target"><span>Türkçe</span></a></li><li class="interlanguage-link interwiki-uk mw-list-item"><a href="https://uk.wikipedia.org/wiki/%D0%9D%D0%B0%D0%B2%D1%87%D0%B0%D0%BD%D0%BD%D1%8F_%D0%B7_%D0%BF%D1%96%D0%B4%D0%BA%D1%80%D1%96%D0%BF%D0%BB%D0%B5%D0%BD%D0%BD%D1%8F%D0%BC" title="Навчання з підкріпленням – Ukrainian" lang="uk" hreflang="uk" class="interlanguage-link-target"><span>Українська</span></a></li><li class="interlanguage-link interwiki-vi mw-list-item"><a href="https://vi.wikipedia.org/wiki/H%E1%BB%8Dc_t%C4%83ng_c%C6%B0%E1%BB%9Dng" title="Học tăng cường – Vietnamese" lang="vi" hreflang="vi" class="interlanguage-link-target"><span>Tiếng Việt</span></a></li><li class="interlanguage-link interwiki-wuu mw-list-item"><a href="https://wuu.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0" title="强化学习 – Wu Chinese" lang="wuu" hreflang="wuu" class="interlanguage-link-target"><span>吴语</span></a></li><li class="interlanguage-link interwiki-zh-yue mw-list-item"><a href="https://zh-yue.wikipedia.org/wiki/%E5%BC%B7%E5%8C%96%E5%AD%B8%E7%BF%92" title="強化學習 – Cantonese" lang="yue" hreflang="yue" class="interlanguage-link-target"><span>粵語</span></a></li><li class="interlanguage-link interwiki-zh mw-list-item"><a href="https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0" title="强化学习 – Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target"><span>中文</span></a></li></ul>
			<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q830687#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
		</div>

	</div>
</div>
				</header>
				<div class="vector-page-toolbar">
					<div class="vector-page-toolbar-container">
						<div id="left-navigation">
							<nav aria-label="Namespaces">
								
<div id="p-associated-pages" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-associated-pages"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			<li id="ca-nstab-main" class="selected vector-tab-noicon mw-list-item"><a data-mw="interface" href="/wiki/Reinforcement_learning" title="View the content page [c]" accesskey="c" class=""><span>Article</span></a>
</li>
<li id="ca-talk" class="vector-tab-noicon mw-list-item"><a data-mw="interface" href="/wiki/Talk:Reinforcement_learning" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t" class=""><span>Talk</span></a>
</li>

		</ul>
		
	</div>
</div>

								

<div id="p-variants" class="vector-dropdown mw-portlet mw-portlet-variants emptyPortlet"  >
	<input type="checkbox"
		id="p-variants-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-variants"
		class="vector-dropdown-checkbox"
		aria-label="Change language variant"
		
		
	/>
	<label
		id="p-variants-label"
		for="p-variants-checkbox"
		class="vector-dropdown-label "
		aria-hidden="true"
		
	>
		
		<span class="vector-dropdown-label-text">English</span>
	</label>
	<div class="vector-dropdown-content">

	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"></ul>
		
	</div>

	</div>
</div>
							</nav>
						</div>
						<div id="right-navigation" class="vector-collapsible">
							<nav aria-label="Views">
								
<div id="p-views" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-views"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			<li id="ca-view" class="selected vector-tab-noicon mw-list-item"><a data-mw="interface" href="/wiki/Reinforcement_learning" class=""><span>Read</span></a>
</li>
<li id="ca-edit" class="vector-tab-noicon mw-list-item"><a data-mw="interface" href="/w/index.php?title=Reinforcement_learning&amp;action=edit" title="Edit this page [e]" accesskey="e" class=""><span>Edit</span></a>
</li>
<li id="ca-history" class="vector-tab-noicon mw-list-item"><a data-mw="interface" href="/w/index.php?title=Reinforcement_learning&amp;action=history" title="Past revisions of this page [h]" accesskey="h" class=""><span>View history</span></a>
</li>

		</ul>
		
	</div>
</div>

							</nav>
				
							<nav class="vector-page-tools-landmark" aria-label="More options">
								
<div id="vector-page-tools-dropdown" class="vector-dropdown vector-page-tools-dropdown"  >
	<input type="checkbox"
		id="vector-page-tools-dropdown-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-vector-page-tools-dropdown"
		class="vector-dropdown-checkbox "
		
		aria-label="Tools"
		
	/>
	<label
		id="vector-page-tools-dropdown-label"
		for="vector-page-tools-dropdown-checkbox"
		class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet"
		aria-hidden="true"
		
	>
		
		<span class="vector-dropdown-label-text">Tools</span>
	</label>
	<div class="vector-dropdown-content">


									<div id="vector-page-tools-unpinned-container" class="vector-unpinned-container">
						
<div id="vector-page-tools" class="vector-page-tools vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-page-tools-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="page-tools-pinned"
	data-pinnable-element-id="vector-page-tools"
	data-pinned-container-id="vector-page-tools-pinned-container"
	data-unpinned-container-id="vector-page-tools-unpinned-container"
>
	<div class="vector-pinnable-header-label">Tools</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-page-tools.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-page-tools.unpin">hide</button>
</div>

	
<div id="p-cactions" class="vector-menu mw-portlet mw-portlet-cactions emptyPortlet vector-has-collapsible-items"  title="More options" >
	<div class="vector-menu-heading">
		Actions
	</div>
	
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="ca-more-view" class="selected vector-more-collapsible-item mw-list-item"><a href="/wiki/Reinforcement_learning"><span>Read</span></a></li><li id="ca-more-edit" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit"><span>Edit</span></a></li><li id="ca-more-history" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=Reinforcement_learning&amp;action=history"><span>View history</span></a></li></ul>
		
	</div>
</div>

<div id="p-tb" class="vector-menu mw-portlet mw-portlet-tb"  >
	<div class="vector-menu-heading">
		General
	</div>
	
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/Reinforcement_learning" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/Reinforcement_learning" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u"><span>Upload file</span></a></li><li id="t-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q"><span>Special pages</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/w/index.php?title=Reinforcement_learning&amp;oldid=1161565586" title="Permanent link to this revision of this page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/w/index.php?title=Reinforcement_learning&amp;action=info" title="More information about this page"><span>Page information</span></a></li><li id="t-cite" class="mw-list-item"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Reinforcement_learning&amp;id=1161565586&amp;wpFormIdentifier=titleform" title="Information on how to cite this page"><span>Cite this page</span></a></li><li id="t-wikibase" class="mw-list-item"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q830687" title="Structured data on this page hosted by Wikidata [g]" accesskey="g"><span>Wikidata item</span></a></li></ul>
		
	</div>
</div>

<div id="p-coll-print_export" class="vector-menu mw-portlet mw-portlet-coll-print_export"  >
	<div class="vector-menu-heading">
		Print/export
	</div>
	
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="coll-download-as-rl" class="mw-list-item"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Reinforcement_learning&amp;action=show-download-screen" title="Download this page as a PDF file"><span>Download as PDF</span></a></li><li id="t-print" class="mw-list-item"><a href="/w/index.php?title=Reinforcement_learning&amp;printable=yes" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li></ul>
		
	</div>
</div>

<div id="p-wikibase-otherprojects" class="vector-menu mw-portlet mw-portlet-wikibase-otherprojects"  >
	<div class="vector-menu-heading">
		In other projects
	</div>
	
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li class="wb-otherproject-link wb-otherproject-commons mw-list-item"><a href="https://commons.wikimedia.org/wiki/Category:Reinforcement_learning" hreflang="en"><span>Wikimedia Commons</span></a></li></ul>
		
	</div>
</div>

</div>

									</div>
				
	</div>
</div>
							</nav>
						</div>
					</div>
				</div>
				<div class="vector-column-end">
					<nav class="vector-page-tools-landmark vector-sticky-pinned-container" aria-label="More options">
						<div id="vector-page-tools-pinned-container" class="vector-pinned-container">
			
						</div>
	</nav>
				</div>
				<div id="bodyContent" class="vector-body" aria-labelledby="firstHeading" data-mw-ve-target-container>
					<div class="vector-body-before-content">
							<div class="mw-indicators">
		</div>

						<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
					</div>
					<div id="contentSub"><div id="mw-content-subtitle"></div></div>
					
					
					<div id="mw-content-text" class="mw-body-content mw-content-ltr" lang="en" dir="ltr"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Field of machine learning</div>
<style data-mw-deduplicate="TemplateStyles:r1033289096">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}</style><div role="note" class="hatnote navigation-not-searchable">For reinforcement learning in psychology, see <a href="/wiki/Reinforcement" title="Reinforcement">Reinforcement</a> and <a href="/wiki/Operant_conditioning" title="Operant conditioning">Operant conditioning</a>.</div>
<style data-mw-deduplicate="TemplateStyles:r1129693374">.mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:": "}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:" · ";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:" (";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:")";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:" "counter(listitem)"\a0 "}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:" ("counter(listitem)"\a0 "}</style><style data-mw-deduplicate="TemplateStyles:r1045330069">.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}</style><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886047488"><table class="sidebar sidebar-collapse nomobile nowraplinks"><tbody><tr><td class="sidebar-pretitle">Part of a series on</td></tr><tr><th class="sidebar-title-with-pretitle"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a><br />and <a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td class="sidebar-image"><span class="mw-default-size" typeof="mw:File/Frameless"><a href="/wiki/File:Kernel_Machine.svg" class="mw-file-description" title="Scatterplot featuring a linear support vector machine&#39;s decision boundary (dashed line)"><img alt="Scatterplot featuring a linear support vector machine&#39;s decision boundary (dashed line)" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png" decoding="async" width="220" height="100" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" data-file-width="512" data-file-height="233" /></a></span></td></tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Paradigms</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Batch_learning" class="mw-redirect" title="Batch learning">Batch learning</a></li>
<li><a href="/wiki/Meta-learning_(computer_science)" title="Meta-learning (computer science)">Meta-learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" class="mw-redirect" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Self-supervised_learning" title="Self-supervised learning">Self-supervised learning</a></li>
<li><a class="mw-selflink selflink">Reinforcement learning</a></li>
<li><a href="/wiki/Rule-based_machine_learning" title="Rule-based machine learning">Rule-based learning</a></li>
<li><a href="/wiki/Quantum_machine_learning" title="Quantum machine learning">Quantum machine learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Problems</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Generative_model" title="Generative model">Generative model</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimension reduction</a></li>
<li><a href="/wiki/Density_estimation" title="Density estimation">density estimation</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Data_Cleaning" class="mw-redirect" title="Data Cleaning">Data Cleaning</a></li>
<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Semantic_analysis_(machine_learning)" title="Semantic analysis (machine learning)">Semantic analysis</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li>
<li><a href="/wiki/Ontology_learning" title="Ontology learning">Ontology learning</a></li>
<li><a href="/wiki/Multimodal_learning" title="Multimodal learning">Multimodal learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><div style="display: inline-block; line-height: 1.2em; padding: .1em 0;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br /><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&#160;&#8226;&#32;<b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Apprenticeship_learning" title="Apprenticeship learning">Apprenticeship learning</a></li>
<li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="/wiki/Support_vector_machine" title="Support vector machine">Support vector machine (SVM)</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a href="/wiki/CURE_algorithm" title="CURE algorithm">CURE</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Fuzzy_clustering" title="Fuzzy clustering">Fuzzy</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br /><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="/wiki/Mean_shift" title="Mean shift">Mean shift</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li>
<li><a href="/wiki/Sparse_dictionary_learning" title="Sparse dictionary learning">SDL</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Random_sample_consensus" title="Random sample consensus">RANSAC</a></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li>
<li><a href="/wiki/Isolation_forest" title="Isolation forest">Isolation forest</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Cognitive_computing" title="Cognitive computing">Cognitive computing</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>
<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li>
<li><a href="/wiki/Echo_state_network" title="Echo state network">ESN</a></li>
<li><a href="/wiki/Reservoir_computing" title="Reservoir computing">reservoir computing</a></li></ul></li>
<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li>
<li><a href="/wiki/Transformer_(machine_learning_model)" title="Transformer (machine learning model)">Transformer</a>
<ul><li><a href="/wiki/Vision_transformer" title="Vision transformer">Vision</a></li></ul></li>
<li><a href="/wiki/Spiking_neural_network" title="Spiking neural network">Spiking neural network</a></li>
<li><a href="/wiki/Memtransistor" title="Memtransistor">Memtransistor</a></li>
<li><a href="/wiki/Electrochemical_RAM" title="Electrochemical RAM">Electrochemical RAM</a> (ECRAM)</li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a class="mw-selflink selflink">Reinforcement learning</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li>
<li><a href="/wiki/Multi-agent_reinforcement_learning" title="Multi-agent reinforcement learning">Multi-agent</a>
<ul><li><a href="/wiki/Self-play_(reinforcement_learning_technique)" class="mw-redirect" title="Self-play (reinforcement learning technique)">Self-play</a></li></ul></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Learning with humans</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Active_learning_(machine_learning)" title="Active learning (machine learning)">Active learning</a></li>
<li><a href="/wiki/Crowdsourcing" title="Crowdsourcing">Crowdsourcing</a></li>
<li><a href="/wiki/Human-in-the-loop" title="Human-in-the-loop">Human-in-the-loop</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Model diagnostics</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Learning_curve_(machine_learning)" title="Learning curve (machine learning)">Learning curve</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Mathematical foundations</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Kernel_machines" class="mw-redirect" title="Kernel machines">Kernel machines</a></li>
<li><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">Bias–variance tradeoff</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Machine-learning venues</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/International_Conference_on_Learning_Representations" title="International Conference on Learning Representations">ICLR</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Related articles</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li>
<li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-navbar"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><style data-mw-deduplicate="TemplateStyles:r1063604349">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}</style><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning" title="Template:Machine learning"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning" title="Template talk:Machine learning"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<p><b>Reinforcement learning</b> (<b>RL</b>) is an area of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> concerned with how <a href="/wiki/Intelligent_agent" title="Intelligent agent">intelligent agents</a> ought to take <a href="/wiki/Action_selection" title="Action selection">actions</a> in an environment in order to maximize the notion of <a href="/wiki/Reward-based_selection" title="Reward-based selection">cumulative reward</a>. Reinforcement learning is one of three basic machine learning paradigms, alongside <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> and <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a>.
</p><p>Reinforcement learning differs from supervised learning in not needing labelled input/output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).<sup id="cite_ref-kaelbling_1-0" class="reference"><a href="#cite_note-kaelbling-1">&#91;1&#93;</a></sup>
</p><p>
The environment is typically stated in the form of a <a href="/wiki/Markov_decision_process" title="Markov decision process">Markov decision process</a> (MDP), because many reinforcement learning algorithms for this context use <a href="/wiki/Dynamic_programming" title="Dynamic programming">dynamic programming</a> techniques.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup> The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.<style data-mw-deduplicate="TemplateStyles:r886046785">.mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}</style></p><div class="toclimit-3"><meta property="mw:PageProp/toc" /></div>
<h2><span class="mw-headline" id="Introduction">Introduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=1" title="Edit section: Introduction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<figure class="mw-halign-right" typeof="mw:File/Thumb"><a href="/wiki/File:Reinforcement_learning_diagram.svg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/250px-Reinforcement_learning_diagram.svg.png" decoding="async" width="250" height="242" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/375px-Reinforcement_learning_diagram.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/500px-Reinforcement_learning_diagram.svg.png 2x" data-file-width="300" data-file-height="290" /></a><figcaption>The typical framing of a Reinforcement Learning (RL) scenario: an agent takes actions in an environment, which is interpreted into a reward and a representation of the state, which are fed back into the agent.</figcaption></figure>
<p>Due to its generality, reinforcement learning is studied in many disciplines, such as <a href="/wiki/Game_theory" title="Game theory">game theory</a>, <a href="/wiki/Control_theory" title="Control theory">control theory</a>, <a href="/wiki/Operations_research" title="Operations research">operations research</a>, <a href="/wiki/Information_theory" title="Information theory">information theory</a>, <a href="/wiki/Simulation-based_optimization" title="Simulation-based optimization">simulation-based optimization</a>, <a href="/wiki/Multi-agent_system" title="Multi-agent system">multi-agent systems</a>, <a href="/wiki/Swarm_intelligence" title="Swarm intelligence">swarm intelligence</a>, and <a href="/wiki/Statistics" title="Statistics">statistics</a>. In the operations research and control literature, reinforcement learning is called <i>approximate dynamic programming,</i> or <i>neuro-dynamic programming.</i> The problems of interest in reinforcement learning have also been studied in the <a href="/wiki/Optimal_control_theory" class="mw-redirect" title="Optimal control theory">theory of optimal control</a>, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment. In <a href="/wiki/Economics" title="Economics">economics</a> and <a href="/wiki/Game_theory" title="Game theory">game theory</a>, reinforcement learning may be used to explain how equilibrium may arise under <a href="/wiki/Bounded_rationality" title="Bounded rationality">bounded rationality</a>.
</p><p>Basic reinforcement learning is modeled as a <a href="/wiki/Markov_decision_process" title="Markov decision process">Markov decision process (MDP)</a>:
</p>
<ul><li>a set of environment and agent states, <span class="texhtml mvar" style="font-style:italic;">S</span>;</li>
<li>a set of actions, <span class="texhtml mvar" style="font-style:italic;">A</span>, of the agent;</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P_{a}(s,s')=\Pr(s_{t+1}=s'\mid s_{t}=s,a_{t}=a)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>P</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>a</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>s</mi>
        <mo>,</mo>
        <msup>
          <mi>s</mi>
          <mo>&#x2032;</mo>
        </msup>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo movablelimits="true" form="prefix">Pr</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>s</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>=</mo>
        <msup>
          <mi>s</mi>
          <mo>&#x2032;</mo>
        </msup>
        <mo>&#x2223;<!-- ∣ --></mo>
        <msub>
          <mi>s</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mi>s</mi>
        <mo>,</mo>
        <msub>
          <mi>a</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mi>a</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P_{a}(s,s')=\Pr(s_{t+1}=s'\mid s_{t}=s,a_{t}=a)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f46bb2846b8b20a08220507fa105540f34850e7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:40.056ex; height:3.009ex;" alt="{\displaystyle P_{a}(s,s&#039;)=\Pr(s_{t+1}=s&#039;\mid s_{t}=s,a_{t}=a)}"></span> is the probability of transition (at time <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="t">
  <semantics>
    <mi>t</mi>
    <annotation encoding="application/x-tex">t</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a69220aabe3435c83e275b3757b11461fcc261c1" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:0.84ex; height:2.009ex;" alt="t"></span>) from state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="s">
  <semantics>
    <mi>s</mi>
    <annotation encoding="application/x-tex">s</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8fc9da81af29e9fa9f3c281bc94bb02b9158621b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:1.676ex;" alt="s"></span> to state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="s'">
  <semantics>
    <msup>
      <mi>s</mi>
      <mo>&#x2032;</mo>
    </msup>
    <annotation encoding="application/x-tex">s'</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6ddaf419306dee490e493375ec6560a017805933" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.775ex; height:2.509ex;" alt="s&#039;"></span> under action <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="a">
  <semantics>
    <mi>a</mi>
    <annotation encoding="application/x-tex">a</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/beb8390643483e5be8b170cab84ea824e4f933da" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.23ex; height:1.676ex;" alt="a"></span>.</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="R_{a}(s,s')">
  <semantics>
    <mrow>
      <msub>
        <mi>R</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mi>a</mi>
        </mrow>
      </msub>
      <mo stretchy="false">(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <msup>
        <mi>s</mi>
        <mo>&#x2032;</mo>
      </msup>
      <mo stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">R_{a}(s,s')</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1f450e8dca989ae096ac824648e2e8c4f4dcc9de" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:8.575ex; height:3.009ex;" alt="R_a(s,s&#039;)"></span> is the immediate reward after transition from <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="s">
  <semantics>
    <mi>s</mi>
    <annotation encoding="application/x-tex">s</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8fc9da81af29e9fa9f3c281bc94bb02b9158621b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:1.676ex;" alt="s"></span> to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="s'">
  <semantics>
    <msup>
      <mi>s</mi>
      <mo>&#x2032;</mo>
    </msup>
    <annotation encoding="application/x-tex">s'</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6ddaf419306dee490e493375ec6560a017805933" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.775ex; height:2.509ex;" alt="s&#039;"></span> with action <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="a">
  <semantics>
    <mi>a</mi>
    <annotation encoding="application/x-tex">a</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/beb8390643483e5be8b170cab84ea824e4f933da" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.23ex; height:1.676ex;" alt="a"></span>.</li></ul>
<p>The purpose of reinforcement learning is for the agent to learn an optimal, or nearly-optimal, policy that maximizes the "reward function" or other user-provided reinforcement signal that accumulates from the immediate rewards. This is similar to processes that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals can learn to engage in behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.<sup id="cite_ref-3" class="reference"><a href="#cite_note-3">&#91;3&#93;</a></sup><sup id="cite_ref-4" class="reference"><a href="#cite_note-4">&#91;4&#93;</a></sup>
</p><p>A basic reinforcement learning agent AI interacts with its environment in discrete time steps. At each time <span class="texhtml mvar" style="font-style:italic;">t</span>, the agent receives the current state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="s_{t}">
  <semantics>
    <msub>
      <mi>s</mi>
      <mrow class="MJX-TeXAtom-ORD">
        <mi>t</mi>
      </mrow>
    </msub>
    <annotation encoding="application/x-tex">s_{t}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/38ae023955a54cd7aaf2c22cb9ba425f6f9a77b0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:1.916ex; height:2.009ex;" alt="s_{t}"></span> and reward <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="r_{t}">
  <semantics>
    <msub>
      <mi>r</mi>
      <mrow class="MJX-TeXAtom-ORD">
        <mi>t</mi>
      </mrow>
    </msub>
    <annotation encoding="application/x-tex">r_{t}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1a5384c941dcf0f582b57817e1b0c4aef47b740b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:1.875ex; height:2.009ex;" alt="r_{t}"></span>. It then chooses an action <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="a_{t}">
  <semantics>
    <msub>
      <mi>a</mi>
      <mrow class="MJX-TeXAtom-ORD">
        <mi>t</mi>
      </mrow>
    </msub>
    <annotation encoding="application/x-tex">a_{t}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b1b5d8709ed161d7557d3e856cdfefa0f9494dd8" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.056ex; height:2.009ex;" alt="a_{t}"></span> from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="s_{t+1}">
  <semantics>
    <msub>
      <mi>s</mi>
      <mrow class="MJX-TeXAtom-ORD">
        <mi>t</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
    <annotation encoding="application/x-tex">s_{t+1}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7f2cadf385d259772a7ac84ac5c84ed68ff02028" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:4.017ex; height:2.009ex;" alt="s_{t+1}"></span> and the reward <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="r_{t+1}">
  <semantics>
    <msub>
      <mi>r</mi>
      <mrow class="MJX-TeXAtom-ORD">
        <mi>t</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
    <annotation encoding="application/x-tex">r_{t+1}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1c098785d701b0a4a3244c27992c0269abc58fe4" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:3.975ex; height:2.009ex;" alt="r_{t+1}"></span> associated with the <i>transition</i> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="(s_{t},a_{t},s_{t+1})">
  <semantics>
    <mrow>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>s</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mi>t</mi>
        </mrow>
      </msub>
      <mo>,</mo>
      <msub>
        <mi>a</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mi>t</mi>
        </mrow>
      </msub>
      <mo>,</mo>
      <msub>
        <mi>s</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mi>t</mi>
          <mo>+</mo>
          <mn>1</mn>
        </mrow>
      </msub>
      <mo stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">(s_{t},a_{t},s_{t+1})</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bef137dce80b10c4446c5dd6a55f092ff0ea92f9" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:11.866ex; height:2.843ex;" alt="(s_{t},a_{t},s_{t+1})"></span> is determined. The goal of a reinforcement learning agent is to learn a <i>policy</i>: <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \pi :A\times S\rightarrow [0,1]}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C0;<!-- π --></mi>
        <mo>:</mo>
        <mi>A</mi>
        <mo>&#x00D7;<!-- × --></mo>
        <mi>S</mi>
        <mo stretchy="false">&#x2192;<!-- → --></mo>
        <mo stretchy="false">[</mo>
        <mn>0</mn>
        <mo>,</mo>
        <mn>1</mn>
        <mo stretchy="false">]</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \pi :A\times S\rightarrow [0,1]}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/53cdefaaea162af26e217b740b90f8f8c692a641" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:17.619ex; height:2.843ex;" alt="{\displaystyle \pi :A\times S\rightarrow [0,1]}"></span>, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \pi (a,s)=\Pr(a_{t}=a\mid s_{t}=s)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C0;<!-- π --></mi>
        <mo stretchy="false">(</mo>
        <mi>a</mi>
        <mo>,</mo>
        <mi>s</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo movablelimits="true" form="prefix">Pr</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>a</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mi>a</mi>
        <mo>&#x2223;<!-- ∣ --></mo>
        <msub>
          <mi>s</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mi>s</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \pi (a,s)=\Pr(a_{t}=a\mid s_{t}=s)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fdd3e1707dd8b639c2bd7c6a3182b0e507611174" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:28.324ex; height:2.843ex;" alt="{\displaystyle \pi (a,s)=\Pr(a_{t}=a\mid s_{t}=s)}"></span> which maximizes the expected cumulative reward.
</p><p>Formulating the problem as an MDP assumes the agent directly observes the current environmental state; in this case the problem is said to have <i>full observability</i>. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have <i>partial observability</i>, and formally the problem must be formulated as a <a href="/wiki/Partially_observable_Markov_decision_process" title="Partially observable Markov decision process">Partially observable Markov decision process</a>. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.
</p><p>When the agent's performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of <i><a href="/wiki/Regret_(game_theory)" class="mw-redirect" title="Regret (game theory)">regret</a></i>. In order to act near optimally, the agent must reason about the long-term consequences of its actions (i.e., maximize future income), although the immediate reward associated with this might be negative.
</p><p>Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including energy storage operation,<sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;5&#93;</a></sup> <a href="/wiki/Robot_control" title="Robot control">robot control</a>,<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">&#91;6&#93;</a></sup> <a href="/wiki/Elevator_algorithm" title="Elevator algorithm">elevator scheduling</a>, <a href="/wiki/Telecommunications" title="Telecommunications">telecommunications</a>, photovoltaic generators dispatch,<sup id="cite_ref-7" class="reference"><a href="#cite_note-7">&#91;7&#93;</a></sup> <a href="/wiki/Backgammon" title="Backgammon">backgammon</a>, <a href="/wiki/Checkers" title="Checkers">checkers</a><sup id="cite_ref-FOOTNOTESuttonBarto2018Chapter_11_8-0" class="reference"><a href="#cite_note-FOOTNOTESuttonBarto2018Chapter_11-8">&#91;8&#93;</a></sup> and <a href="/wiki/Go_(game)" title="Go (game)">Go</a> (<a href="/wiki/AlphaGo" title="AlphaGo">AlphaGo</a>).
</p><p>Two elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in the following situations:
</p>
<ul><li>A model of the environment is known, but an <a href="/wiki/Closed-form_expression" title="Closed-form expression">analytic solution</a> is not available;</li>
<li>Only a simulation model of the environment is given (the subject of <a href="/wiki/Simulation-based_optimization" title="Simulation-based optimization">simulation-based optimization</a>);<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup></li>
<li>The only way to collect information about the environment is to interact with it.</li></ul>
<p>The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> problems.
</p>
<h2><span class="mw-headline" id="Exploration">Exploration</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=2" title="Edit section: Exploration">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The exploration vs. exploitation trade-off has been most thoroughly studied through the <a href="/wiki/Multi-armed_bandit" title="Multi-armed bandit">multi-armed bandit</a> problem and for finite state space MDPs in Burnetas and Katehakis (1997).<sup id="cite_ref-Optimal_adaptive_policies_for_Marko_10-0" class="reference"><a href="#cite_note-Optimal_adaptive_policies_for_Marko-10">&#91;10&#93;</a></sup>
</p><p>Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite MDPs is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.
</p><p>One such method is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\varepsilon ">
  <semantics>
    <mi>&#x03B5;<!-- ε --></mi>
    <annotation encoding="application/x-tex">\varepsilon</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0427e0b3035bd227c90f9e99f11823dea81c19a7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.083ex; height:1.676ex;" alt="\varepsilon "></span>-greedy, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle 0&lt;\varepsilon &lt;1}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mn>0</mn>
        <mo>&lt;</mo>
        <mi>&#x03B5;<!-- ε --></mi>
        <mo>&lt;</mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle 0&lt;\varepsilon &lt;1}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a62c0bbc36352f492bde6217430cfe342032487d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:9.605ex; height:2.176ex;" alt="{\displaystyle 0&lt;\varepsilon &lt;1}"></span> is a parameter controlling the amount of exploration vs. exploitation.  With probability <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="1-\varepsilon ">
  <semantics>
    <mrow>
      <mn>1</mn>
      <mo>&#x2212;<!-- − --></mo>
      <mi>&#x03B5;<!-- ε --></mi>
    </mrow>
    <annotation encoding="application/x-tex">1-\varepsilon</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/213bfb1e70c70fe7f9f5bd3d6acd0baf6d0dfaf3" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.505ex; width:5.086ex; height:2.343ex;" alt="1-\varepsilon"></span>, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\varepsilon ">
  <semantics>
    <mi>&#x03B5;<!-- ε --></mi>
    <annotation encoding="application/x-tex">\varepsilon</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0427e0b3035bd227c90f9e99f11823dea81c19a7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.083ex; height:1.676ex;" alt="\varepsilon "></span>, exploration is chosen, and the action is chosen uniformly at random. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\varepsilon ">
  <semantics>
    <mi>&#x03B5;<!-- ε --></mi>
    <annotation encoding="application/x-tex">\varepsilon</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0427e0b3035bd227c90f9e99f11823dea81c19a7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.083ex; height:1.676ex;" alt="\varepsilon "></span> is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">&#91;11&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Algorithms_for_control_learning">Algorithms for control learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=3" title="Edit section: Algorithms for control learning">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.
</p>
<h3><span class="mw-headline" id="Criterion_of_optimality">Criterion of optimality</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=4" title="Edit section: Criterion of optimality">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Policy">Policy</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=5" title="Edit section: Policy">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>The agent's action selection is modeled as a map called <i>policy</i>:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \pi :A\times S\rightarrow [0,1]}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C0;<!-- π --></mi>
        <mo>:</mo>
        <mi>A</mi>
        <mo>&#x00D7;<!-- × --></mo>
        <mi>S</mi>
        <mo stretchy="false">&#x2192;<!-- → --></mo>
        <mo stretchy="false">[</mo>
        <mn>0</mn>
        <mo>,</mo>
        <mn>1</mn>
        <mo stretchy="false">]</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \pi :A\times S\rightarrow [0,1]}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/53cdefaaea162af26e217b740b90f8f8c692a641" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:17.619ex; height:2.843ex;" alt="{\displaystyle \pi :A\times S\rightarrow [0,1]}"></span></dd>
<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \pi (a,s)=\Pr(a_{t}=a\mid s_{t}=s)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C0;<!-- π --></mi>
        <mo stretchy="false">(</mo>
        <mi>a</mi>
        <mo>,</mo>
        <mi>s</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo movablelimits="true" form="prefix">Pr</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>a</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mi>a</mi>
        <mo>&#x2223;<!-- ∣ --></mo>
        <msub>
          <mi>s</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mi>s</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \pi (a,s)=\Pr(a_{t}=a\mid s_{t}=s)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fdd3e1707dd8b639c2bd7c6a3182b0e507611174" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:28.324ex; height:2.843ex;" alt="{\displaystyle \pi (a,s)=\Pr(a_{t}=a\mid s_{t}=s)}"></span></dd></dl>
<p>The policy map gives the probability of taking action <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="a">
  <semantics>
    <mi>a</mi>
    <annotation encoding="application/x-tex">a</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/beb8390643483e5be8b170cab84ea824e4f933da" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.23ex; height:1.676ex;" alt="a"></span> when in state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="s">
  <semantics>
    <mi>s</mi>
    <annotation encoding="application/x-tex">s</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8fc9da81af29e9fa9f3c281bc94bb02b9158621b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:1.676ex;" alt="s"></span>.<sup id="cite_ref-:0_12-0" class="reference"><a href="#cite_note-:0-12">&#91;12&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 61">&#58;&#8202;61&#8202;</span></sup> There are also deterministic policies.
</p>
<h4><span class="mw-headline" id="State-value_function">State-value function</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=6" title="Edit section: State-value function">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>The value function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle V_{\pi }(s)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>V</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>&#x03C0;<!-- π --></mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>s</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle V_{\pi }(s)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/04f08b168d05891950e565017e284abee8bf7cf1" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.429ex; height:2.843ex;" alt="{\displaystyle V_{\pi }(s)}"></span> is defined as, <i>expected return</i> starting with state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="s">
  <semantics>
    <mi>s</mi>
    <annotation encoding="application/x-tex">s</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8fc9da81af29e9fa9f3c281bc94bb02b9158621b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:1.676ex;" alt="s"></span>, i.e. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle s_{0}=s}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>s</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>0</mn>
          </mrow>
        </msub>
        <mo>=</mo>
        <mi>s</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle s_{0}=s}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e7693f1a42955a4a160e79816945e3469cbdf6b2" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:6.334ex; height:2.009ex;" alt="{\displaystyle s_{0}=s}"></span>, and successively following policy <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\pi ">
  <semantics>
    <mi>&#x03C0;<!-- π --></mi>
    <annotation encoding="application/x-tex">\pi</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f136797a15dbace015c994b0e4f73392ecd8aa0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.332ex; height:1.676ex;" alt="\pi "></span>. Hence, roughly speaking, the value function estimates "how good" it is to be in a given state.<sup id="cite_ref-:0_12-1" class="reference"><a href="#cite_note-:0-12">&#91;12&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 60">&#58;&#8202;60&#8202;</span></sup>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle V_{\pi }(s)=\operatorname {E} [R\mid s_{0}=s]=\operatorname {E} \left[\sum _{t=0}^{\infty }\gamma ^{t}r_{t}\mid s_{0}=s\right],}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>V</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>&#x03C0;<!-- π --></mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>s</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi mathvariant="normal">E</mi>
        <mo>&#x2061;<!-- ⁡ --></mo>
        <mo stretchy="false">[</mo>
        <mi>R</mi>
        <mo>&#x2223;<!-- ∣ --></mo>
        <msub>
          <mi>s</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>0</mn>
          </mrow>
        </msub>
        <mo>=</mo>
        <mi>s</mi>
        <mo stretchy="false">]</mo>
        <mo>=</mo>
        <mi mathvariant="normal">E</mi>
        <mo>&#x2061;<!-- ⁡ --></mo>
        <mrow>
          <mo>[</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>t</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="normal">&#x221E;<!-- ∞ --></mi>
              </mrow>
            </munderover>
            <msup>
              <mi>&#x03B3;<!-- γ --></mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>t</mi>
              </mrow>
            </msup>
            <msub>
              <mi>r</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>t</mi>
              </mrow>
            </msub>
            <mo>&#x2223;<!-- ∣ --></mo>
            <msub>
              <mi>s</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>0</mn>
              </mrow>
            </msub>
            <mo>=</mo>
            <mi>s</mi>
          </mrow>
          <mo>]</mo>
        </mrow>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle V_{\pi }(s)=\operatorname {E} [R\mid s_{0}=s]=\operatorname {E} \left[\sum _{t=0}^{\infty }\gamma ^{t}r_{t}\mid s_{0}=s\right],}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/42cacec1da9e5861cc7538e33cd239dc76136dbf" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.171ex; width:45.471ex; height:7.509ex;" alt="{\displaystyle V_{\pi }(s)=\operatorname {E} [R\mid s_{0}=s]=\operatorname {E} \left[\sum _{t=0}^{\infty }\gamma ^{t}r_{t}\mid s_{0}=s\right],}"></span></dd></dl>
<p>where the random variable <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="R">
  <semantics>
    <mi>R</mi>
    <annotation encoding="application/x-tex">R</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/36319c8bc57092b4ba05917511292516bb0ca764" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.764ex; height:2.176ex;" alt="R"></span> denotes the <b>return</b>, and is defined as the sum of future discounted rewards:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle R=\sum _{t=0}^{\infty }\gamma ^{t}r_{t},}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>R</mi>
        <mo>=</mo>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
            <mo>=</mo>
            <mn>0</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">&#x221E;<!-- ∞ --></mi>
          </mrow>
        </munderover>
        <msup>
          <mi>&#x03B3;<!-- γ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msup>
        <msub>
          <mi>r</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle R=\sum _{t=0}^{\infty }\gamma ^{t}r_{t},}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6ce68ed7929df97bae40600d8e97f223e755d481" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:13.232ex; height:6.843ex;" alt="{\displaystyle R=\sum _{t=0}^{\infty }\gamma ^{t}r_{t},}"></span></dd></dl>
<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="r_{t}">
  <semantics>
    <msub>
      <mi>r</mi>
      <mrow class="MJX-TeXAtom-ORD">
        <mi>t</mi>
      </mrow>
    </msub>
    <annotation encoding="application/x-tex">r_{t}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1a5384c941dcf0f582b57817e1b0c4aef47b740b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:1.875ex; height:2.009ex;" alt="r_{t}"></span> is the reward at step <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="t">
  <semantics>
    <mi>t</mi>
    <annotation encoding="application/x-tex">t</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a69220aabe3435c83e275b3757b11461fcc261c1" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:0.84ex; height:2.009ex;" alt="t"></span>, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \gamma \in [0,1)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03B3;<!-- γ --></mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <mo stretchy="false">[</mo>
        <mn>0</mn>
        <mo>,</mo>
        <mn>1</mn>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \gamma \in [0,1)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5566ea32e811db1a014d523cb82833b9e9d039b5" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:9.013ex; height:2.843ex;" alt="{\displaystyle \gamma \in [0,1)}"></span> is the <a href="/wiki/Q-learning#Discount_factor" title="Q-learning">discount-rate</a>. Gamma is less than 1, so events in the distant future are weighted less than events in the immediate future.
</p><p>The algorithm must find a policy with maximum expected return. From the theory of MDPs it is known that, without loss of generality, the search can be restricted to the set of so-called <i>stationary</i> policies. A policy is <i>stationary</i> if the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted to <i>deterministic</i> stationary policies. A <i>deterministic stationary</i> policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.
</p>
<h3><span class="mw-headline" id="Brute_force">Brute force</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=7" title="Edit section: Brute force">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The <a href="/wiki/Brute-force_search" title="Brute-force search">brute force</a> approach entails two steps:
</p>
<ul><li>For each possible policy, sample returns while following it</li>
<li>Choose the policy with the largest expected return</li></ul>
<p>One problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the return of each policy.
</p><p>These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are <a href="#Value_function">value function estimation</a> and <a href="#Direct_policy_search">direct policy search</a>.
</p>
<h3><span class="mw-headline" id="Value_function">Value function</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=8" title="Edit section: Value function">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1033289096"><div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Value_function" title="Value function">Value function</a></div>
<p>Value function approaches attempt to find a policy that maximizes the return by maintaining a set of estimates of expected returns for some policy (usually either the "current" [on-policy] or the optimal [off-policy] one).
</p><p>These methods rely on the theory of Markov decision processes, where optimality is defined in a sense that is stronger than the above one: A policy is called optimal if it achieves the best-expected return from <i>any</i> initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found amongst stationary policies.
</p><p>To define optimality in a formal manner, define the value of a policy <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\pi ">
  <semantics>
    <mi>&#x03C0;<!-- π --></mi>
    <annotation encoding="application/x-tex">\pi</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f136797a15dbace015c994b0e4f73392ecd8aa0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.332ex; height:1.676ex;" alt="\pi "></span> by
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle V^{\pi }(s)=E[R\mid s,\pi ],}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>V</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>&#x03C0;<!-- π --></mi>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <mi>s</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi>E</mi>
        <mo stretchy="false">[</mo>
        <mi>R</mi>
        <mo>&#x2223;<!-- ∣ --></mo>
        <mi>s</mi>
        <mo>,</mo>
        <mi>&#x03C0;<!-- π --></mi>
        <mo stretchy="false">]</mo>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle V^{\pi }(s)=E[R\mid s,\pi ],}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d8ff1cf02d3c0c16f1d0da9a204cf9e4d955c48c" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:19.963ex; height:2.843ex;" alt="{\displaystyle V^{\pi }(s)=E[R\mid s,\pi ],}"></span></dd></dl>
<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="R">
  <semantics>
    <mi>R</mi>
    <annotation encoding="application/x-tex">R</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/36319c8bc57092b4ba05917511292516bb0ca764" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.764ex; height:2.176ex;" alt="R"></span> stands for the return associated with following <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\pi ">
  <semantics>
    <mi>&#x03C0;<!-- π --></mi>
    <annotation encoding="application/x-tex">\pi</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f136797a15dbace015c994b0e4f73392ecd8aa0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.332ex; height:1.676ex;" alt="\pi "></span> from the initial state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="s">
  <semantics>
    <mi>s</mi>
    <annotation encoding="application/x-tex">s</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8fc9da81af29e9fa9f3c281bc94bb02b9158621b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:1.676ex;" alt="s"></span>. Defining <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="V^{*}(s)">
  <semantics>
    <mrow>
      <msup>
        <mi>V</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>&#x2217;<!-- ∗ --></mo>
        </mrow>
      </msup>
      <mo stretchy="false">(</mo>
      <mi>s</mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">V^{*}(s)</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/634d57ab397892e81f1c9c9456873814959ad185" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.871ex; height:2.843ex;" alt="V^{*}(s)"></span> as the maximum possible value of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="V^{\pi }(s)">
  <semantics>
    <mrow>
      <msup>
        <mi>V</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mi>&#x03C0;<!-- π --></mi>
        </mrow>
      </msup>
      <mo stretchy="false">(</mo>
      <mi>s</mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">V^{\pi }(s)</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1f2facaa1b0f3d84f0b6101590b8e949ab822939" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.991ex; height:2.843ex;" alt="V^{\pi }(s)"></span>, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\pi ">
  <semantics>
    <mi>&#x03C0;<!-- π --></mi>
    <annotation encoding="application/x-tex">\pi</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f136797a15dbace015c994b0e4f73392ecd8aa0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.332ex; height:1.676ex;" alt="\pi "></span> is allowed to change,
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle V^{*}(s)=\max _{\pi }V^{\pi }(s).}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>V</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <mi>s</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <munder>
          <mo movablelimits="true" form="prefix">max</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>&#x03C0;<!-- π --></mi>
          </mrow>
        </munder>
        <msup>
          <mi>V</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>&#x03C0;<!-- π --></mi>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <mi>s</mi>
        <mo stretchy="false">)</mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle V^{*}(s)=\max _{\pi }V^{\pi }(s).}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3a1364980b05ea95dedcefb2082869eab54dd1cc" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.005ex; width:20.32ex; height:4.009ex;" alt="{\displaystyle V^{*}(s)=\max _{\pi }V^{\pi }(s).}"></span></dd></dl>
<p>A policy that achieves these optimal values in each state is called <i>optimal</i>. Clearly, a policy that is optimal in this strong sense is also optimal in the sense that it maximizes the expected return <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\rho ^{\pi }">
  <semantics>
    <msup>
      <mi>&#x03C1;<!-- ρ --></mi>
      <mrow class="MJX-TeXAtom-ORD">
        <mi>&#x03C0;<!-- π --></mi>
      </mrow>
    </msup>
    <annotation encoding="application/x-tex">\rho ^{\pi }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/57e077225550e579813f4421ff4557f4366640f6" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:2.376ex; height:2.843ex;" alt="\rho ^{\pi }"></span>, since <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\rho ^{\pi }=E[V^{\pi }(S)]">
  <semantics>
    <mrow>
      <msup>
        <mi>&#x03C1;<!-- ρ --></mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mi>&#x03C0;<!-- π --></mi>
        </mrow>
      </msup>
      <mo>=</mo>
      <mi>E</mi>
      <mo stretchy="false">[</mo>
      <msup>
        <mi>V</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mi>&#x03C0;<!-- π --></mi>
        </mrow>
      </msup>
      <mo stretchy="false">(</mo>
      <mi>S</mi>
      <mo stretchy="false">)</mo>
      <mo stretchy="false">]</mo>
    </mrow>
    <annotation encoding="application/x-tex">\rho ^{\pi }=E[V^{\pi }(S)]</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/be6ed784bbb389397215904c6199944be538f311" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:14.943ex; height:2.843ex;" alt="\rho ^{\pi }=E[V^{\pi }(S)]"></span>, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="S">
  <semantics>
    <mi>S</mi>
    <annotation encoding="application/x-tex">S</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f80bc46671f74ec1cd329b24c48f39480c962b87" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.499ex; height:2.176ex;" alt="S"></span> is a state randomly sampled from the distribution <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\mu ">
  <semantics>
    <mi>&#x03BC;<!-- μ --></mi>
    <annotation encoding="application/x-tex">\mu</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/eadf83405ce8a69fbbd03de0a0642e1409b3adfc" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:1.402ex; height:2.176ex;" alt="\mu "></span> of initial states (so <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mu (s)=\Pr(s_{0}=s)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03BC;<!-- μ --></mi>
        <mo stretchy="false">(</mo>
        <mi>s</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo movablelimits="true" form="prefix">Pr</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>s</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>0</mn>
          </mrow>
        </msub>
        <mo>=</mo>
        <mi>s</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mu (s)=\Pr(s_{0}=s)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2557c2142cd27936bdbf8531d9acc250e49ad916" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:18.037ex; height:2.843ex;" alt="{\displaystyle \mu (s)=\Pr(s_{0}=s)}"></span>).
</p><p>Although state-values suffice to define optimality, it is useful to define action-values. Given a state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="s">
  <semantics>
    <mi>s</mi>
    <annotation encoding="application/x-tex">s</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8fc9da81af29e9fa9f3c281bc94bb02b9158621b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:1.676ex;" alt="s"></span>, an action <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="a">
  <semantics>
    <mi>a</mi>
    <annotation encoding="application/x-tex">a</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/beb8390643483e5be8b170cab84ea824e4f933da" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.23ex; height:1.676ex;" alt="a"></span> and a policy <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\pi ">
  <semantics>
    <mi>&#x03C0;<!-- π --></mi>
    <annotation encoding="application/x-tex">\pi</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f136797a15dbace015c994b0e4f73392ecd8aa0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.332ex; height:1.676ex;" alt="\pi "></span>, the action-value of the pair <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="(s,a)">
  <semantics>
    <mrow>
      <mo stretchy="false">(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">(s,a)</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f6cfb5df21f3530ff1710982e3e0107ceb46a274" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.164ex; height:2.843ex;" alt="(s,a)"></span> under <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\pi ">
  <semantics>
    <mi>&#x03C0;<!-- π --></mi>
    <annotation encoding="application/x-tex">\pi</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f136797a15dbace015c994b0e4f73392ecd8aa0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.332ex; height:1.676ex;" alt="\pi "></span> is defined by
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle Q^{\pi }(s,a)=\operatorname {E} [R\mid s,a,\pi ],\,}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>Q</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>&#x03C0;<!-- π --></mi>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <mi>s</mi>
        <mo>,</mo>
        <mi>a</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi mathvariant="normal">E</mi>
        <mo>&#x2061;<!-- ⁡ --></mo>
        <mo stretchy="false">[</mo>
        <mi>R</mi>
        <mo>&#x2223;<!-- ∣ --></mo>
        <mi>s</mi>
        <mo>,</mo>
        <mi>a</mi>
        <mo>,</mo>
        <mi>&#x03C0;<!-- π --></mi>
        <mo stretchy="false">]</mo>
        <mo>,</mo>
        <mspace width="thinmathspace" />
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle Q^{\pi }(s,a)=\operatorname {E} [R\mid s,a,\pi ],\,}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/589b970c42fc5cd7b8a1c4859046d18e97046090" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:24.606ex; height:2.843ex;" alt="{\displaystyle Q^{\pi }(s,a)=\operatorname {E} [R\mid s,a,\pi ],\,}"></span></dd></dl>
<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="R">
  <semantics>
    <mi>R</mi>
    <annotation encoding="application/x-tex">R</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/36319c8bc57092b4ba05917511292516bb0ca764" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.764ex; height:2.176ex;" alt="R"></span> now stands for the random return associated with first taking action <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="a">
  <semantics>
    <mi>a</mi>
    <annotation encoding="application/x-tex">a</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/beb8390643483e5be8b170cab84ea824e4f933da" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.23ex; height:1.676ex;" alt="a"></span> in state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="s">
  <semantics>
    <mi>s</mi>
    <annotation encoding="application/x-tex">s</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8fc9da81af29e9fa9f3c281bc94bb02b9158621b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:1.676ex;" alt="s"></span> and following <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\pi ">
  <semantics>
    <mi>&#x03C0;<!-- π --></mi>
    <annotation encoding="application/x-tex">\pi</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f136797a15dbace015c994b0e4f73392ecd8aa0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.332ex; height:1.676ex;" alt="\pi "></span>, thereafter.
</p><p>The theory of MDPs states that if <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\pi ^{*}">
  <semantics>
    <msup>
      <mi>&#x03C0;<!-- π --></mi>
      <mrow class="MJX-TeXAtom-ORD">
        <mo>&#x2217;<!-- ∗ --></mo>
      </mrow>
    </msup>
    <annotation encoding="application/x-tex">\pi ^{*}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f60750296eb9fbe2d3d81546250d7a2feec6e795" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.388ex; height:2.343ex;" alt="\pi^*"></span> is an optimal policy, we act optimally (take the optimal action) by choosing the action from <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle Q^{\pi ^{*}}(s,\cdot )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>Q</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msup>
              <mi>&#x03C0;<!-- π --></mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo>&#x2217;<!-- ∗ --></mo>
              </mrow>
            </msup>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <mi>s</mi>
        <mo>,</mo>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle Q^{\pi ^{*}}(s,\cdot )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fafa0cb84fd68dbe99e9b5baa0b421837c2835d8" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:8.426ex; height:3.343ex;" alt="{\displaystyle Q^{\pi ^{*}}(s,\cdot )}"></span> with the highest value at each state, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="s">
  <semantics>
    <mi>s</mi>
    <annotation encoding="application/x-tex">s</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8fc9da81af29e9fa9f3c281bc94bb02b9158621b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:1.676ex;" alt="s"></span>. The <i>action-value function</i> of such an optimal policy (<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle Q^{\pi ^{*}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>Q</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msup>
              <mi>&#x03C0;<!-- π --></mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo>&#x2217;<!-- ∗ --></mo>
              </mrow>
            </msup>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle Q^{\pi ^{*}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/efdae1153bf5fc57c8777369756d7c5cc20fd90a" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:3.846ex; height:3.176ex;" alt="{\displaystyle Q^{\pi ^{*}}}"></span>) is called the <i>optimal action-value function</i> and is commonly denoted by <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="Q^{*}">
  <semantics>
    <msup>
      <mi>Q</mi>
      <mrow class="MJX-TeXAtom-ORD">
        <mo>&#x2217;<!-- ∗ --></mo>
      </mrow>
    </msup>
    <annotation encoding="application/x-tex">Q^{*}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4ad911120ce1315802e850243a2e9578bb2b5855" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.893ex; height:2.676ex;" alt="Q^{*}"></span>. In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.
</p><p>Assuming full knowledge of the MDP, the two basic approaches to compute the optimal action-value function are <a href="/wiki/Value_iteration" class="mw-redirect" title="Value iteration">value iteration</a> and <a href="/wiki/Policy_iteration" class="mw-redirect" title="Policy iteration">policy iteration</a>. Both algorithms compute a sequence of functions <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="Q_{k}">
  <semantics>
    <msub>
      <mi>Q</mi>
      <mrow class="MJX-TeXAtom-ORD">
        <mi>k</mi>
      </mrow>
    </msub>
    <annotation encoding="application/x-tex">Q_{k}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cbb70773cb27f435f0040629f3fcf71ab3857aee" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.927ex; height:2.509ex;" alt="Q_{k}"></span> (<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="k=0,1,2,\ldots ">
  <semantics>
    <mrow>
      <mi>k</mi>
      <mo>=</mo>
      <mn>0</mn>
      <mo>,</mo>
      <mn>1</mn>
      <mo>,</mo>
      <mn>2</mn>
      <mo>,</mo>
      <mo>&#x2026;<!-- … --></mo>
    </mrow>
    <annotation encoding="application/x-tex">k=0,1,2,\ldots</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c01b4a3bf47ec73a4324207efcbb2af4b0fd9dfd" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:13.622ex; height:2.509ex;" alt="k=0,1,2,\ldots "></span>) that converge to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="Q^{*}">
  <semantics>
    <msup>
      <mi>Q</mi>
      <mrow class="MJX-TeXAtom-ORD">
        <mo>&#x2217;<!-- ∗ --></mo>
      </mrow>
    </msup>
    <annotation encoding="application/x-tex">Q^{*}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4ad911120ce1315802e850243a2e9578bb2b5855" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.893ex; height:2.676ex;" alt="Q^{*}"></span>. Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) MDPs. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.
</p>
<h4><span class="mw-headline" id="Monte_Carlo_methods">Monte Carlo methods</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=9" title="Edit section: Monte Carlo methods">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p><a href="/wiki/Monte_Carlo_sampling" class="mw-redirect" title="Monte Carlo sampling">Monte Carlo methods</a> can be used in an algorithm that mimics policy iteration. Policy iteration consists of two steps: <i>policy evaluation</i> and <i>policy improvement</i>.
</p><p>Monte Carlo is used in the policy evaluation step. In this step, given a stationary, deterministic policy <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\pi ">
  <semantics>
    <mi>&#x03C0;<!-- π --></mi>
    <annotation encoding="application/x-tex">\pi</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f136797a15dbace015c994b0e4f73392ecd8aa0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.332ex; height:1.676ex;" alt="\pi "></span>, the goal is to compute the function values <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="Q^{\pi }(s,a)">
  <semantics>
    <mrow>
      <msup>
        <mi>Q</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mi>&#x03C0;<!-- π --></mi>
        </mrow>
      </msup>
      <mo stretchy="false">(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">Q^{\pi }(s,a)</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1f6f2577255c6b8158cfaa96a27049b600466248" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:8.176ex; height:2.843ex;" alt="Q^{\pi }(s,a)"></span> (or a good approximation to them) for all state-action pairs <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="(s,a)">
  <semantics>
    <mrow>
      <mo stretchy="false">(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">(s,a)</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f6cfb5df21f3530ff1710982e3e0107ceb46a274" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.164ex; height:2.843ex;" alt="(s,a)"></span>. Assume (for simplicity) that the MDP is finite, that sufficient memory is available to accommodate the action-values and that the problem is episodic and after each episode a new one starts from some random initial state. Then, the estimate of the value of a given state-action pair <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="(s,a)">
  <semantics>
    <mrow>
      <mo stretchy="false">(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">(s,a)</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f6cfb5df21f3530ff1710982e3e0107ceb46a274" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.164ex; height:2.843ex;" alt="(s,a)"></span> can be computed by averaging the sampled returns that originated from <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="(s,a)">
  <semantics>
    <mrow>
      <mo stretchy="false">(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">(s,a)</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f6cfb5df21f3530ff1710982e3e0107ceb46a274" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.164ex; height:2.843ex;" alt="(s,a)"></span> over time.  Given sufficient time, this procedure can thus construct a precise estimate <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="Q">
  <semantics>
    <mi>Q</mi>
    <annotation encoding="application/x-tex">Q</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a58f821939f10fed529c3d014263bf4455b19c48" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:1.838ex; height:2.509ex;" alt="Q"></span> of the action-value function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="Q^{\pi }">
  <semantics>
    <msup>
      <mi>Q</mi>
      <mrow class="MJX-TeXAtom-ORD">
        <mi>&#x03C0;<!-- π --></mi>
      </mrow>
    </msup>
    <annotation encoding="application/x-tex">Q^{\pi }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9592669f12403f0c066dccc36f19599752d8cc7b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:3.012ex; height:2.676ex;" alt="Q^{\pi }"></span>. This finishes the description of the policy evaluation step.
</p><p>In the policy improvement step, the next policy is obtained by computing a <i>greedy</i> policy with respect to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="Q">
  <semantics>
    <mi>Q</mi>
    <annotation encoding="application/x-tex">Q</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a58f821939f10fed529c3d014263bf4455b19c48" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:1.838ex; height:2.509ex;" alt="Q"></span>: Given a state <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="s">
  <semantics>
    <mi>s</mi>
    <annotation encoding="application/x-tex">s</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8fc9da81af29e9fa9f3c281bc94bb02b9158621b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:1.676ex;" alt="s"></span>, this new policy returns an action that maximizes <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="Q(s,\cdot )">
  <semantics>
    <mrow>
      <mi>Q</mi>
      <mo stretchy="false">(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <mo>&#x22C5;<!-- ⋅ --></mo>
      <mo stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">Q(s,\cdot )</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/47f7ac9178480482628a0ec516807d3a86b38e28" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:6.419ex; height:2.843ex;" alt="Q(s,\cdot )"></span>. In practice <a href="/wiki/Lazy_evaluation" title="Lazy evaluation">lazy evaluation</a> can defer the computation of the maximizing actions to when they are needed.
</p><p>Problems with this procedure include:
</p><p>1. The procedure may spend too much time evaluating a suboptimal policy.
</p><p>2. It uses samples inefficiently in that a long trajectory improves the estimate only of the <i>single</i> state-action pair that started the trajectory.
</p><p>3. When the returns along the trajectories have <i>high variance</i>, convergence is slow.
</p><p>4. It works in <u>episodic problems</u> only.
</p><p>5. It works in small, finite MDPs only.
</p>
<h4><span class="mw-headline" id="Temporal_difference_methods">Temporal difference methods</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=10" title="Edit section: Temporal difference methods">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1033289096"><div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference learning</a></div>
<p>The first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of <i>generalized policy iteration</i> algorithms. Many <i>actor-critic</i> methods belong to this category.
</p><p>The second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton's <a href="/wiki/Temporal_difference" class="mw-redirect" title="Temporal difference">temporal difference</a> (TD) methods that are based on the recursive <a href="/wiki/Bellman_equation" title="Bellman equation">Bellman equation</a>.<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup><sup id="cite_ref-FOOTNOTESuttonBarto2018&#91;httpincompleteideasnetsuttonbookebooknode60html_§6._Temporal-Difference_Learning&#93;_14-0" class="reference"><a href="#cite_note-FOOTNOTESuttonBarto2018[httpincompleteideasnetsuttonbookebooknode60html_§6._Temporal-Difference_Learning]-14">&#91;14&#93;</a></sup> The computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method,<sup id="cite_ref-15" class="reference"><a href="#cite_note-15">&#91;15&#93;</a></sup> may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.
</p><p>Another problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\lambda ">
  <semantics>
    <mi>&#x03BB;<!-- λ --></mi>
    <annotation encoding="application/x-tex">\lambda</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e888c0a0c21333e7e9ed2c930894c1bdbe9f7cfc" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.355ex; height:2.176ex;" alt="\lambda "></span> parameter <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="(0\leq \lambda \leq 1)">
  <semantics>
    <mrow>
      <mo stretchy="false">(</mo>
      <mn>0</mn>
      <mo>&#x2264;<!-- ≤ --></mo>
      <mi>&#x03BB;<!-- λ --></mi>
      <mo>&#x2264;<!-- ≤ --></mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">(0\leq \lambda \leq 1)</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/61749e6bd2c519851013f33833d3971b81a61c4b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:11.686ex; height:2.843ex;" alt="(0\leq \lambda \leq 1)"></span> that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.
</p>
<h4><span class="mw-headline" id="Function_approximation_methods">Function approximation methods</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=11" title="Edit section: Function approximation methods">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>In order to address the fifth issue, <i>function approximation methods</i> are used. <i>Linear function approximation</i> starts with a mapping <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\phi ">
  <semantics>
    <mi>&#x03D5;<!-- ϕ --></mi>
    <annotation encoding="application/x-tex">\phi</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/676831a76463c1c14474cd2502135c44226d7c24" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:1.385ex; height:2.509ex;" alt="\phi "></span> that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="(s,a)">
  <semantics>
    <mrow>
      <mo stretchy="false">(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">(s,a)</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f6cfb5df21f3530ff1710982e3e0107ceb46a274" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.164ex; height:2.843ex;" alt="(s,a)"></span> are obtained by linearly combining the components of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\phi (s,a)">
  <semantics>
    <mrow>
      <mi>&#x03D5;<!-- ϕ --></mi>
      <mo stretchy="false">(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\phi (s,a)</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5b0d53769fab9fbfff22cde80cda23c0d96c295d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:6.549ex; height:2.843ex;" alt="\phi (s,a)"></span> with some <i>weights</i> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\theta ">
  <semantics>
    <mi>&#x03B8;<!-- θ --></mi>
    <annotation encoding="application/x-tex">\theta</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/528115c670a13eca2cc38e7c5ae283edd1026b06" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:2.176ex;" alt="\theta "></span>:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle Q(s,a)=\sum _{i=1}^{d}\theta _{i}\phi _{i}(s,a).}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>Q</mi>
        <mo stretchy="false">(</mo>
        <mi>s</mi>
        <mo>,</mo>
        <mi>a</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </munderover>
        <msub>
          <mi>&#x03B8;<!-- θ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <msub>
          <mi>&#x03D5;<!-- ϕ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>s</mi>
        <mo>,</mo>
        <mi>a</mi>
        <mo stretchy="false">)</mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle Q(s,a)=\sum _{i=1}^{d}\theta _{i}\phi _{i}(s,a).}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b8fb9c17e9850dfa123aac5cf0541b629df47de7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:23.728ex; height:7.343ex;" alt="{\displaystyle Q(s,a)=\sum _{i=1}^{d}\theta _{i}\phi _{i}(s,a).}"></span></dd></dl>
<p>The algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from <a href="/wiki/Nonparametric_statistics" title="Nonparametric statistics">nonparametric statistics</a> (which can be seen to construct their own features) have been explored.
</p><p>Value iteration can also be used as a starting point, giving rise to the <a href="/wiki/Q-learning" title="Q-learning">Q-learning</a> algorithm and its many variants.<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">&#91;16&#93;</a></sup> Including Deep Q-learning methods when a neural network is used to represent Q, with various applications in stochastic search problems.<sup id="cite_ref-MBK_17-0" class="reference"><a href="#cite_note-MBK-17">&#91;17&#93;</a></sup>
</p><p>The problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency.
</p>
<h3><span class="mw-headline" id="Direct_policy_search">Direct policy search</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=12" title="Edit section: Direct policy search">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>An alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of <a href="/wiki/Stochastic_optimization" title="Stochastic optimization">stochastic optimization</a>. The two approaches available are gradient-based and gradient-free methods.
</p><p><a href="/wiki/Gradient" title="Gradient">Gradient</a>-based methods (<i>policy gradient methods</i>) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\theta ">
  <semantics>
    <mi>&#x03B8;<!-- θ --></mi>
    <annotation encoding="application/x-tex">\theta</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/528115c670a13eca2cc38e7c5ae283edd1026b06" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:2.176ex;" alt="\theta "></span>, let <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\pi _{\theta }">
  <semantics>
    <msub>
      <mi>&#x03C0;<!-- π --></mi>
      <mrow class="MJX-TeXAtom-ORD">
        <mi>&#x03B8;<!-- θ --></mi>
      </mrow>
    </msub>
    <annotation encoding="application/x-tex">\pi _{\theta }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b50d10da694176a6960e299914d1d647e7e254f6" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.328ex; height:2.009ex;" alt="\pi _{\theta }"></span> denote the policy associated to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\theta ">
  <semantics>
    <mi>&#x03B8;<!-- θ --></mi>
    <annotation encoding="application/x-tex">\theta</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/528115c670a13eca2cc38e7c5ae283edd1026b06" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:2.176ex;" alt="\theta "></span>. Defining the performance function by
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \rho (\theta )=\rho ^{\pi _{\theta }},}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C1;<!-- ρ --></mi>
        <mo stretchy="false">(</mo>
        <mi>&#x03B8;<!-- θ --></mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <msup>
          <mi>&#x03C1;<!-- ρ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>&#x03C0;<!-- π --></mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>&#x03B8;<!-- θ --></mi>
              </mrow>
            </msub>
          </mrow>
        </msup>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \rho (\theta )=\rho ^{\pi _{\theta }},}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1c2c7d9ba9122778b0d21ec7e71e40dca6583807" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:11.008ex; height:2.843ex;" alt="{\displaystyle \rho (\theta )=\rho ^{\pi _{\theta }},}"></span></dd></dl>
<p>under mild conditions this function will be differentiable as a function of the parameter vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\theta ">
  <semantics>
    <mi>&#x03B8;<!-- θ --></mi>
    <annotation encoding="application/x-tex">\theta</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/528115c670a13eca2cc38e7c5ae283edd1026b06" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:2.176ex;" alt="\theta "></span>. If the gradient of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="\rho ">
  <semantics>
    <mi>&#x03C1;<!-- ρ --></mi>
    <annotation encoding="application/x-tex">\rho</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/27206924d7be95a50654b89b25c13dec3617ee7c" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:1.202ex; height:2.176ex;" alt="\rho "></span> was known, one could use <a href="/wiki/Gradient_descent" title="Gradient descent">gradient ascent</a>. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams' REINFORCE method<sup id="cite_ref-18" class="reference"><a href="#cite_note-18">&#91;18&#93;</a></sup> (which is known as the likelihood ratio method in the <a href="/wiki/Simulation-based_optimization" title="Simulation-based optimization">simulation-based optimization</a> literature).<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">&#91;19&#93;</a></sup> Policy search methods have been used in the <a href="/wiki/Robotics" title="Robotics">robotics</a> context.<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">&#91;20&#93;</a></sup> Many policy search methods may get stuck in local optima (as they are based on <a href="/wiki/Local_search_(optimization)" title="Local search (optimization)">local search</a>).
</p><p>A large class of methods avoids relying on gradient information. These include <a href="/wiki/Simulated_annealing" title="Simulated annealing">simulated annealing</a>, <a href="/wiki/Cross-entropy_method" title="Cross-entropy method">cross-entropy search</a> or methods of <a href="/wiki/Evolutionary_computation" title="Evolutionary computation">evolutionary computation</a>. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.
</p><p>Policy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years, <i>actor–critic methods</i> have been proposed and performed well on various problems.<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">&#91;21&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Model-based_algorithms">Model-based algorithms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=13" title="Edit section: Model-based algorithms">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Finally, all of the above methods can be combined with algorithms that first learn a model.  For instance, the Dyna algorithm<sup id="cite_ref-22" class="reference"><a href="#cite_note-22">&#91;22&#93;</a></sup> learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions.  Such methods can sometimes be extended to use of non-parametric models, such as when the transitions are simply stored and 'replayed'<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">&#91;23&#93;</a></sup> to the learning algorithm.
</p><p>There are other ways to use models than to update a value function.<sup id="cite_ref-24" class="reference"><a href="#cite_note-24">&#91;24&#93;</a></sup> For instance, in <a href="/wiki/Model_predictive_control" title="Model predictive control">model predictive control</a> the model is used to update the behavior directly.
</p>
<h2><span class="mw-headline" id="Theory">Theory</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=14" title="Edit section: Theory">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Both the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.
</p><p>Efficient exploration of MDPs is given in  Burnetas and Katehakis (1997).<sup id="cite_ref-Optimal_adaptive_policies_for_Marko_10-1" class="reference"><a href="#cite_note-Optimal_adaptive_policies_for_Marko-10">&#91;10&#93;</a></sup> Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.
</p><p>For incremental algorithms, asymptotic convergence issues have been settled<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="What are the issues that have been settled? (January 2020)">clarification needed</span></a></i>&#93;</sup>. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).
</p>
<h2><span class="mw-headline" id="Research">Research</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=15" title="Edit section: Research">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1097763485">.mw-parser-output .ambox{border:1px solid #a2a9b1;border-left:10px solid #36c;background-color:#fbfbfb;box-sizing:border-box}.mw-parser-output .ambox+link+.ambox,.mw-parser-output .ambox+link+style+.ambox,.mw-parser-output .ambox+link+link+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+style+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+link+.ambox{margin-top:-1px}html body.mediawiki .mw-parser-output .ambox.mbox-small-left{margin:4px 1em 4px 0;overflow:hidden;width:238px;border-collapse:collapse;font-size:88%;line-height:1.25em}.mw-parser-output .ambox-speedy{border-left:10px solid #b32424;background-color:#fee7e6}.mw-parser-output .ambox-delete{border-left:10px solid #b32424}.mw-parser-output .ambox-content{border-left:10px solid #f28500}.mw-parser-output .ambox-style{border-left:10px solid #fc3}.mw-parser-output .ambox-move{border-left:10px solid #9932cc}.mw-parser-output .ambox-protection{border-left:10px solid #a2a9b1}.mw-parser-output .ambox .mbox-text{border:none;padding:0.25em 0.5em;width:100%}.mw-parser-output .ambox .mbox-image{border:none;padding:2px 0 2px 0.5em;text-align:center}.mw-parser-output .ambox .mbox-imageright{border:none;padding:2px 0.5em 2px 0;text-align:center}.mw-parser-output .ambox .mbox-empty-cell{border:none;padding:0;width:1px}.mw-parser-output .ambox .mbox-image-div{width:52px}html.client-js body.skin-minerva .mw-parser-output .mbox-text-span{margin-left:23px!important}@media(min-width:720px){.mw-parser-output .ambox{margin:0 10%}}</style><table class="box-More_citations_needed_section plainlinks metadata ambox ambox-content ambox-Refimprove" role="presentation"><tbody><tr><td class="mbox-image"><div class="mbox-image-div"><span typeof="mw:File"><a href="/wiki/File:Question_book-new.svg" class="mw-file-description"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png" decoding="async" width="50" height="39" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x" data-file-width="512" data-file-height="399" /></a></span></div></td><td class="mbox-text"><div class="mbox-text-span">This section <b>needs additional citations for <a href="/wiki/Wikipedia:Verifiability" title="Wikipedia:Verifiability">verification</a></b>.<span class="hide-when-compact"> Please help <a class="external text" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit">improve this article</a> by <a href="/wiki/Help:Referencing_for_beginners" title="Help:Referencing for beginners">adding citations to reliable sources</a>&#32;in this section. Unsourced material may be challenged and removed.</span>  <span class="date-container"><i>(<span class="date">October 2022</span>)</i></span><span class="hide-when-compact"><i> (<small><a href="/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a></small>)</i></span></div></td></tr></tbody></table>
<p>Research topics include: 
</p>
<ul><li>actor-critic</li>
<li>adaptive methods that work with fewer (or no) parameters under a large number of conditions</li>
<li>bug detection in software projects<sup id="cite_ref-25" class="reference"><a href="#cite_note-25">&#91;25&#93;</a></sup></li>
<li>continuous learning</li>
<li>combinations with logic-based frameworks<sup id="cite_ref-26" class="reference"><a href="#cite_note-26">&#91;26&#93;</a></sup></li>
<li>exploration in large MDPs</li>
<li><a href="/wiki/Reinforcement_learning_from_human_feedback" title="Reinforcement learning from human feedback">human feedback</a><sup id="cite_ref-27" class="reference"><a href="#cite_note-27">&#91;27&#93;</a></sup></li>
<li>interaction between implicit and explicit learning in skill acquisition</li>
<li><a href="/wiki/Intrinsic_motivation_(artificial_intelligence)" title="Intrinsic motivation (artificial intelligence)">intrinsic motivation</a> which differentiates information-seeking, curiosity-type behaviours from task-dependent goal-directed behaviours large-scale empirical evaluations</li>
<li>large (or continuous) action spaces</li>
<li>modular and hierarchical reinforcement learning<sup id="cite_ref-28" class="reference"><a href="#cite_note-28">&#91;28&#93;</a></sup></li>
<li>multiagent/distributed reinforcement learning is a topic of interest. Applications are expanding.<sup id="cite_ref-29" class="reference"><a href="#cite_note-29">&#91;29&#93;</a></sup></li>
<li>occupant-centric control</li>
<li>optimization of computing resources<sup id="cite_ref-30" class="reference"><a href="#cite_note-30">&#91;30&#93;</a></sup><sup id="cite_ref-31" class="reference"><a href="#cite_note-31">&#91;31&#93;</a></sup><sup id="cite_ref-32" class="reference"><a href="#cite_note-32">&#91;32&#93;</a></sup></li>
<li><a href="/wiki/Partially_observable_Markov_decision_process" title="Partially observable Markov decision process">partial information</a> (e.g., using <a href="/wiki/Predictive_state_representation" title="Predictive state representation">predictive state representation</a>)</li>
<li>reward function based on maximising novel information<sup id="cite_ref-kaplan2004_33-0" class="reference"><a href="#cite_note-kaplan2004-33">&#91;33&#93;</a></sup><sup id="cite_ref-klyubin2008_34-0" class="reference"><a href="#cite_note-klyubin2008-34">&#91;34&#93;</a></sup><sup id="cite_ref-barto2013_35-0" class="reference"><a href="#cite_note-barto2013-35">&#91;35&#93;</a></sup></li>
<li>sample-based planning (e.g., based on <a href="/wiki/Monte_Carlo_tree_search" title="Monte Carlo tree search">Monte Carlo tree search</a>).</li>
<li>securities trading<sup id="cite_ref-36" class="reference"><a href="#cite_note-36">&#91;36&#93;</a></sup></li>
<li><a href="/wiki/Transfer_learning" title="Transfer learning">transfer learning</a><sup id="cite_ref-37" class="reference"><a href="#cite_note-37">&#91;37&#93;</a></sup></li>
<li>TD learning modeling <a href="/wiki/Dopamine" title="Dopamine">dopamine</a>-based learning in the brain. <a href="/wiki/Dopaminergic" title="Dopaminergic">Dopaminergic</a> projections from the <a href="/wiki/Substantia_nigra" title="Substantia nigra">substantia nigra</a> to the <a href="/wiki/Basal_ganglia" title="Basal ganglia">basal ganglia</a> function are the prediction error.</li>
<li>value-function and policy search methods</li></ul>
<h2><span class="mw-headline" id="Comparison_of_reinforcement_learning_algorithms">Comparison of reinforcement learning algorithms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=16" title="Edit section: Comparison of reinforcement learning algorithms">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<table class="wikitable sortable">

<tbody><tr>
<th>Algorithm</th>
<th>Description</th>
<th>Policy</th>
<th>Action space</th>
<th>State space</th>
<th>Operator
</th></tr>
<tr>
<td><a href="/wiki/Monte_Carlo_method" title="Monte Carlo method">Monte Carlo</a></td>
<td>Every visit to Monte Carlo</td>
<td>Either</td>
<td>Discrete</td>
<td>Discrete</td>
<td>Sample-means
</td></tr>
<tr>
<td><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></td>
<td>State–action–reward–state</td>
<td>Off-policy</td>
<td>Discrete</td>
<td>Discrete</td>
<td>Q-value
</td></tr>
<tr>
<td><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></td>
<td>State–action–reward–state–action</td>
<td>On-policy</td>
<td>Discrete</td>
<td>Discrete</td>
<td>Q-value
</td></tr>
<tr>
<td><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a> - Lambda</td>
<td>State–action–reward–state with eligibility traces</td>
<td>Off-policy</td>
<td>Discrete</td>
<td>Discrete</td>
<td>Q-value
</td></tr>
<tr>
<td><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a> - Lambda</td>
<td>State–action–reward–state–action with eligibility traces</td>
<td>On-policy</td>
<td>Discrete</td>
<td>Discrete</td>
<td>Q-value
</td></tr>
<tr>
<td><a href="/wiki/Q-learning#Deep_Q-learning" title="Q-learning">DQN</a></td>
<td>Deep Q Network</td>
<td>Off-policy</td>
<td>Discrete</td>
<td>Continuous</td>
<td>Q-value
</td></tr>
<tr>
<td>DDPG</td>
<td>Deep Deterministic Policy Gradient</td>
<td>Off-policy</td>
<td>Continuous</td>
<td>Continuous</td>
<td>Q-value
</td></tr>
<tr>
<td>A3C</td>
<td>Asynchronous Advantage Actor-Critic Algorithm</td>
<td>On-policy</td>
<td>Continuous</td>
<td>Continuous</td>
<td>Advantage
</td></tr>
<tr>
<td>NAF</td>
<td>Q-Learning with Normalized Advantage Functions</td>
<td>Off-policy</td>
<td>Continuous</td>
<td>Continuous</td>
<td>Advantage
</td></tr>
<tr>
<td>TRPO</td>
<td>Trust Region Policy Optimization</td>
<td>On-policy</td>
<td>Continuous or Discrete</td>
<td>Continuous</td>
<td>Advantage
</td></tr>
<tr>
<td><a href="/wiki/Proximal_Policy_Optimization" title="Proximal Policy Optimization">PPO</a></td>
<td>Proximal Policy Optimization</td>
<td>On-policy</td>
<td>Continuous or Discrete</td>
<td>Continuous</td>
<td>Advantage
</td></tr>
<tr>
<td>TD3
</td>
<td>Twin Delayed Deep Deterministic Policy Gradient
</td>
<td>Off-policy
</td>
<td>Continuous
</td>
<td>Continuous
</td>
<td>Q-value
</td></tr>
<tr>
<td>SAC
</td>
<td>Soft Actor-Critic
</td>
<td>Off-policy
</td>
<td>Continuous
</td>
<td>Continuous
</td>
<td>Advantage
</td></tr></tbody></table>
<h3><span class="mw-headline" id="Associative_reinforcement_learning">Associative reinforcement learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=17" title="Edit section: Associative reinforcement learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Associative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. In associative reinforcement learning tasks, the learning system interacts in a closed loop with its environment.<sup id="cite_ref-38" class="reference"><a href="#cite_note-38">&#91;38&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Deep_reinforcement_learning">Deep reinforcement learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=18" title="Edit section: Deep reinforcement learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>This approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space.<sup id="cite_ref-intro_deep_RL_39-0" class="reference"><a href="#cite_note-intro_deep_RL-39">&#91;39&#93;</a></sup> The work on learning ATARI games by Google <a href="/wiki/DeepMind" class="mw-redirect" title="DeepMind">DeepMind</a> increased attention to <a href="/wiki/Deep_reinforcement_learning" title="Deep reinforcement learning">deep reinforcement learning</a> or <a href="/wiki/End-to-end_reinforcement_learning" class="mw-redirect" title="End-to-end reinforcement learning">end-to-end reinforcement learning</a>.<sup id="cite_ref-DQN2_40-0" class="reference"><a href="#cite_note-DQN2-40">&#91;40&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Adversarial_deep_reinforcement_learning">Adversarial deep reinforcement learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=19" title="Edit section: Adversarial deep reinforcement learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Adversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations.<sup id="cite_ref-41" class="reference"><a href="#cite_note-41">&#91;41&#93;</a></sup><sup id="cite_ref-42" class="reference"><a href="#cite_note-42">&#91;42&#93;</a></sup><sup id="cite_ref-43" class="reference"><a href="#cite_note-43">&#91;43&#93;</a></sup> While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies.<sup id="cite_ref-44" class="reference"><a href="#cite_note-44">&#91;44&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Fuzzy_reinforcement_learning">Fuzzy reinforcement learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=20" title="Edit section: Fuzzy reinforcement learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>By introducing <a href="/wiki/Fuzzy_control_system" title="Fuzzy control system">fuzzy inference</a> in RL,<sup id="cite_ref-45" class="reference"><a href="#cite_note-45">&#91;45&#93;</a></sup> approximating the state-action value function with <a href="/wiki/Fuzzy_rule" title="Fuzzy rule">fuzzy rules</a> in continuous space becomes possible. The IF - THEN form of fuzzy rules make this approach suitable for expressing the results in a form close to natural language. Extending FRL with Fuzzy Rule Interpolation <sup id="cite_ref-46" class="reference"><a href="#cite_note-46">&#91;46&#93;</a></sup> allows the use of reduced size sparse fuzzy rule-bases to emphasize cardinal rules (most important state-action values).
</p>
<h3><span class="mw-headline" id="Inverse_reinforcement_learning">Inverse reinforcement learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=21" title="Edit section: Inverse reinforcement learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal.<sup id="cite_ref-47" class="reference"><a href="#cite_note-47">&#91;47&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Safe_reinforcement_learning">Safe reinforcement learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=22" title="Edit section: Safe reinforcement learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Safe reinforcement learning (SRL) can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes.<sup id="cite_ref-48" class="reference"><a href="#cite_note-48">&#91;48&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=23" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1147244281">.mw-parser-output .div-col{margin-top:0.3em;column-width:30em}.mw-parser-output .div-col-small{font-size:90%}.mw-parser-output .div-col-rules{column-rule:1px solid #aaa}.mw-parser-output .div-col dl,.mw-parser-output .div-col ol,.mw-parser-output .div-col ul{margin-top:0}.mw-parser-output .div-col li,.mw-parser-output .div-col dd{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .plainlist ol,.mw-parser-output .plainlist ul{line-height:inherit;list-style:none;margin:0}.mw-parser-output .plainlist ol li,.mw-parser-output .plainlist ul li{margin-bottom:0}</style><div class="div-col" style="column-width: 20em;">
<ul><li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference learning</a></li>
<li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">State–action–reward–state–action</a> (SARSA)</li>
<li><a href="/wiki/Reinforcement_learning_from_human_feedback" title="Reinforcement learning from human feedback">Reinforcement learning from human feedback</a></li>
<li><a href="/wiki/Fictitious_play" title="Fictitious play">Fictitious play</a></li>
<li><a href="/wiki/Learning_classifier_system" title="Learning classifier system">Learning classifier system</a></li>
<li><a href="/wiki/Optimal_control" title="Optimal control">Optimal control</a></li>
<li><a href="/wiki/Dynamic_treatment_regimes" class="mw-redirect" title="Dynamic treatment regimes">Dynamic treatment regimes</a></li>
<li><a href="/wiki/Error-driven_learning" title="Error-driven learning">Error-driven learning</a></li>
<li><a href="/wiki/Multi-agent_reinforcement_learning" title="Multi-agent reinforcement learning">Multi-agent reinforcement learning</a></li>
<li><a href="/wiki/Multi-agent_system" title="Multi-agent system">Multi-agent system</a></li>
<li><a href="/wiki/Distributed_artificial_intelligence" title="Distributed artificial intelligence">Distributed artificial intelligence</a></li>
<li><a href="/wiki/Intrinsic_motivation_(artificial_intelligence)" title="Intrinsic motivation (artificial intelligence)">Intrinsic motivation</a></li>
<li><a href="/wiki/Genetic_algorithm" title="Genetic algorithm">Genetic algorithms</a></li>
<li><a href="/wiki/Apprenticeship_learning" title="Apprenticeship learning">Apprenticeship learning</a></li>
<li><a href="/wiki/Model-free_(reinforcement_learning)" title="Model-free (reinforcement learning)">Model-free (reinforcement learning)</a></li></ul>
</div>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=24" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1011085734">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class="reflist reflist-columns references-column-width" style="column-width: 30em;">
<ol class="references">
<li id="cite_note-kaelbling-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-kaelbling_1-0">^</a></b></span> <span class="reference-text"><style data-mw-deduplicate="TemplateStyles:r1133582631">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style><cite id="CITEREFKaelblingLittmanMoore1996" class="citation journal cs1"><a href="/wiki/Leslie_P._Kaelbling" title="Leslie P. Kaelbling">Kaelbling, Leslie P.</a>; <a href="/wiki/Michael_L._Littman" title="Michael L. Littman">Littman, Michael L.</a>; <a href="/w/index.php?title=Andrew_W._Moore&amp;action=edit&amp;redlink=1" class="new" title="Andrew W. Moore (page does not exist)">Moore, Andrew W.</a> (1996). <a rel="nofollow" class="external text" href="http://webarchive.loc.gov/all/20011120234539/http://www.cs.washington.edu/research/jair/abstracts/kaelbling96a.html">"Reinforcement Learning: A Survey"</a>. <i>Journal of Artificial Intelligence Research</i>. <b>4</b>: 237–285. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/cs/9605103">cs/9605103</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1613%2Fjair.301">10.1613/jair.301</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:1708582">1708582</a>. Archived from <a rel="nofollow" class="external text" href="http://www.cs.washington.edu/research/jair/abstracts/kaelbling96a.html">the original</a> on 2001-11-20.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Artificial+Intelligence+Research&amp;rft.atitle=Reinforcement+Learning%3A+A+Survey&amp;rft.volume=4&amp;rft.pages=237-285&amp;rft.date=1996&amp;rft_id=info%3Aarxiv%2Fcs%2F9605103&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A1708582%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1613%2Fjair.301&amp;rft.aulast=Kaelbling&amp;rft.aufirst=Leslie+P.&amp;rft.au=Littman%2C+Michael+L.&amp;rft.au=Moore%2C+Andrew+W.&amp;rft_id=http%3A%2F%2Fwww.cs.washington.edu%2Fresearch%2Fjair%2Fabstracts%2Fkaelbling96a.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFvan_Otterlo,_M.Wiering,_M.2012" class="citation book cs1">van Otterlo, M.; Wiering, M. (2012). <i>Reinforcement learning and markov decision processes</i>. <i>Reinforcement Learning</i>. Adaptation, Learning, and Optimization. Vol.&#160;12. pp.&#160;3–42. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2F978-3-642-27645-3_1">10.1007/978-3-642-27645-3_1</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-642-27644-6" title="Special:BookSources/978-3-642-27644-6"><bdi>978-3-642-27644-6</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Reinforcement+learning+and+markov+decision+processes&amp;rft.series=Adaptation%2C+Learning%2C+and+Optimization&amp;rft.pages=3-42&amp;rft.date=2012&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-27645-3_1&amp;rft.isbn=978-3-642-27644-6&amp;rft.au=van+Otterlo%2C+M.&amp;rft.au=Wiering%2C+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFRussellNorvig2010" class="citation book cs1">Russell, Stuart J.; Norvig, Peter (2010). <i>Artificial intelligence&#160;: a modern approach</i> (Third&#160;ed.). Upper Saddle River, New Jersey. pp.&#160;830, 831. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-13-604259-4" title="Special:BookSources/978-0-13-604259-4"><bdi>978-0-13-604259-4</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Artificial+intelligence+%3A+a+modern+approach&amp;rft.place=Upper+Saddle+River%2C+New+Jersey&amp;rft.pages=830%2C+831&amp;rft.edition=Third&amp;rft.date=2010&amp;rft.isbn=978-0-13-604259-4&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart+J.&amp;rft.au=Norvig%2C+Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFLeeSeoJung2012" class="citation journal cs1">Lee, Daeyeol; Seo, Hyojung; Jung, Min Whan (21 July 2012). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3490621">"Neural Basis of Reinforcement Learning and Decision Making"</a>. <i>Annual Review of Neuroscience</i>. <b>35</b> (1): 287–308. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1146%2Fannurev-neuro-062111-150512">10.1146/annurev-neuro-062111-150512</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3490621">3490621</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/22462543">22462543</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annual+Review+of+Neuroscience&amp;rft.atitle=Neural+Basis+of+Reinforcement+Learning+and+Decision+Making&amp;rft.volume=35&amp;rft.issue=1&amp;rft.pages=287-308&amp;rft.date=2012-07-21&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3490621%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F22462543&amp;rft_id=info%3Adoi%2F10.1146%2Fannurev-neuro-062111-150512&amp;rft.aulast=Lee&amp;rft.aufirst=Daeyeol&amp;rft.au=Seo%2C+Hyojung&amp;rft.au=Jung%2C+Min+Whan&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3490621&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text">Duque, Edgar Mauricio Salazar, Juan S. Giraldo, Pedro P. Vergara, Phuong Nguyen, Anne van der Molen, and Han Slootweg. "Community energy storage operation via reinforcement learning with eligibility traces." Electric Power Systems Research 212 (2022): 108515. <a rel="nofollow" class="external free" href="https://doi.org/10.1016/j.epsr.2022.108515">https://doi.org/10.1016/j.epsr.2022.108515</a></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text">Xie, Zhaoming, et al. "<a rel="nofollow" class="external text" href="https://arxiv.org/pdf/2005.04323">ALLSTEPS: Curriculum‐driven Learning of Stepping Stone Skills</a>." Computer Graphics Forum. Vol. 39. No. 8. 2020.</span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text">Vergara, Pedro P., Mauricio Salazar, Juan S. Giraldo, and Peter Palensky. "Optimal dispatch of PV inverters in unbalanced distribution systems using Reinforcement Learning." International Journal of Electrical Power &amp; Energy Systems 136 (2022): 107628. <a rel="nofollow" class="external free" href="https://doi.org/10.1016/j.ijepes.2021.107628">https://doi.org/10.1016/j.ijepes.2021.107628</a></span>
</li>
<li id="cite_note-FOOTNOTESuttonBarto2018Chapter_11-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-FOOTNOTESuttonBarto2018Chapter_11_8-0">^</a></b></span> <span class="reference-text"><a href="#CITEREFSuttonBarto2018">Sutton &amp; Barto 2018</a>, Chapter 11.</span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFGosavi2003" class="citation book cs1"><a href="/w/index.php?title=Abhijit_Gosavi&amp;action=edit&amp;redlink=1" class="new" title="Abhijit Gosavi (page does not exist)">Gosavi, Abhijit</a> (2003). <a rel="nofollow" class="external text" href="https://www.springer.com/mathematics/applications/book/978-1-4020-7454-7"><i>Simulation-based Optimization: Parametric Optimization Techniques and Reinforcement</i></a>. Operations Research/Computer Science Interfaces Series. Springer. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4020-7454-7" title="Special:BookSources/978-1-4020-7454-7"><bdi>978-1-4020-7454-7</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Simulation-based+Optimization%3A+Parametric+Optimization+Techniques+and+Reinforcement&amp;rft.series=Operations+Research%2FComputer+Science+Interfaces+Series&amp;rft.pub=Springer&amp;rft.date=2003&amp;rft.isbn=978-1-4020-7454-7&amp;rft.aulast=Gosavi&amp;rft.aufirst=Abhijit&amp;rft_id=https%3A%2F%2Fwww.springer.com%2Fmathematics%2Fapplications%2Fbook%2F978-1-4020-7454-7&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-Optimal_adaptive_policies_for_Marko-10"><span class="mw-cite-backlink">^ <a href="#cite_ref-Optimal_adaptive_policies_for_Marko_10-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Optimal_adaptive_policies_for_Marko_10-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBurnetasKatehakis1997" class="citation cs2">Burnetas, Apostolos N.; <a href="/wiki/Michael_N._Katehakis" class="mw-redirect" title="Michael N. Katehakis">Katehakis, Michael N.</a> (1997), "Optimal adaptive policies for Markov Decision Processes", <i>Mathematics of Operations Research</i>, <b>22</b>: 222–255, <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1287%2Fmoor.22.1.222">10.1287/moor.22.1.222</a></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Mathematics+of+Operations+Research&amp;rft.atitle=Optimal+adaptive+policies+for+Markov+Decision+Processes&amp;rft.volume=22&amp;rft.pages=222-255&amp;rft.date=1997&amp;rft_id=info%3Adoi%2F10.1287%2Fmoor.22.1.222&amp;rft.aulast=Burnetas&amp;rft.aufirst=Apostolos+N.&amp;rft.au=Katehakis%2C+Michael+N.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFTokicPalm2011" class="citation cs2">Tokic, Michel; Palm, Günther (2011), <a rel="nofollow" class="external text" href="http://www.tokic.com/www/tokicm/publikationen/papers/KI2011.pdf">"Value-Difference Based Exploration: Adaptive Control Between Epsilon-Greedy and Softmax"</a> <span class="cs1-format">(PDF)</span>, <i>KI 2011: Advances in Artificial Intelligence</i>, Lecture Notes in Computer Science, vol.&#160;7006, Springer, pp.&#160;335–346, <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-642-24455-1" title="Special:BookSources/978-3-642-24455-1"><bdi>978-3-642-24455-1</bdi></a></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Value-Difference+Based+Exploration%3A+Adaptive+Control+Between+Epsilon-Greedy+and+Softmax&amp;rft.btitle=KI+2011%3A+Advances+in+Artificial+Intelligence&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=335-346&amp;rft.pub=Springer&amp;rft.date=2011&amp;rft.isbn=978-3-642-24455-1&amp;rft.aulast=Tokic&amp;rft.aufirst=Michel&amp;rft.au=Palm%2C+G%C3%BCnther&amp;rft_id=http%3A%2F%2Fwww.tokic.com%2Fwww%2Ftokicm%2Fpublikationen%2Fpapers%2FKI2011.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-:0-12"><span class="mw-cite-backlink">^ <a href="#cite_ref-:0_12-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_12-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf">"Reinforcement learning: An introduction"</a> <span class="cs1-format">(PDF)</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Reinforcement+learning%3A+An+introduction&amp;rft_id=http%3A%2F%2Fpeople.inf.elte.hu%2Florincz%2FFiles%2FRL_2006%2FSuttonBook.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSutton1984" class="citation thesis cs1"><a href="/wiki/Richard_S._Sutton" title="Richard S. Sutton">Sutton, Richard S.</a> (1984). <a rel="nofollow" class="external text" href="http://incompleteideas.net/sutton/publications.html#PhDthesis"><i>Temporal Credit Assignment in Reinforcement Learning</i></a> (PhD thesis). University of Massachusetts, Amherst, MA.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adissertation&amp;rft.title=Temporal+Credit+Assignment+in+Reinforcement+Learning&amp;rft.degree=PhD&amp;rft.inst=University+of+Massachusetts%2C+Amherst%2C+MA&amp;rft.date=1984&amp;rft.aulast=Sutton&amp;rft.aufirst=Richard+S.&amp;rft_id=http%3A%2F%2Fincompleteideas.net%2Fsutton%2Fpublications.html%23PhDthesis&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-FOOTNOTESuttonBarto2018&#91;httpincompleteideasnetsuttonbookebooknode60html_§6._Temporal-Difference_Learning&#93;-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-FOOTNOTESuttonBarto2018[httpincompleteideasnetsuttonbookebooknode60html_§6._Temporal-Difference_Learning]_14-0">^</a></b></span> <span class="reference-text"><a href="#CITEREFSuttonBarto2018">Sutton &amp; Barto 2018</a>, <a rel="nofollow" class="external text" href="http://incompleteideas.net/sutton/book/ebook/node60.html">§6. Temporal-Difference Learning</a>.</span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBradtkeBarto1996" class="citation journal cs1"><a href="/w/index.php?title=Steven_J._Bradtke&amp;action=edit&amp;redlink=1" class="new" title="Steven J. Bradtke (page does not exist)">Bradtke, Steven J.</a>; <a href="/wiki/Andrew_G._Barto" class="mw-redirect" title="Andrew G. Barto">Barto, Andrew G.</a> (1996). "Learning to predict by the method of temporal differences". <i>Machine Learning</i>. <b>22</b>: 33–57. <a href="/wiki/CiteSeerX_(identifier)" class="mw-redirect" title="CiteSeerX (identifier)">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.143.857">10.1.1.143.857</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1023%2FA%3A1018056104778">10.1023/A:1018056104778</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:20327856">20327856</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Machine+Learning&amp;rft.atitle=Learning+to+predict+by+the+method+of+temporal+differences&amp;rft.volume=22&amp;rft.pages=33-57&amp;rft.date=1996&amp;rft_id=https%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.143.857%23id-name%3DCiteSeerX&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A20327856%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1023%2FA%3A1018056104778&amp;rft.aulast=Bradtke&amp;rft.aufirst=Steven+J.&amp;rft.au=Barto%2C+Andrew+G.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFWatkins1989" class="citation thesis cs1"><a href="/w/index.php?title=Christopher_J.C.H._Watkins&amp;action=edit&amp;redlink=1" class="new" title="Christopher J.C.H. Watkins (page does not exist)">Watkins, Christopher J.C.H.</a> (1989). <a rel="nofollow" class="external text" href="http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf"><i>Learning from Delayed Rewards</i></a> <span class="cs1-format">(PDF)</span> (PhD thesis). King’s College, Cambridge, UK.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adissertation&amp;rft.title=Learning+from+Delayed+Rewards&amp;rft.degree=PhD&amp;rft.inst=King%E2%80%99s+College%2C+Cambridge%2C+UK&amp;rft.date=1989&amp;rft.aulast=Watkins&amp;rft.aufirst=Christopher+J.C.H.&amp;rft_id=http%3A%2F%2Fwww.cs.rhul.ac.uk%2F~chrisw%2Fnew_thesis.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-MBK-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-MBK_17-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFMatzliachBen-GalKagan2022" class="citation journal cs1">Matzliach, Barouch; Ben-Gal, Irad; Kagan, Evgeny (2022). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9407070">"Detection of Static and Mobile Targets by an Autonomous Agent with Deep Q-Learning Abilities"</a>. <i>Entropy</i>. <b>24</b> (8): 1168. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2022Entrp..24.1168M">2022Entrp..24.1168M</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.3390%2Fe24081168">10.3390/e24081168</a></span>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9407070">9407070</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/36010832">36010832</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Entropy&amp;rft.atitle=Detection+of+Static+and+Mobile+Targets+by+an+Autonomous+Agent+with+Deep+Q-Learning+Abilities&amp;rft.volume=24&amp;rft.issue=8&amp;rft.pages=1168&amp;rft.date=2022&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC9407070%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F36010832&amp;rft_id=info%3Adoi%2F10.3390%2Fe24081168&amp;rft_id=info%3Abibcode%2F2022Entrp..24.1168M&amp;rft.aulast=Matzliach&amp;rft.aufirst=Barouch&amp;rft.au=Ben-Gal%2C+Irad&amp;rft.au=Kagan%2C+Evgeny&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC9407070&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFWilliams1987" class="citation conference cs1"><a href="/wiki/Ronald_J._Williams" title="Ronald J. Williams">Williams, Ronald J.</a> (1987). "A class of gradient-estimating algorithms for reinforcement learning in neural networks". <i>Proceedings of the IEEE First International Conference on Neural Networks</i>. <a href="/wiki/CiteSeerX_(identifier)" class="mw-redirect" title="CiteSeerX (identifier)">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.129.8871">10.1.1.129.8871</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=A+class+of+gradient-estimating+algorithms+for+reinforcement+learning+in+neural+networks&amp;rft.btitle=Proceedings+of+the+IEEE+First+International+Conference+on+Neural+Networks&amp;rft.date=1987&amp;rft_id=https%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.129.8871%23id-name%3DCiteSeerX&amp;rft.aulast=Williams&amp;rft.aufirst=Ronald+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFPetersVijayakumarSchaal2003" class="citation conference cs1"><a href="/w/index.php?title=Jan_Peters_(researcher)&amp;action=edit&amp;redlink=1" class="new" title="Jan Peters (researcher) (page does not exist)">Peters, Jan</a>; <a href="/wiki/Sethu_Vijayakumar" title="Sethu Vijayakumar">Vijayakumar, Sethu</a>; <a href="/wiki/Stefan_Schaal" title="Stefan Schaal">Schaal, Stefan</a> (2003). <a rel="nofollow" class="external text" href="http://www-clmc.usc.edu/publications/p/peters-ICHR2003.pdf">"Reinforcement Learning for Humanoid Robotics"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE-RAS International Conference on Humanoid Robots</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Reinforcement+Learning+for+Humanoid+Robotics&amp;rft.btitle=IEEE-RAS+International+Conference+on+Humanoid+Robots&amp;rft.date=2003&amp;rft.aulast=Peters&amp;rft.aufirst=Jan&amp;rft.au=Vijayakumar%2C+Sethu&amp;rft.au=Schaal%2C+Stefan&amp;rft_id=http%3A%2F%2Fwww-clmc.usc.edu%2Fpublications%2Fp%2Fpeters-ICHR2003.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFDeisenrothNeumannPeters2013" class="citation book cs1"><a href="/w/index.php?title=Marc_Peter_Deisenroth&amp;action=edit&amp;redlink=1" class="new" title="Marc Peter Deisenroth (page does not exist)">Deisenroth, Marc Peter</a>; <a href="/wiki/Gerhard_Neumann" title="Gerhard Neumann">Neumann, Gerhard</a>; <a href="/w/index.php?title=Jan_Peters_(researcher)&amp;action=edit&amp;redlink=1" class="new" title="Jan Peters (researcher) (page does not exist)">Peters, Jan</a> (2013). <a rel="nofollow" class="external text" href="http://eprints.lincoln.ac.uk/28029/1/PolicySearchReview.pdf"><i>A Survey on Policy Search for Robotics</i></a> <span class="cs1-format">(PDF)</span>. Foundations and Trends in Robotics. Vol.&#160;2. NOW Publishers. pp.&#160;1–142. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1561%2F2300000021">10.1561/2300000021</a>. <a href="/wiki/Hdl_(identifier)" class="mw-redirect" title="Hdl (identifier)">hdl</a>:<a rel="nofollow" class="external text" href="https://hdl.handle.net/10044%2F1%2F12051">10044/1/12051</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=A+Survey+on+Policy+Search+for+Robotics&amp;rft.series=Foundations+and+Trends+in+Robotics&amp;rft.pages=1-142&amp;rft.pub=NOW+Publishers&amp;rft.date=2013&amp;rft_id=info%3Ahdl%2F10044%2F1%2F12051&amp;rft_id=info%3Adoi%2F10.1561%2F2300000021&amp;rft.aulast=Deisenroth&amp;rft.aufirst=Marc+Peter&amp;rft.au=Neumann%2C+Gerhard&amp;rft.au=Peters%2C+Jan&amp;rft_id=http%3A%2F%2Feprints.lincoln.ac.uk%2F28029%2F1%2FPolicySearchReview.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFJuliani2016" class="citation web cs1">Juliani, Arthur (2016-12-17). <a rel="nofollow" class="external text" href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2">"Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)"</a>. <i>Medium</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2018-02-22</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Medium&amp;rft.atitle=Simple+Reinforcement+Learning+with+Tensorflow+Part+8%3A+Asynchronous+Actor-Critic+Agents+%28A3C%29&amp;rft.date=2016-12-17&amp;rft.aulast=Juliani&amp;rft.aufirst=Arthur&amp;rft_id=https%3A%2F%2Fmedium.com%2Femergent-future%2Fsimple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSutton1990" class="citation conference cs1">Sutton, Richard (1990). "Integrated Architectures for Learning, Planning and Reacting based on Dynamic Programming". <i>Machine Learning: Proceedings of the Seventh International Workshop</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Integrated+Architectures+for+Learning%2C+Planning+and+Reacting+based+on+Dynamic+Programming&amp;rft.btitle=Machine+Learning%3A+Proceedings+of+the+Seventh+International+Workshop&amp;rft.date=1990&amp;rft.aulast=Sutton&amp;rft.aufirst=Richard&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFLin1992" class="citation conference cs1">Lin, Long-Ji (1992). <a rel="nofollow" class="external text" href="https://link.springer.com/content/pdf/10.1007/BF00992699.pdf">"Self-improving reactive agents based on reinforcement learning, planning and teaching"</a> <span class="cs1-format">(PDF)</span>. <i>Machine Learning volume 8</i>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2FBF00992699">10.1007/BF00992699</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Self-improving+reactive+agents+based+on+reinforcement+learning%2C+planning+and+teaching&amp;rft.btitle=Machine+Learning+volume+8&amp;rft.date=1992&amp;rft_id=info%3Adoi%2F10.1007%2FBF00992699&amp;rft.aulast=Lin&amp;rft.aufirst=Long-Ji&amp;rft_id=https%3A%2F%2Flink.springer.com%2Fcontent%2Fpdf%2F10.1007%2FBF00992699.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFvan_HasseltHesselAslanides2019" class="citation conference cs1">van Hasselt, Hado; Hessel, Matteo; Aslanides, John (2019). <a rel="nofollow" class="external text" href="https://proceedings.neurips.cc/paper/2019/file/1b742ae215adf18b75449c6e272fd92d-Paper.pdf">"When to use parametric models in reinforcement learning?"</a> <span class="cs1-format">(PDF)</span>. <i>Advances in Neural Information Processing Systems 32</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=When+to+use+parametric+models+in+reinforcement+learning%3F&amp;rft.btitle=Advances+in+Neural+Information+Processing+Systems+32&amp;rft.date=2019&amp;rft.aulast=van+Hasselt&amp;rft.aufirst=Hado&amp;rft.au=Hessel%2C+Matteo&amp;rft.au=Aslanides%2C+John&amp;rft_id=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper%2F2019%2Ffile%2F1b742ae215adf18b75449c6e272fd92d-Paper.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://cie.acm.org/articles/use-reinforcements-learning-testing-game-mechanics/">"On the Use of Reinforcement Learning for Testing Game Mechanics&#160;: ACM - Computers in Entertainment"</a>. <i>cie.acm.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2018-11-27</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=cie.acm.org&amp;rft.atitle=On+the+Use+of+Reinforcement+Learning+for+Testing+Game+Mechanics+%3A+ACM+-+Computers+in+Entertainment&amp;rft_id=https%3A%2F%2Fcie.acm.org%2Farticles%2Fuse-reinforcements-learning-testing-game-mechanics%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFRiveretGao2019" class="citation journal cs1">Riveret, Regis; Gao, Yang (2019). "A probabilistic argumentation framework for reinforcement learning agents". <i>Autonomous Agents and Multi-Agent Systems</i>. <b>33</b> (1–2): 216–274. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs10458-019-09404-2">10.1007/s10458-019-09404-2</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:71147890">71147890</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Autonomous+Agents+and+Multi-Agent+Systems&amp;rft.atitle=A+probabilistic+argumentation+framework+for+reinforcement+learning+agents&amp;rft.volume=33&amp;rft.issue=1%E2%80%932&amp;rft.pages=216-274&amp;rft.date=2019&amp;rft_id=info%3Adoi%2F10.1007%2Fs10458-019-09404-2&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A71147890%23id-name%3DS2CID&amp;rft.aulast=Riveret&amp;rft.aufirst=Regis&amp;rft.au=Gao%2C+Yang&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFYamagataMcConvilleSantos-Rodriguez2021" class="citation arxiv cs1">Yamagata, Taku; McConville, Ryan; Santos-Rodriguez, Raul (2021-11-16). "Reinforcement Learning with Feedback from Multiple Humans with Diverse Skills". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2111.08596">2111.08596</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.LG">cs.LG</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Reinforcement+Learning+with+Feedback+from+Multiple+Humans+with+Diverse+Skills&amp;rft.date=2021-11-16&amp;rft_id=info%3Aarxiv%2F2111.08596&amp;rft.aulast=Yamagata&amp;rft.aufirst=Taku&amp;rft.au=McConville%2C+Ryan&amp;rft.au=Santos-Rodriguez%2C+Raul&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFKulkarniNarasimhanSaeediTenenbaum2016" class="citation journal cs1">Kulkarni, Tejas D.; Narasimhan, Karthik R.; Saeedi, Ardavan; Tenenbaum, Joshua B. (2016). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=3157382.3157509">"Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation"</a>. <i>Proceedings of the 30th International Conference on Neural Information Processing Systems</i>. NIPS'16. USA: Curran Associates Inc.: 3682–3690. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1604.06057">1604.06057</a></span>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2016arXiv160406057K">2016arXiv160406057K</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-5108-3881-9" title="Special:BookSources/978-1-5108-3881-9"><bdi>978-1-5108-3881-9</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+30th+International+Conference+on+Neural+Information+Processing+Systems&amp;rft.atitle=Hierarchical+Deep+Reinforcement+Learning%3A+Integrating+Temporal+Abstraction+and+Intrinsic+Motivation&amp;rft.pages=3682-3690&amp;rft.date=2016&amp;rft_id=info%3Aarxiv%2F1604.06057&amp;rft_id=info%3Abibcode%2F2016arXiv160406057K&amp;rft.isbn=978-1-5108-3881-9&amp;rft.aulast=Kulkarni&amp;rft.aufirst=Tejas+D.&amp;rft.au=Narasimhan%2C+Karthik+R.&amp;rft.au=Saeedi%2C+Ardavan&amp;rft.au=Tenenbaum%2C+Joshua+B.&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D3157382.3157509&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-29">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="http://umichrl.pbworks.com/Successes-of-Reinforcement-Learning/">"Reinforcement Learning / Successes of Reinforcement Learning"</a>. <i>umichrl.pbworks.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-08-06</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=umichrl.pbworks.com&amp;rft.atitle=Reinforcement+Learning+%2F+Successes+of+Reinforcement+Learning&amp;rft_id=http%3A%2F%2Fumichrl.pbworks.com%2FSuccesses-of-Reinforcement-Learning%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFDeySinghWangMcDonald-Maier2020" class="citation journal cs1">Dey, Somdip; Singh, Amit Kumar; Wang, Xiaohang; McDonald-Maier, Klaus (March 2020). <a rel="nofollow" class="external text" href="https://ieeexplore.ieee.org/document/9116294">"User Interaction Aware Reinforcement Learning for Power and Thermal Efficiency of CPU-GPU Mobile MPSoCs"</a>. <i>2020 Design, Automation Test in Europe Conference Exhibition (DATE)</i>: 1728–1733. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.23919%2FDATE48585.2020.9116294">10.23919/DATE48585.2020.9116294</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-9819263-4-7" title="Special:BookSources/978-3-9819263-4-7"><bdi>978-3-9819263-4-7</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:219858480">219858480</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=2020+Design%2C+Automation+Test+in+Europe+Conference+Exhibition+%28DATE%29&amp;rft.atitle=User+Interaction+Aware+Reinforcement+Learning+for+Power+and+Thermal+Efficiency+of+CPU-GPU+Mobile+MPSoCs&amp;rft.pages=1728-1733&amp;rft.date=2020-03&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A219858480%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.23919%2FDATE48585.2020.9116294&amp;rft.isbn=978-3-9819263-4-7&amp;rft.aulast=Dey&amp;rft.aufirst=Somdip&amp;rft.au=Singh%2C+Amit+Kumar&amp;rft.au=Wang%2C+Xiaohang&amp;rft.au=McDonald-Maier%2C+Klaus&amp;rft_id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F9116294&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFQuested" class="citation web cs1">Quested, Tony. <a rel="nofollow" class="external text" href="https://www.businessweekly.co.uk/news/academia-research/smartphones-get-smarter-essex-innovation">"Smartphones get smarter with Essex innovation"</a>. <i>Business Weekly</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2021-06-17</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Business+Weekly&amp;rft.atitle=Smartphones+get+smarter+with+Essex+innovation&amp;rft.aulast=Quested&amp;rft.aufirst=Tony&amp;rft_id=https%3A%2F%2Fwww.businessweekly.co.uk%2Fnews%2Facademia-research%2Fsmartphones-get-smarter-essex-innovation&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span><span class="cs1-maint citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_web" title="Template:Cite web">cite web</a>}}</code>:  CS1 maint: url-status (<a href="/wiki/Category:CS1_maint:_url-status" title="Category:CS1 maint: url-status">link</a>)</span></span>
</li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFWilliams2020" class="citation web cs1">Williams, Rhiannon (2020-07-21). <a rel="nofollow" class="external text" href="https://inews.co.uk/news/technology/future-smartphones-prolong-battery-life-monitoring-behaviour-558689">"Future smartphones 'will prolong their own battery life by monitoring owners' behaviour'<span class="cs1-kern-right"></span>"</a>. <i><a href="/wiki/I_(newspaper)" title="I (newspaper)">i</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">2021-06-17</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=i&amp;rft.atitle=Future+smartphones+%27will+prolong+their+own+battery+life+by+monitoring+owners%27+behaviour%27&amp;rft.date=2020-07-21&amp;rft.aulast=Williams&amp;rft.aufirst=Rhiannon&amp;rft_id=https%3A%2F%2Finews.co.uk%2Fnews%2Ftechnology%2Ffuture-smartphones-prolong-battery-life-monitoring-behaviour-558689&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span><span class="cs1-maint citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_web" title="Template:Cite web">cite web</a>}}</code>:  CS1 maint: url-status (<a href="/wiki/Category:CS1_maint:_url-status" title="Category:CS1 maint: url-status">link</a>)</span></span>
</li>
<li id="cite_note-kaplan2004-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-kaplan2004_33-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFKaplanOudeyer2004" class="citation book cs1">Kaplan, F.; Oudeyer, P. (2004). "Maximizing learning progress: an internal reward system for development".  In Iida, F.; Pfeifer, R.; Steels, L.; Kuniyoshi, Y. (eds.). <i>Embodied Artificial Intelligence</i>. Lecture Notes in Computer Science. Vol.&#160;3139. Berlin; Heidelberg: Springer. pp.&#160;259–270. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2F978-3-540-27833-7_19">10.1007/978-3-540-27833-7_19</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-540-22484-6" title="Special:BookSources/978-3-540-22484-6"><bdi>978-3-540-22484-6</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:9781221">9781221</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Maximizing+learning+progress%3A+an+internal+reward+system+for+development&amp;rft.btitle=Embodied+Artificial+Intelligence&amp;rft.place=Berlin%3B+Heidelberg&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=259-270&amp;rft.pub=Springer&amp;rft.date=2004&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A9781221%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-540-27833-7_19&amp;rft.isbn=978-3-540-22484-6&amp;rft.aulast=Kaplan&amp;rft.aufirst=F.&amp;rft.au=Oudeyer%2C+P.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-klyubin2008-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-klyubin2008_34-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFKlyubinPolaniNehaniv2008" class="citation journal cs1">Klyubin, A.; Polani, D.; Nehaniv, C. (2008). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2607028">"Keep your options open: an information-based driving principle for sensorimotor systems"</a>. <i>PLOS ONE</i>. <b>3</b> (12): e4018. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2008PLoSO...3.4018K">2008PLoSO...3.4018K</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1371%2Fjournal.pone.0004018">10.1371/journal.pone.0004018</a></span>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2607028">2607028</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/19107219">19107219</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=PLOS+ONE&amp;rft.atitle=Keep+your+options+open%3A+an+information-based+driving+principle+for+sensorimotor+systems&amp;rft.volume=3&amp;rft.issue=12&amp;rft.pages=e4018&amp;rft.date=2008&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC2607028%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F19107219&amp;rft_id=info%3Adoi%2F10.1371%2Fjournal.pone.0004018&amp;rft_id=info%3Abibcode%2F2008PLoSO...3.4018K&amp;rft.aulast=Klyubin&amp;rft.aufirst=A.&amp;rft.au=Polani%2C+D.&amp;rft.au=Nehaniv%2C+C.&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC2607028&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-barto2013-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-barto2013_35-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBarto2013" class="citation book cs1">Barto, A. G. (2013). "Intrinsic motivation and reinforcement learning". <a rel="nofollow" class="external text" href="https://people.cs.umass.edu/~barto/IMCleVer-chapter-totypeset2.pdf"><i>Intrinsically Motivated Learning in Natural and Artificial Systems</i></a> <span class="cs1-format">(PDF)</span>. Berlin; Heidelberg: Springer. pp.&#160;17–47.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Intrinsic+motivation+and+reinforcement+learning&amp;rft.btitle=Intrinsically+Motivated+Learning+in+Natural+and+Artificial+Systems&amp;rft.place=Berlin%3B+Heidelberg&amp;rft.pages=17-47&amp;rft.pub=Springer&amp;rft.date=2013&amp;rft.aulast=Barto&amp;rft.aufirst=A.+G.&amp;rft_id=https%3A%2F%2Fpeople.cs.umass.edu%2F~barto%2FIMCleVer-chapter-totypeset2.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-36">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFDabériusGranatKarlsson2020" class="citation journal cs1">Dabérius, Kevin; Granat, Elvin; Karlsson, Patrik (2020). "Deep Execution - Value and Policy Based Reinforcement Learning for Trading and Beating Market Benchmarks". <i>The Journal of Machine Learning in Finance</i>. <b>1</b>. <a href="/wiki/SSRN_(identifier)" class="mw-redirect" title="SSRN (identifier)">SSRN</a>&#160;<a rel="nofollow" class="external text" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3374766">3374766</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Journal+of+Machine+Learning+in+Finance&amp;rft.atitle=Deep+Execution+-+Value+and+Policy+Based+Reinforcement+Learning+for+Trading+and+Beating+Market+Benchmarks&amp;rft.volume=1&amp;rft.date=2020&amp;rft_id=https%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D3374766%23id-name%3DSSRN&amp;rft.aulast=Dab%C3%A9rius&amp;rft.aufirst=Kevin&amp;rft.au=Granat%2C+Elvin&amp;rft.au=Karlsson%2C+Patrik&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-37">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFGeorge_KarimpanalBouffanais2019" class="citation journal cs1">George Karimpanal, Thommen; Bouffanais, Roland (2019). "Self-organizing maps for storage and transfer of knowledge in reinforcement learning". <i>Adaptive Behavior</i>. <b>27</b> (2): 111–126. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1811.08318">1811.08318</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1177%2F1059712318818568">10.1177/1059712318818568</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1059-7123">1059-7123</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:53774629">53774629</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Adaptive+Behavior&amp;rft.atitle=Self-organizing+maps+for+storage+and+transfer+of+knowledge+in+reinforcement+learning&amp;rft.volume=27&amp;rft.issue=2&amp;rft.pages=111-126&amp;rft.date=2019&amp;rft_id=info%3Aarxiv%2F1811.08318&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A53774629%23id-name%3DS2CID&amp;rft.issn=1059-7123&amp;rft_id=info%3Adoi%2F10.1177%2F1059712318818568&amp;rft.aulast=George+Karimpanal&amp;rft.aufirst=Thommen&amp;rft.au=Bouffanais%2C+Roland&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-38">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSoucek1992" class="citation book cs1">Soucek, Branko (6 May 1992). <i>Dynamic, Genetic and Chaotic Programming: The Sixth-Generation Computer Technology Series</i>. John Wiley &amp; Sons, Inc. p.&#160;38. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-471-55717-X" title="Special:BookSources/0-471-55717-X"><bdi>0-471-55717-X</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Dynamic%2C+Genetic+and+Chaotic+Programming%3A+The+Sixth-Generation+Computer+Technology+Series&amp;rft.pages=38&amp;rft.pub=John+Wiley+%26+Sons%2C+Inc&amp;rft.date=1992-05-06&amp;rft.isbn=0-471-55717-X&amp;rft.aulast=Soucek&amp;rft.aufirst=Branko&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-intro_deep_RL-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-intro_deep_RL_39-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFFrancois-Lavet2018" class="citation journal cs1">Francois-Lavet, Vincent;  et&#160;al. (2018). "An Introduction to Deep Reinforcement Learning". <i>Foundations and Trends in Machine Learning</i>. <b>11</b> (3–4): 219–354. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1811.12560">1811.12560</a></span>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2018arXiv181112560F">2018arXiv181112560F</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1561%2F2200000071">10.1561/2200000071</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:54434537">54434537</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Foundations+and+Trends+in+Machine+Learning&amp;rft.atitle=An+Introduction+to+Deep+Reinforcement+Learning&amp;rft.volume=11&amp;rft.issue=3%E2%80%934&amp;rft.pages=219-354&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1811.12560&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A54434537%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1561%2F2200000071&amp;rft_id=info%3Abibcode%2F2018arXiv181112560F&amp;rft.aulast=Francois-Lavet&amp;rft.aufirst=Vincent&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-DQN2-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-DQN2_40-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFMnih2015" class="citation journal cs1">Mnih, Volodymyr;  et&#160;al. (2015). <a rel="nofollow" class="external text" href="https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d">"Human-level control through deep reinforcement learning"</a>. <i>Nature</i>. <b>518</b> (7540): 529–533. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2015Natur.518..529M">2015Natur.518..529M</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fnature14236">10.1038/nature14236</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/25719670">25719670</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:205242740">205242740</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Human-level+control+through+deep+reinforcement+learning&amp;rft.volume=518&amp;rft.issue=7540&amp;rft.pages=529-533&amp;rft.date=2015&amp;rft_id=info%3Adoi%2F10.1038%2Fnature14236&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A205242740%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F25719670&amp;rft_id=info%3Abibcode%2F2015Natur.518..529M&amp;rft.aulast=Mnih&amp;rft.aufirst=Volodymyr&amp;rft_id=https%3A%2F%2Fwww.semanticscholar.org%2Fpaper%2Fe0e9a94c4a6ba219e768b4e59f72c18f0a22e23d&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-41">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFGoodfellowShlensSzegedy2015" class="citation journal cs1">Goodfellow, Ian; Shlens, Jonathan; Szegedy, Christian (2015). "Explaining and Harnessing Adversarial Examples". <i>International Conference on Learning Representations</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1412.6572">1412.6572</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Conference+on+Learning+Representations&amp;rft.atitle=Explaining+and+Harnessing+Adversarial+Examples&amp;rft.date=2015&amp;rft_id=info%3Aarxiv%2F1412.6572&amp;rft.aulast=Goodfellow&amp;rft.aufirst=Ian&amp;rft.au=Shlens%2C+Jonathan&amp;rft.au=Szegedy%2C+Christian&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-42"><span class="mw-cite-backlink"><b><a href="#cite_ref-42">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBehzadanMunir2017" class="citation journal cs1">Behzadan, Vahid; Munir, Arslan (2017). "Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks". <i>International Conference on Machine Learning and Data Mining in Pattern Recognition</i>. Lecture Notes in Computer Science. <b>10358</b>: 262–275. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1701.04143">1701.04143</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2F978-3-319-62416-7_19">10.1007/978-3-319-62416-7_19</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-319-62415-0" title="Special:BookSources/978-3-319-62415-0"><bdi>978-3-319-62415-0</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:1562290">1562290</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Conference+on+Machine+Learning+and+Data+Mining+in+Pattern+Recognition&amp;rft.atitle=Vulnerability+of+Deep+Reinforcement+Learning+to+Policy+Induction+Attacks&amp;rft.volume=10358&amp;rft.pages=262-275&amp;rft.date=2017&amp;rft_id=info%3Aarxiv%2F1701.04143&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A1562290%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-319-62416-7_19&amp;rft.isbn=978-3-319-62415-0&amp;rft.aulast=Behzadan&amp;rft.aufirst=Vahid&amp;rft.au=Munir%2C+Arslan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-43">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFPieter2017" class="citation book cs1">Pieter, Huang, Sandy Papernot, Nicolas Goodfellow, Ian Duan, Yan Abbeel (2017-02-07). <a rel="nofollow" class="external text" href="http://worldcat.org/oclc/1106256905"><i>Adversarial Attacks on Neural Network Policies</i></a>. <a href="/wiki/OCLC_(identifier)" class="mw-redirect" title="OCLC (identifier)">OCLC</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/oclc/1106256905">1106256905</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Adversarial+Attacks+on+Neural+Network+Policies&amp;rft.date=2017-02-07&amp;rft_id=info%3Aoclcnum%2F1106256905&amp;rft.aulast=Pieter&amp;rft.aufirst=Huang%2C+Sandy+Papernot%2C+Nicolas+Goodfellow%2C+Ian+Duan%2C+Yan+Abbeel&amp;rft_id=http%3A%2F%2Fworldcat.org%2Foclc%2F1106256905&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-44">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFKorkmaz2022" class="citation journal cs1">Korkmaz, Ezgi (2022). <a rel="nofollow" class="external text" href="https://doi.org/10.1609%2Faaai.v36i7.20684">"Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs"</a>. <i>Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)</i>. <b>36</b> (7): 7229–7238. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1609%2Faaai.v36i7.20684">10.1609/aaai.v36i7.20684</a></span>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:245219157">245219157</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Thirty-Sixth+AAAI+Conference+on+Artificial+Intelligence+%28AAAI-22%29&amp;rft.atitle=Deep+Reinforcement+Learning+Policies+Learn+Shared+Adversarial+Features+Across+MDPs.&amp;rft.volume=36&amp;rft.issue=7&amp;rft.pages=7229-7238&amp;rft.date=2022&amp;rft_id=info%3Adoi%2F10.1609%2Faaai.v36i7.20684&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A245219157%23id-name%3DS2CID&amp;rft.aulast=Korkmaz&amp;rft.aufirst=Ezgi&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1609%252Faaai.v36i7.20684&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-45">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBerenji1994" class="citation journal cs1">Berenji, H.R. (1994). <a rel="nofollow" class="external text" href="https://ieeexplore.ieee.org/document/343737">"Fuzzy Q-learning: a new approach for fuzzy dynamic programming"</a>. <i>Proc. IEEE 3rd International Fuzzy Systems Conference</i>. Orlando, FL, USA: IEEE: 486–491. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FFUZZY.1994.343737">10.1109/FUZZY.1994.343737</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-7803-1896-X" title="Special:BookSources/0-7803-1896-X"><bdi>0-7803-1896-X</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:56694947">56694947</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proc.+IEEE+3rd+International+Fuzzy+Systems+Conference&amp;rft.atitle=Fuzzy+Q-learning%3A+a+new+approach+for+fuzzy+dynamic+programming&amp;rft.pages=486-491&amp;rft.date=1994&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A56694947%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1109%2FFUZZY.1994.343737&amp;rft.isbn=0-7803-1896-X&amp;rft.aulast=Berenji&amp;rft.aufirst=H.R.&amp;rft_id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F343737&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-46">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFVincze2017" class="citation book cs1">Vincze, David (2017). <a rel="nofollow" class="external text" href="http://users.iit.uni-miskolc.hu/~vinczed/research/vinczed_sami2017_author_draft.pdf">"Fuzzy rule interpolation and reinforcement learning"</a> <span class="cs1-format">(PDF)</span>. <i>2017 IEEE 15th International Symposium on Applied Machine Intelligence and Informatics (SAMI)</i>. IEEE. pp.&#160;173–178. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FSAMI.2017.7880298">10.1109/SAMI.2017.7880298</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-5090-5655-2" title="Special:BookSources/978-1-5090-5655-2"><bdi>978-1-5090-5655-2</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:17590120">17590120</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Fuzzy+rule+interpolation+and+reinforcement+learning&amp;rft.btitle=2017+IEEE+15th+International+Symposium+on+Applied+Machine+Intelligence+and+Informatics+%28SAMI%29&amp;rft.pages=173-178&amp;rft.pub=IEEE&amp;rft.date=2017&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A17590120%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1109%2FSAMI.2017.7880298&amp;rft.isbn=978-1-5090-5655-2&amp;rft.aulast=Vincze&amp;rft.aufirst=David&amp;rft_id=http%3A%2F%2Fusers.iit.uni-miskolc.hu%2F~vinczed%2Fresearch%2Fvinczed_sami2017_author_draft.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-47"><span class="mw-cite-backlink"><b><a href="#cite_ref-47">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFNgRussell2000" class="citation book cs1">Ng, A. Y.; Russell, S. J. (2000). <a rel="nofollow" class="external text" href="https://ai.stanford.edu/~ang/papers/icml00-irl.pdf">"Algorithms for Inverse Reinforcement Learning"</a> <span class="cs1-format">(PDF)</span>. <i>Proceeding ICML '00 Proceedings of the Seventeenth International Conference on Machine Learning</i>. pp.&#160;663–670. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/1-55860-707-2" title="Special:BookSources/1-55860-707-2"><bdi>1-55860-707-2</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Algorithms+for+Inverse+Reinforcement+Learning&amp;rft.btitle=Proceeding+ICML+%2700+Proceedings+of+the+Seventeenth+International+Conference+on+Machine+Learning&amp;rft.pages=663-670&amp;rft.date=2000&amp;rft.isbn=1-55860-707-2&amp;rft.aulast=Ng&amp;rft.aufirst=A.+Y.&amp;rft.au=Russell%2C+S.+J.&amp;rft_id=https%3A%2F%2Fai.stanford.edu%2F~ang%2Fpapers%2Ficml00-irl.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
<li id="cite_note-48"><span class="mw-cite-backlink"><b><a href="#cite_ref-48">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFGarcíaFernández2015" class="citation journal cs1">García, Javier; Fernández, Fernando (1 January 2015). <a rel="nofollow" class="external text" href="https://jmlr.org/papers/volume16/garcia15a/garcia15a.pdf">"A comprehensive survey on safe reinforcement learning"</a> <span class="cs1-format">(PDF)</span>. <i>The Journal of Machine Learning Research</i>. <b>16</b> (1): 1437–1480.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Journal+of+Machine+Learning+Research&amp;rft.atitle=A+comprehensive+survey+on+safe+reinforcement+learning&amp;rft.volume=16&amp;rft.issue=1&amp;rft.pages=1437-1480&amp;rft.date=2015-01-01&amp;rft.aulast=Garc%C3%ADa&amp;rft.aufirst=Javier&amp;rft.au=Fern%C3%A1ndez%2C+Fernando&amp;rft_id=https%3A%2F%2Fjmlr.org%2Fpapers%2Fvolume16%2Fgarcia15a%2Fgarcia15a.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></span>
</li>
</ol></div>
<h2><span class="mw-headline" id="Sources">Sources</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=25" title="Edit section: Sources">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSuttonBarto2018" class="citation book cs1"><a href="/wiki/Richard_S._Sutton" title="Richard S. Sutton">Sutton, Richard S.</a>; <a href="/wiki/Andrew_Barto" title="Andrew Barto">Barto, Andrew G.</a> (2018) [1998]. <a rel="nofollow" class="external text" href="http://incompleteideas.net/sutton/book/the-book.html"><i>Reinforcement Learning: An Introduction</i></a> (2nd&#160;ed.). MIT Press. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-262-03924-6" title="Special:BookSources/978-0-262-03924-6"><bdi>978-0-262-03924-6</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Reinforcement+Learning%3A+An+Introduction&amp;rft.edition=2nd&amp;rft.pub=MIT+Press&amp;rft.date=2018&amp;rft.isbn=978-0-262-03924-6&amp;rft.aulast=Sutton&amp;rft.aufirst=Richard+S.&amp;rft.au=Barto%2C+Andrew+G.&amp;rft_id=http%3A%2F%2Fincompleteideas.net%2Fsutton%2Fbook%2Fthe-book.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></li></ul>
<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=26" title="Edit section: Further reading">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFAnnaswamy2023" class="citation journal cs1">Annaswamy, Anuradha M. (3 May 2023). <a rel="nofollow" class="external text" href="https://doi.org/10.1146/annurev-control-062922-090153">"Adaptive Control and Intersections with Reinforcement Learning"</a>. <i>Annual Review of Control, Robotics, and Autonomous Systems</i>. <b>6</b> (1): 65–93. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1146%2Fannurev-control-062922-090153">10.1146/annurev-control-062922-090153</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/2573-5144">2573-5144</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:255702873">255702873</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annual+Review+of+Control%2C+Robotics%2C+and+Autonomous+Systems&amp;rft.atitle=Adaptive+Control+and+Intersections+with+Reinforcement+Learning&amp;rft.volume=6&amp;rft.issue=1&amp;rft.pages=65-93&amp;rft.date=2023-05-03&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A255702873%23id-name%3DS2CID&amp;rft.issn=2573-5144&amp;rft_id=info%3Adoi%2F10.1146%2Fannurev-control-062922-090153&amp;rft.aulast=Annaswamy&amp;rft.aufirst=Anuradha+M.&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1146%2Fannurev-control-062922-090153&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFAuerJakschOrtner2010" class="citation journal cs1"><a href="/wiki/Peter_Auer" title="Peter Auer">Auer, Peter</a>; Jaksch, Thomas; Ortner, Ronald (2010). <a rel="nofollow" class="external text" href="http://jmlr.csail.mit.edu/papers/v11/jaksch10a.html">"Near-optimal regret bounds for reinforcement learning"</a>. <i>Journal of Machine Learning Research</i>. <b>11</b>: 1563–1600.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.atitle=Near-optimal+regret+bounds+for+reinforcement+learning&amp;rft.volume=11&amp;rft.pages=1563-1600&amp;rft.date=2010&amp;rft.aulast=Auer&amp;rft.aufirst=Peter&amp;rft.au=Jaksch%2C+Thomas&amp;rft.au=Ortner%2C+Ronald&amp;rft_id=http%3A%2F%2Fjmlr.csail.mit.edu%2Fpapers%2Fv11%2Fjaksch10a.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBusoniuBabuskaDe_SchutterErnst2010" class="citation book cs1">Busoniu, Lucian; Babuska, Robert; <a href="/wiki/Bart_De_Schutter" title="Bart De Schutter">De Schutter, Bart</a>; Ernst, Damien (2010). <a rel="nofollow" class="external text" href="http://www.dcsc.tudelft.nl/rlbook/"><i>Reinforcement Learning and Dynamic Programming using Function Approximators</i></a>. Taylor &amp; Francis CRC Press. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4398-2108-4" title="Special:BookSources/978-1-4398-2108-4"><bdi>978-1-4398-2108-4</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Reinforcement+Learning+and+Dynamic+Programming+using+Function+Approximators&amp;rft.pub=Taylor+%26+Francis+CRC+Press&amp;rft.date=2010&amp;rft.isbn=978-1-4398-2108-4&amp;rft.aulast=Busoniu&amp;rft.aufirst=Lucian&amp;rft.au=Babuska%2C+Robert&amp;rft.au=De+Schutter%2C+Bart&amp;rft.au=Ernst%2C+Damien&amp;rft_id=http%3A%2F%2Fwww.dcsc.tudelft.nl%2Frlbook%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFFrançois-LavetHendersonIslamBellemare2018" class="citation journal cs1">François-Lavet, Vincent; Henderson, Peter; Islam, Riashat; Bellemare, Marc G.; Pineau, Joelle (2018). "An Introduction to Deep Reinforcement Learning". <i>Foundations and Trends in Machine Learning</i>. <b>11</b> (3–4): 219–354. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1811.12560">1811.12560</a></span>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2018arXiv181112560F">2018arXiv181112560F</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1561%2F2200000071">10.1561/2200000071</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:54434537">54434537</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Foundations+and+Trends+in+Machine+Learning&amp;rft.atitle=An+Introduction+to+Deep+Reinforcement+Learning&amp;rft.volume=11&amp;rft.issue=3%E2%80%934&amp;rft.pages=219-354&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1811.12560&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A54434537%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1561%2F2200000071&amp;rft_id=info%3Abibcode%2F2018arXiv181112560F&amp;rft.aulast=Fran%C3%A7ois-Lavet&amp;rft.aufirst=Vincent&amp;rft.au=Henderson%2C+Peter&amp;rft.au=Islam%2C+Riashat&amp;rft.au=Bellemare%2C+Marc+G.&amp;rft.au=Pineau%2C+Joelle&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFPowell2011" class="citation book cs1">Powell, Warren (2011). <a rel="nofollow" class="external text" href="http://www.castlelab.princeton.edu/adp.htm"><i>Approximate dynamic programming: solving the curses of dimensionality</i></a>. Wiley-Interscience.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Approximate+dynamic+programming%3A+solving+the+curses+of+dimensionality&amp;rft.pub=Wiley-Interscience&amp;rft.date=2011&amp;rft.aulast=Powell&amp;rft.aufirst=Warren&amp;rft_id=http%3A%2F%2Fwww.castlelab.princeton.edu%2Fadp.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSutton1988" class="citation journal cs1"><a href="/wiki/Richard_S._Sutton" title="Richard S. Sutton">Sutton, Richard S.</a> (1988). <a rel="nofollow" class="external text" href="http://incompleteideas.net/sutton/publications.html#TD_paper">"Learning to predict by the method of temporal differences"</a>. <i>Machine Learning</i>. <b>3</b>: 9–44. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1007%2FBF00115009">10.1007/BF00115009</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Machine+Learning&amp;rft.atitle=Learning+to+predict+by+the+method+of+temporal+differences&amp;rft.volume=3&amp;rft.pages=9-44&amp;rft.date=1988&amp;rft_id=info%3Adoi%2F10.1007%2FBF00115009&amp;rft.aulast=Sutton&amp;rft.aufirst=Richard+S.&amp;rft_id=http%3A%2F%2Fincompleteideas.net%2Fsutton%2Fpublications.html%23TD_paper&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSzitaSzepesvari2010" class="citation conference cs1">Szita, Istvan; Szepesvari, Csaba (2010). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20100714095438/http://www.icml2010.org/papers/546.pdf">"Model-based Reinforcement Learning with Nearly Tight Exploration Complexity Bounds"</a> <span class="cs1-format">(PDF)</span>. <i>ICML 2010</i>. Omnipress. pp.&#160;1031–1038. Archived from <a rel="nofollow" class="external text" href="http://www.icml2010.org/papers/546.pdf">the original</a> <span class="cs1-format">(PDF)</span> on 2010-07-14.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Model-based+Reinforcement+Learning+with+Nearly+Tight+Exploration+Complexity+Bounds&amp;rft.btitle=ICML+2010&amp;rft.pages=1031-1038&amp;rft.pub=Omnipress&amp;rft.date=2010&amp;rft.aulast=Szita&amp;rft.aufirst=Istvan&amp;rft.au=Szepesvari%2C+Csaba&amp;rft_id=http%3A%2F%2Fwww.icml2010.org%2Fpapers%2F546.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AReinforcement+learning" class="Z3988"></span></li></ul>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=27" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a rel="nofollow" class="external text" href="https://all.cs.umass.edu/rlr/">Reinforcement Learning Repository</a></li>
<li><a rel="nofollow" class="external text" href="http://spaces.facsci.ualberta.ca/rlai/">Reinforcement Learning and Artificial Intelligence</a> (RLAI, Rich Sutton's lab at the <a href="/wiki/University_of_Alberta" title="University of Alberta">University of Alberta</a>)</li>
<li><a rel="nofollow" class="external text" href="http://www-all.cs.umass.edu/">Autonomous Learning Laboratory</a> (ALL, Andrew Barto's lab at the <a href="/wiki/University_of_Massachusetts_Amherst" title="University of Massachusetts Amherst">University of Massachusetts Amherst</a>)</li>
<li><a rel="nofollow" class="external text" href="http://www.dcsc.tudelft.nl/~robotics/media.html">Real-world reinforcement learning experiments</a> <a rel="nofollow" class="external text" href="https://web.archive.org/web/20181008104644/http://www.dcsc.tudelft.nl/~robotics/media.html">Archived</a> 2018-10-08 at the <a href="/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a> at <a href="/wiki/Delft_University_of_Technology" title="Delft University of Technology">Delft University of Technology</a></li>
<li><a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=RtxI449ZjSc&amp;feature=relmfu">Stanford University Andrew Ng Lecture on Reinforcement Learning</a></li>
<li><a rel="nofollow" class="external text" href="https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html">Dissecting Reinforcement Learning</a> Series of blog post on RL with Python code</li>
<li><a rel="nofollow" class="external text" href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/">A (Long) Peek into Reinforcement Learning</a></li></ul>
<div class="navbox-styles"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><style data-mw-deduplicate="TemplateStyles:r1061467846">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}</style></div><div role="navigation" class="navbox" aria-labelledby="Differentiable_computing" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1063604349"><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Differentiable_computing" title="Template:Differentiable computing"><abbr title="View this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Differentiable_computing" title="Template talk:Differentiable computing"><abbr title="Discuss this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Differentiable_computing&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">e</abbr></a></li></ul></div><div id="Differentiable_computing" style="font-size:114%;margin:0 4em">Differentiable computing</div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Differentiable_function" title="Differentiable function">General</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><b><a href="/wiki/Differentiable_programming" title="Differentiable programming">Differentiable programming</a></b></li>
<li><a href="/wiki/Information_geometry" title="Information geometry">Information geometry</a></li>
<li><a href="/wiki/Statistical_manifold" title="Statistical manifold">Statistical manifold</a><br /></li></ul>
<ul><li><a href="/wiki/Automatic_differentiation" title="Automatic differentiation">Automatic differentiation</a></li>
<li><a href="/wiki/Neuromorphic_engineering" title="Neuromorphic engineering">Neuromorphic engineering</a></li>
<li><a href="/wiki/Pattern_recognition" title="Pattern recognition">Pattern recognition</a></li>
<li><a href="/wiki/Tensor_calculus" title="Tensor calculus">Tensor calculus</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Inductive_bias" title="Inductive bias">Inductive bias</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Gradient_descent" title="Gradient descent">Gradient descent</a>
<ul><li><a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">SGD</a></li></ul></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a>
<ul><li><a href="/wiki/Overfitting" title="Overfitting">Overfitting</a></li></ul></li>
<li><a href="/wiki/Hallucination_(artificial_intelligence)" title="Hallucination (artificial intelligence)">Hallucination</a></li>
<li><a href="/wiki/Adversarial_machine_learning" title="Adversarial machine learning">Adversary</a></li>
<li><a href="/wiki/Attention_(machine_learning)" title="Attention (machine learning)">Attention</a></li>
<li><a href="/wiki/Convolution" title="Convolution">Convolution</a></li>
<li><a href="/wiki/Loss_functions_for_classification" title="Loss functions for classification">Loss functions</a></li>
<li><a href="/wiki/Backpropagation" title="Backpropagation">Backpropagation</a></li>
<li><a href="/w/index.php?title=Normalization_of_deep_neural_networks&amp;action=edit&amp;redlink=1" class="new" title="Normalization of deep neural networks (page does not exist)">Normalization</a> (<a href="/wiki/Batch_normalization" title="Batch normalization">Batchnorm</a>)</li>
<li><a href="/wiki/Activation_function" title="Activation function">Activation</a>
<ul><li><a href="/wiki/Softmax_function" title="Softmax function">Softmax</a></li>
<li><a href="/wiki/Sigmoid_function" title="Sigmoid function">Sigmoid</a></li>
<li><a href="/wiki/Rectifier_(neural_networks)" title="Rectifier (neural networks)">Rectifier</a></li></ul></li>
<li><a href="/wiki/Regularization_(mathematics)" title="Regularization (mathematics)">Regularization</a></li>
<li><a href="/wiki/Training,_validation,_and_test_sets" class="mw-redirect" title="Training, validation, and test sets">Datasets</a>
<ul><li><a href="/wiki/Data_augmentation" title="Data augmentation">Augmentation</a></li></ul></li>
<li><a href="/wiki/Diffusion_process" title="Diffusion process">Diffusion</a></li>
<li><a href="/wiki/Autoregressive_model" title="Autoregressive model">Autoregression</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Applications</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a>
<ul><li><a href="/wiki/In-context_learning_(natural_language_processing)" class="mw-redirect" title="In-context learning (natural language processing)">In-context learning</a></li></ul></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a>
<ul><li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li></ul></li>
<li><a href="/wiki/Computational_science" title="Computational science">Scientific computing</a></li>
<li><a href="/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial Intelligence</a></li>
<li><a href="/wiki/Language_model" title="Language model">Language model</a>
<ul><li><a href="/wiki/Large_language_model" title="Large language model">Large language model</a></li></ul></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Hardware</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Graphcore" title="Graphcore">IPU</a></li>
<li><a href="/wiki/Tensor_Processing_Unit" title="Tensor Processing Unit">TPU</a></li>
<li><a href="/wiki/Vision_processing_unit" title="Vision processing unit">VPU</a></li>
<li><a href="/wiki/Memristor" title="Memristor">Memristor</a></li>
<li><a href="/wiki/SpiNNaker" title="SpiNNaker">SpiNNaker</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Software libraries</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/TensorFlow" title="TensorFlow">TensorFlow</a></li>
<li><a href="/wiki/PyTorch" title="PyTorch">PyTorch</a></li>
<li><a href="/wiki/Keras" title="Keras">Keras</a></li>
<li><a href="/wiki/Theano_(software)" title="Theano (software)">Theano</a></li>
<li><a href="/wiki/Google_JAX" title="Google JAX">JAX</a></li>
<li><a href="/wiki/LangChain" title="LangChain">LangChain</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Implementations</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%">Audio–visual</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AlexNet" title="AlexNet">AlexNet</a></li>
<li><a href="/wiki/WaveNet" title="WaveNet">WaveNet</a></li>
<li><a href="/wiki/Human_image_synthesis" title="Human image synthesis">Human image synthesis</a></li>
<li><a href="/wiki/Handwriting_recognition" title="Handwriting recognition">HWR</a></li>
<li><a href="/wiki/Optical_character_recognition" title="Optical character recognition">OCR</a></li>
<li><a href="/wiki/Deep_learning_speech_synthesis" title="Deep learning speech synthesis">Speech synthesis</a></li>
<li><a href="/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a></li>
<li><a href="/wiki/Facial_recognition_system" title="Facial recognition system">Facial recognition</a></li>
<li><a href="/wiki/AlphaFold" title="AlphaFold">AlphaFold</a></li>
<li><a href="/wiki/DALL-E" title="DALL-E">DALL-E</a></li>
<li><a href="/wiki/Midjourney" title="Midjourney">Midjourney</a></li>
<li><a href="/wiki/Stable_Diffusion" title="Stable Diffusion">Stable Diffusion</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Verbal</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Word2vec" title="Word2vec">Word2vec</a></li>
<li><a href="/wiki/Seq2seq" title="Seq2seq">Seq2seq</a></li>
<li><a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a></li>
<li><a href="/wiki/LaMDA" title="LaMDA">LaMDA</a>
<ul><li><a href="/wiki/Bard_(chatbot)" title="Bard (chatbot)">Bard</a></li></ul></li>
<li><a href="/wiki/Neural_machine_translation" title="Neural machine translation">NMT</a></li>
<li><a href="/wiki/Project_Debater" title="Project Debater">Project Debater</a></li>
<li><a href="/wiki/IBM_Watson" title="IBM Watson">IBM Watson</a></li>
<li><a href="/wiki/GPT-2" title="GPT-2">GPT-2</a></li>
<li><a href="/wiki/GPT-3" title="GPT-3">GPT-3</a></li>
<li><a href="/wiki/ChatGPT" title="ChatGPT">ChatGPT</a></li>
<li><a href="/wiki/GPT-4" title="GPT-4">GPT-4</a></li>
<li><a href="/wiki/GPT-J" title="GPT-J">GPT-J</a></li>
<li><a href="/wiki/Chinchilla_AI" title="Chinchilla AI">Chinchilla AI</a></li>
<li><a href="/wiki/PaLM" title="PaLM">PaLM</a></li>
<li><a href="/wiki/BLOOM_(language_model)" title="BLOOM (language model)">BLOOM</a></li>
<li><a href="/wiki/LLaMA" title="LLaMA">LLaMA</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Decisional</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AlphaGo" title="AlphaGo">AlphaGo</a></li>
<li><a href="/wiki/AlphaZero" title="AlphaZero">AlphaZero</a></li>
<li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/OpenAI_Five" title="OpenAI Five">OpenAI Five</a></li>
<li><a href="/wiki/Self-driving_car" title="Self-driving car">Self-driving car</a></li>
<li><a href="/wiki/MuZero" title="MuZero">MuZero</a></li>
<li><a href="/wiki/Action_selection" title="Action selection">Action selection</a>
<ul><li><a href="/wiki/Auto-GPT" title="Auto-GPT">Auto-GPT</a></li></ul></li>
<li><a href="/wiki/Robot_control" title="Robot control">Robot control</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Yoshua Bengio</a></li>
<li><a href="/wiki/Alex_Graves_(computer_scientist)" title="Alex Graves (computer scientist)">Alex Graves</a></li>
<li><a href="/wiki/Ian_Goodfellow" title="Ian Goodfellow">Ian Goodfellow</a></li>
<li><a href="/wiki/Stephen_Grossberg" title="Stephen Grossberg">Stephen Grossberg</a></li>
<li><a href="/wiki/Demis_Hassabis" title="Demis Hassabis">Demis Hassabis</a></li>
<li><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a></li>
<li><a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a></li>
<li><a href="/wiki/Fei-Fei_Li" title="Fei-Fei Li">Fei-Fei Li</a></li>
<li><a href="/wiki/Andrew_Ng" title="Andrew Ng">Andrew Ng</a></li>
<li><a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a></li>
<li><a href="/wiki/David_Silver_(computer_scientist)" title="David Silver (computer scientist)">David Silver</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Organizations</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Anthropic" title="Anthropic">Anthropic</a></li>
<li><a href="/wiki/EleutherAI" title="EleutherAI">EleutherAI</a></li>
<li><a href="/wiki/Google_DeepMind" title="Google DeepMind">Google DeepMind</a></li>
<li><a href="/wiki/Hugging_Face" title="Hugging Face">Hugging Face</a></li>
<li><a href="/wiki/OpenAI" title="OpenAI">OpenAI</a></li>
<li><a href="/wiki/Meta_AI" title="Meta AI">Meta AI</a></li>
<li><a href="/wiki/Mila_(research_institute)" title="Mila (research institute)">Mila</a></li>
<li><a href="/wiki/MIT_Computer_Science_and_Artificial_Intelligence_Laboratory" title="MIT Computer Science and Artificial Intelligence Laboratory">MIT CSAIL</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Architectures</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Neural_Turing_machine" title="Neural Turing machine">Neural Turing machine</a></li>
<li><a href="/wiki/Differentiable_neural_computer" title="Differentiable neural computer">Differentiable neural computer</a></li>
<li><a href="/wiki/Transformer_(machine_learning_model)" title="Transformer (machine learning model)">Transformer</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Recurrent neural network (RNN)</a></li>
<li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">Long short-term memory (LSTM)</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">Gated recurrent unit (GRU)</a></li>
<li><a href="/wiki/Echo_state_network" title="Echo state network">Echo state network</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron (MLP)</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a></li>
<li><a href="/wiki/Residual_network" class="mw-redirect" title="Residual network">Residual network</a></li>
<li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Variational_autoencoder" title="Variational autoencoder">Variational autoencoder (VAE)</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">Generative adversarial network (GAN)</a></li>
<li><a href="/wiki/Graph_neural_network" title="Graph neural network">Graph neural network</a></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<ul><li><span class="noviewer" typeof="mw:File"><a href="/wiki/File:Symbol_portal_class.svg" class="mw-file-description" title="Portal"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/16px-Symbol_portal_class.svg.png" decoding="async" width="16" height="16" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/23px-Symbol_portal_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/31px-Symbol_portal_class.svg.png 2x" data-file-width="180" data-file-height="185" /></a></span> Portals
<ul><li><a href="/wiki/Portal:Computer_programming" title="Portal:Computer programming">Computer programming</a></li>
<li><a href="/wiki/Portal:Technology" title="Portal:Technology">Technology</a></li></ul></li>
<li><span class="noviewer" typeof="mw:File"><span title="Category"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png" decoding="async" width="16" height="16" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/23px-Symbol_category_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/31px-Symbol_category_class.svg.png 2x" data-file-width="180" data-file-height="185" /></span></span> Categories
<ul><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li>
<li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></li></ul>
</div></td></tr></tbody></table></div>
<div class="navbox-styles"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1061467846"></div><div role="navigation" class="navbox" aria-labelledby="Computer_science" style="padding:3px"><table class="nowraplinks hlist mw-collapsible mw-collapsed navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="3"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1063604349"><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Computer_science" title="Template:Computer science"><abbr title="View this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Computer_science" title="Template talk:Computer science"><abbr title="Discuss this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Computer_science&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">e</abbr></a></li></ul></div><div id="Computer_science" style="font-size:114%;margin:0 4em"><a href="/wiki/Computer_science" title="Computer science">Computer science</a></div></th></tr><tr><td class="navbox-abovebelow" colspan="3"><div>Note: This template roughly follows the 2012 <a href="/wiki/ACM_Computing_Classification_System" title="ACM Computing Classification System">ACM Computing Classification System</a>.</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Computer_hardware" title="Computer hardware">Hardware</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Printed_circuit_board" title="Printed circuit board">Printed circuit board</a></li>
<li><a href="/wiki/Peripheral" title="Peripheral">Peripheral</a></li>
<li><a href="/wiki/Integrated_circuit" title="Integrated circuit">Integrated circuit</a></li>
<li><a href="/wiki/Very_Large_Scale_Integration" title="Very Large Scale Integration">Very Large Scale Integration</a></li>
<li><a href="/wiki/System_on_a_chip" title="System on a chip">Systems on Chip (SoCs)</a></li>
<li><a href="/wiki/Green_computing" title="Green computing">Energy consumption (Green computing)</a></li>
<li><a href="/wiki/Electronic_design_automation" title="Electronic design automation">Electronic design automation</a></li>
<li><a href="/wiki/Hardware_acceleration" title="Hardware acceleration">Hardware acceleration</a></li></ul>
</div></td><td class="noviewer navbox-image" rowspan="17" style="width:1px;padding:0 0 0 2px"><div><span typeof="mw:File"><a href="/wiki/File:Computer_Retro.svg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/5/51/Computer_Retro.svg/50px-Computer_Retro.svg.png" decoding="async" width="50" height="50" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/5/51/Computer_Retro.svg/75px-Computer_Retro.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/51/Computer_Retro.svg/100px-Computer_Retro.svg.png 2x" data-file-width="512" data-file-height="512" /></a></span></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Computer systems organization</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Computer_architecture" title="Computer architecture">Computer architecture</a></li>
<li><a href="/wiki/Embedded_system" title="Embedded system">Embedded system</a></li>
<li><a href="/wiki/Real-time_computing" title="Real-time computing">Real-time computing</a></li>
<li><a href="/wiki/Dependability" title="Dependability">Dependability</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Computer_network" title="Computer network">Networks</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Network_architecture" title="Network architecture">Network architecture</a></li>
<li><a href="/wiki/Network_protocol" class="mw-redirect" title="Network protocol">Network protocol</a></li>
<li><a href="/wiki/Networking_hardware" title="Networking hardware">Network components</a></li>
<li><a href="/wiki/Network_scheduler" title="Network scheduler">Network scheduler</a></li>
<li><a href="/wiki/Network_performance" title="Network performance">Network performance evaluation</a></li>
<li><a href="/wiki/Network_service" title="Network service">Network service</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Software organization</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Interpreter_(computing)" title="Interpreter (computing)">Interpreter</a></li>
<li><a href="/wiki/Middleware" title="Middleware">Middleware</a></li>
<li><a href="/wiki/Virtual_machine" title="Virtual machine">Virtual machine</a></li>
<li><a href="/wiki/Operating_system" title="Operating system">Operating system</a></li>
<li><a href="/wiki/Software_quality" title="Software quality">Software quality</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Programming_language_theory" title="Programming language theory">Software notations</a> and <a href="/wiki/Programming_tool" title="Programming tool">tools</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Programming_paradigm" title="Programming paradigm">Programming paradigm</a></li>
<li><a href="/wiki/Programming_language" title="Programming language">Programming language</a></li>
<li><a href="/wiki/Compiler_construction" class="mw-redirect" title="Compiler construction">Compiler</a></li>
<li><a href="/wiki/Domain-specific_language" title="Domain-specific language">Domain-specific language</a></li>
<li><a href="/wiki/Modeling_language" title="Modeling language">Modeling language</a></li>
<li><a href="/wiki/Software_framework" title="Software framework">Software framework</a></li>
<li><a href="/wiki/Integrated_development_environment" title="Integrated development environment">Integrated development environment</a></li>
<li><a href="/wiki/Software_configuration_management" title="Software configuration management">Software configuration management</a></li>
<li><a href="/wiki/Library_(computing)" title="Library (computing)">Software library</a></li>
<li><a href="/wiki/Software_repository" title="Software repository">Software repository</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Software_development" title="Software development">Software development</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Control_variable_(programming)" class="mw-redirect" title="Control variable (programming)">Control variable</a></li>
<li><a href="/wiki/Software_development_process" title="Software development process">Software development process</a></li>
<li><a href="/wiki/Requirements_analysis" title="Requirements analysis">Requirements analysis</a></li>
<li><a href="/wiki/Software_design" title="Software design">Software design</a></li>
<li><a href="/wiki/Software_construction" title="Software construction">Software construction</a></li>
<li><a href="/wiki/Software_deployment" title="Software deployment">Software deployment</a></li>
<li><a href="/wiki/Software_engineering" title="Software engineering">Software engineering</a></li>
<li><a href="/wiki/Software_maintenance" title="Software maintenance">Software maintenance</a></li>
<li><a href="/wiki/Programming_team" title="Programming team">Programming team</a></li>
<li><a href="/wiki/Open-source_software" title="Open-source software">Open-source model</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Theory_of_computation" title="Theory of computation">Theory of computation</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Model_of_computation" title="Model of computation">Model of computation</a></li>
<li><a href="/wiki/Formal_language" title="Formal language">Formal language</a></li>
<li><a href="/wiki/Automata_theory" title="Automata theory">Automata theory</a></li>
<li><a href="/wiki/Computability_theory" title="Computability theory">Computability theory</a></li>
<li><a href="/wiki/Computational_complexity_theory" title="Computational complexity theory">Computational complexity theory</a></li>
<li><a href="/wiki/Logic_in_computer_science" title="Logic in computer science">Logic</a></li>
<li><a href="/wiki/Semantics_(computer_science)" title="Semantics (computer science)">Semantics</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Algorithm" title="Algorithm">Algorithms</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Algorithm_design" class="mw-redirect" title="Algorithm design">Algorithm design</a></li>
<li><a href="/wiki/Analysis_of_algorithms" title="Analysis of algorithms">Analysis of algorithms</a></li>
<li><a href="/wiki/Algorithmic_efficiency" title="Algorithmic efficiency">Algorithmic efficiency</a></li>
<li><a href="/wiki/Randomized_algorithm" title="Randomized algorithm">Randomized algorithm</a></li>
<li><a href="/wiki/Computational_geometry" title="Computational geometry">Computational geometry</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Mathematics of computing</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Discrete_mathematics" title="Discrete mathematics">Discrete mathematics</a></li>
<li><a href="/wiki/Probability" title="Probability">Probability</a></li>
<li><a href="/wiki/Statistics" title="Statistics">Statistics</a></li>
<li><a href="/wiki/Mathematical_software" title="Mathematical software">Mathematical software</a></li>
<li><a href="/wiki/Information_theory" title="Information theory">Information theory</a></li>
<li><a href="/wiki/Mathematical_analysis" title="Mathematical analysis">Mathematical analysis</a></li>
<li><a href="/wiki/Numerical_analysis" title="Numerical analysis">Numerical analysis</a></li>
<li><a href="/wiki/Theoretical_computer_science" title="Theoretical computer science">Theoretical computer science</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Information_system" title="Information system">Information systems</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Database" title="Database">Database management system</a></li>
<li><a href="/wiki/Computer_data_storage" title="Computer data storage">Information storage systems</a></li>
<li><a href="/wiki/Enterprise_information_system" title="Enterprise information system">Enterprise information system</a></li>
<li><a href="/wiki/Social_software" title="Social software">Social information systems</a></li>
<li><a href="/wiki/Geographic_information_system" title="Geographic information system">Geographic information system</a></li>
<li><a href="/wiki/Decision_support_system" title="Decision support system">Decision support system</a></li>
<li><a href="/wiki/Process_control" class="mw-redirect" title="Process control">Process control system</a></li>
<li><a href="/wiki/Multimedia_database" title="Multimedia database">Multimedia information system</a></li>
<li><a href="/wiki/Data_mining" title="Data mining">Data mining</a></li>
<li><a href="/wiki/Digital_library" title="Digital library">Digital library</a></li>
<li><a href="/wiki/Computing_platform" title="Computing platform">Computing platform</a></li>
<li><a href="/wiki/Digital_marketing" title="Digital marketing">Digital marketing</a></li>
<li><a href="/wiki/World_Wide_Web" title="World Wide Web">World Wide Web</a></li>
<li><a href="/wiki/Information_retrieval" title="Information retrieval">Information retrieval</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Computer_security" title="Computer security">Security</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Cryptography" title="Cryptography">Cryptography</a></li>
<li><a href="/wiki/Formal_methods" title="Formal methods">Formal methods</a></li>
<li><a href="/wiki/Security_service_(telecommunication)" title="Security service (telecommunication)">Security services</a></li>
<li><a href="/wiki/Intrusion_detection_system" title="Intrusion detection system">Intrusion detection system</a></li>
<li><a href="/wiki/Computer_security_compromised_by_hardware_failure" title="Computer security compromised by hardware failure">Hardware security</a></li>
<li><a href="/wiki/Network_security" title="Network security">Network security</a></li>
<li><a href="/wiki/Information_security" title="Information security">Information security</a></li>
<li><a href="/wiki/Application_security" title="Application security">Application security</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Human%E2%80%93computer_interaction" title="Human–computer interaction">Human–computer interaction</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Interaction_design" title="Interaction design">Interaction design</a></li>
<li><a href="/wiki/Social_computing" title="Social computing">Social computing</a></li>
<li><a href="/wiki/Ubiquitous_computing" title="Ubiquitous computing">Ubiquitous computing</a></li>
<li><a href="/wiki/Visualization_(graphics)" title="Visualization (graphics)">Visualization</a></li>
<li><a href="/wiki/Computer_accessibility" title="Computer accessibility">Accessibility</a></li>
<li><a href="/wiki/Synthography" title="Synthography">Synthography</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Concurrency_(computer_science)" title="Concurrency (computer science)">Concurrency</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Concurrent_computing" title="Concurrent computing">Concurrent computing</a></li>
<li><a href="/wiki/Parallel_computing" title="Parallel computing">Parallel computing</a></li>
<li><a href="/wiki/Distributed_computing" title="Distributed computing">Distributed computing</a></li>
<li><a href="/wiki/Multithreading_(computer_architecture)" title="Multithreading (computer architecture)">Multithreading</a></li>
<li><a href="/wiki/Multiprocessing" title="Multiprocessing">Multiprocessing</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial intelligence</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Natural_language_processing" title="Natural language processing">Natural language processing</a></li>
<li><a href="/wiki/Knowledge_representation_and_reasoning" title="Knowledge representation and reasoning">Knowledge representation and reasoning</a></li>
<li><a href="/wiki/Computer_vision" title="Computer vision">Computer vision</a></li>
<li><a href="/wiki/Automated_planning_and_scheduling" title="Automated planning and scheduling">Automated planning and scheduling</a></li>
<li><a href="/wiki/Mathematical_optimization" title="Mathematical optimization">Search methodology</a></li>
<li><a href="/wiki/Control_theory" title="Control theory">Control method</a></li>
<li><a href="/wiki/Philosophy_of_artificial_intelligence" title="Philosophy of artificial intelligence">Philosophy of artificial intelligence</a></li>
<li><a href="/wiki/Distributed_artificial_intelligence" title="Distributed artificial intelligence">Distributed artificial intelligence</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a class="mw-selflink selflink">Reinforcement learning</a></li>
<li><a href="/wiki/Multi-task_learning" title="Multi-task learning">Multi-task learning</a></li>
<li><a href="/wiki/Cross-validation_(statistics)" title="Cross-validation (statistics)">Cross-validation</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Computer_graphics" title="Computer graphics">Graphics</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Computer_animation" title="Computer animation">Animation</a></li>
<li><a href="/wiki/Rendering_(computer_graphics)" title="Rendering (computer graphics)">Rendering</a></li>
<li><a href="/wiki/Photograph_manipulation" title="Photograph manipulation">Photograph manipulation</a></li>
<li><a href="/wiki/Graphics_processing_unit" title="Graphics processing unit">Graphics processing unit</a></li>
<li><a href="/wiki/Mixed_reality" title="Mixed reality">Mixed reality</a></li>
<li><a href="/wiki/Virtual_reality" title="Virtual reality">Virtual reality</a></li>
<li><a href="/wiki/Image_compression" title="Image compression">Image compression</a></li>
<li><a href="/wiki/Solid_modeling" title="Solid modeling">Solid modeling</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Applied computing</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/E-commerce" title="E-commerce">E-commerce</a></li>
<li><a href="/wiki/Enterprise_software" title="Enterprise software">Enterprise software</a></li>
<li><a href="/wiki/Computational_mathematics" title="Computational mathematics">Computational mathematics</a></li>
<li><a href="/wiki/Computational_physics" title="Computational physics">Computational physics</a></li>
<li><a href="/wiki/Computational_chemistry" title="Computational chemistry">Computational chemistry</a></li>
<li><a href="/wiki/Computational_biology" title="Computational biology">Computational biology</a></li>
<li><a href="/wiki/Computational_social_science" title="Computational social science">Computational social science</a></li>
<li><a href="/wiki/Computational_engineering" title="Computational engineering">Computational engineering</a></li>
<li><a href="/wiki/Health_informatics" title="Health informatics">Computational healthcare</a></li>
<li><a href="/wiki/Digital_art" title="Digital art">Digital art</a></li>
<li><a href="/wiki/Electronic_publishing" title="Electronic publishing">Electronic publishing</a></li>
<li><a href="/wiki/Cyberwarfare" title="Cyberwarfare">Cyberwarfare</a></li>
<li><a href="/wiki/Electronic_voting" title="Electronic voting">Electronic voting</a></li>
<li><a href="/wiki/Video_game" title="Video game">Video games</a></li>
<li><a href="/wiki/Word_processor" title="Word processor">Word processing</a></li>
<li><a href="/wiki/Operations_research" title="Operations research">Operations research</a></li>
<li><a href="/wiki/Educational_technology" title="Educational technology">Educational technology</a></li>
<li><a href="/wiki/Document_management_system" title="Document management system">Document management</a></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="3"><div>
<ul><li><span class="noviewer" typeof="mw:File"><span title="Category"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png" decoding="async" width="16" height="16" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/23px-Symbol_category_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/31px-Symbol_category_class.svg.png 2x" data-file-width="180" data-file-height="185" /></span></span> <b><a href="/wiki/Category:Computer_science" title="Category:Computer science">Category</a></b></li>
<li><span class="noviewer" typeof="mw:File"><span title="Outline"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/41/Global_thinking.svg/10px-Global_thinking.svg.png" decoding="async" width="10" height="16" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/41/Global_thinking.svg/15px-Global_thinking.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/41/Global_thinking.svg/21px-Global_thinking.svg.png 2x" data-file-width="130" data-file-height="200" /></span></span> <b><a href="/wiki/Outline_of_computer_science" title="Outline of computer science">Outline</a></b></li>
<li><span class="noviewer" typeof="mw:File"><span title="WikiProject"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/37/People_icon.svg/16px-People_icon.svg.png" decoding="async" width="16" height="16" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/37/People_icon.svg/24px-People_icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/37/People_icon.svg/32px-People_icon.svg.png 2x" data-file-width="100" data-file-height="100" /></span></span><b><a href="/wiki/Wikipedia:WikiProject_Computer_science" title="Wikipedia:WikiProject Computer science">WikiProject</a></b></li>
<li><span class="noviewer" typeof="mw:File"><span title="Commons page"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/12px-Commons-logo.svg.png" decoding="async" width="12" height="16" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/18px-Commons-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/24px-Commons-logo.svg.png 2x" data-file-width="1024" data-file-height="1376" /></span></span> <b><a href="https://commons.wikimedia.org/wiki/Category:Computer_science" class="extiw" title="commons:Category:Computer science">Commons</a></b></li></ul>
</div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw2303
Cached time: 20230712230802
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 3.092 seconds
Real time usage: 4.382 seconds
Preprocessor visited node count: 4786/1000000
Post‐expand include size: 214125/2097152 bytes
Template argument size: 3055/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 6/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 181771/5000000 bytes
Lua time usage: 0.450/10.000 seconds
Lua memory usage: 10786895/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00% 4136.975      1 -total
 88.94% 3679.337      1 Template:Reflist
  3.70%  152.882     21 Template:Cite_journal
  1.88%   77.646      3 Template:Navbox
  1.75%   72.331      1 Template:Machine_learning
  1.69%   69.772      1 Template:Sidebar_with_collapsible_lists
  1.67%   68.919      1 Template:Differentiable_computing
  1.29%   53.465     13 Template:Cite_book
  1.25%   51.812      2 Template:Sfn
  1.22%   50.364      1 Template:Short_description
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:66294-0!canonical and timestamp 20230712230757 and revision id 1161565586. Rendering was triggered because: page-view
 -->
</div><!--esi <esi:include src="/esitest-fa8a495983347898/content" /> --><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript>
<div class="printfooter" data-nosnippet="">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;oldid=1161565586">https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;oldid=1161565586</a>"</div></div>
					<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Reinforcement_learning" title="Category:Reinforcement learning">Reinforcement learning</a></li><li><a href="/wiki/Category:Markov_models" title="Category:Markov models">Markov models</a></li><li><a href="/wiki/Category:Belief_revision" title="Category:Belief revision">Belief revision</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:CS1_maint:_url-status" title="Category:CS1 maint: url-status">CS1 maint: url-status</a></li><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Short_description_matches_Wikidata" title="Category:Short description matches Wikidata">Short description matches Wikidata</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_January_2020" title="Category:Wikipedia articles needing clarification from January 2020">Wikipedia articles needing clarification from January 2020</a></li><li><a href="/wiki/Category:Articles_needing_additional_references_from_October_2022" title="Category:Articles needing additional references from October 2022">Articles needing additional references from October 2022</a></li><li><a href="/wiki/Category:All_articles_needing_additional_references" title="Category:All articles needing additional references">All articles needing additional references</a></li><li><a href="/wiki/Category:Webarchive_template_wayback_links" title="Category:Webarchive template wayback links">Webarchive template wayback links</a></li></ul></div></div>
				</div>
			</main>
			
		</div>
		<div class="mw-footer-container">
			
<footer id="footer" class="mw-footer" role="contentinfo" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 23 June 2023, at 15:15<span class="anonymous-show">&#160;(UTC)</span>.</li>
	<li id="footer-info-copyright">Text is available under the <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike License 4.0</a><a rel="license" href="//creativecommons.org/licenses/by-sa/4.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/Wikipedia:About">About Wikipedia</a></li>
	<li id="footer-places-disclaimers"><a href="/wiki/Wikipedia:General_disclaimer">Disclaimers</a></li>
	<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
	<li id="footer-places-wm-codeofconduct"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Universal_Code_of_Conduct">Code of Conduct</a></li>
	<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	<li id="footer-places-developers"><a href="https://developer.wikimedia.org">Developers</a></li>
	<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
	<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement">Cookie statement</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation" loading="lazy" /></a></li>
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/footer/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88" height="31" loading="lazy"></a></li>
</ul>

</footer>

		</div>
	</div> 
</div> 
<div class='vector-settings'>
	
<button
	id=""
	class="cdx-button cdx-button--icon-only vector-limited-width-toggle"
	><span class="vector-icon mw-ui-icon-fullScreen mw-ui-icon-wikimedia-fullScreen"></span>

<span>Toggle limited content width</span>

</button>

</div>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgHostname":"mw2303","wgBackendResponseTime":4515,"wgPageParseReport":{"limitreport":{"cputime":"3.092","walltime":"4.382","ppvisitednodes":{"value":4786,"limit":1000000},"postexpandincludesize":{"value":214125,"limit":2097152},"templateargumentsize":{"value":3055,"limit":2097152},"expansiondepth":{"value":16,"limit":100},"expensivefunctioncount":{"value":6,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":181771,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00% 4136.975      1 -total"," 88.94% 3679.337      1 Template:Reflist","  3.70%  152.882     21 Template:Cite_journal","  1.88%   77.646      3 Template:Navbox","  1.75%   72.331      1 Template:Machine_learning","  1.69%   69.772      1 Template:Sidebar_with_collapsible_lists","  1.67%   68.919      1 Template:Differentiable_computing","  1.29%   53.465     13 Template:Cite_book","  1.25%   51.812      2 Template:Sfn","  1.22%   50.364      1 Template:Short_description"]},"scribunto":{"limitreport-timeusage":{"value":"0.450","limit":"10.000"},"limitreport-memusage":{"value":10786895,"limit":52428800},"limitreport-logs":"anchor_id_list = table#1 {\n    [\"CITEREFAnnaswamy2023\"] = 1,\n    [\"CITEREFAuerJakschOrtner2010\"] = 1,\n    [\"CITEREFBarto2013\"] = 1,\n    [\"CITEREFBehzadanMunir2017\"] = 1,\n    [\"CITEREFBerenji1994\"] = 1,\n    [\"CITEREFBradtkeBarto1996\"] = 1,\n    [\"CITEREFBurnetasKatehakis1997\"] = 1,\n    [\"CITEREFBusoniuBabuskaDe_SchutterErnst2010\"] = 1,\n    [\"CITEREFDabériusGranatKarlsson2020\"] = 1,\n    [\"CITEREFDeisenrothNeumannPeters2013\"] = 1,\n    [\"CITEREFDeySinghWangMcDonald-Maier2020\"] = 1,\n    [\"CITEREFFrancois-Lavet2018\"] = 1,\n    [\"CITEREFFrançois-LavetHendersonIslamBellemare2018\"] = 1,\n    [\"CITEREFGarcíaFernández2015\"] = 1,\n    [\"CITEREFGeorge_KarimpanalBouffanais2019\"] = 1,\n    [\"CITEREFGoodfellowShlensSzegedy2015\"] = 1,\n    [\"CITEREFGosavi2003\"] = 1,\n    [\"CITEREFJuliani2016\"] = 1,\n    [\"CITEREFKaelblingLittmanMoore1996\"] = 1,\n    [\"CITEREFKaplanOudeyer2004\"] = 1,\n    [\"CITEREFKlyubinPolaniNehaniv2008\"] = 1,\n    [\"CITEREFKorkmaz2022\"] = 1,\n    [\"CITEREFKulkarniNarasimhanSaeediTenenbaum2016\"] = 1,\n    [\"CITEREFLeeSeoJung2012\"] = 1,\n    [\"CITEREFLin1992\"] = 1,\n    [\"CITEREFMatzliachBen-GalKagan2022\"] = 1,\n    [\"CITEREFMnih2015\"] = 1,\n    [\"CITEREFNgRussell2000\"] = 1,\n    [\"CITEREFPetersVijayakumarSchaal2003\"] = 1,\n    [\"CITEREFPieter2017\"] = 1,\n    [\"CITEREFPowell2011\"] = 1,\n    [\"CITEREFQuested\"] = 1,\n    [\"CITEREFRiveretGao2019\"] = 1,\n    [\"CITEREFRussellNorvig2010\"] = 1,\n    [\"CITEREFSoucek1992\"] = 1,\n    [\"CITEREFSutton1984\"] = 1,\n    [\"CITEREFSutton1988\"] = 1,\n    [\"CITEREFSutton1990\"] = 1,\n    [\"CITEREFSuttonBarto2018\"] = 1,\n    [\"CITEREFSzitaSzepesvari2010\"] = 1,\n    [\"CITEREFTokicPalm2011\"] = 1,\n    [\"CITEREFVincze2017\"] = 1,\n    [\"CITEREFWatkins1989\"] = 1,\n    [\"CITEREFWilliams1987\"] = 1,\n    [\"CITEREFWilliams2020\"] = 1,\n    [\"CITEREFYamagataMcConvilleSantos-Rodriguez2021\"] = 1,\n    [\"CITEREFvan_HasseltHesselAslanides2019\"] = 1,\n    [\"CITEREFvan_Otterlo,_M.Wiering,_M.2012\"] = 1,\n}\ntemplate_list = table#1 {\n    [\"Citation\"] = 2,\n    [\"Cite arXiv\"] = 1,\n    [\"Cite book\"] = 13,\n    [\"Cite conference\"] = 6,\n    [\"Cite journal\"] = 21,\n    [\"Cite thesis\"] = 2,\n    [\"Cite web\"] = 6,\n    [\"Clarify\"] = 1,\n    [\"Computer science\"] = 1,\n    [\"Differentiable computing\"] = 1,\n    [\"Div col\"] = 1,\n    [\"Div col end\"] = 1,\n    [\"For\"] = 1,\n    [\"Machine learning\"] = 1,\n    [\"Main\"] = 1,\n    [\"More citations needed section\"] = 1,\n    [\"Mvar\"] = 3,\n    [\"Reflist\"] = 1,\n    [\"Rp\"] = 2,\n    [\"See also\"] = 1,\n    [\"Sfn\"] = 2,\n    [\"Short description\"] = 1,\n    [\"Toclimit\"] = 1,\n    [\"Webarchive\"] = 1,\n}\narticle_whitelist = table#1 {\n}\n"},"cachereport":{"origin":"mw2303","timestamp":"20230712230802","ttl":1814400,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Reinforcement learning","url":"https:\/\/en.wikipedia.org\/wiki\/Reinforcement_learning","sameAs":"http:\/\/www.wikidata.org\/entity\/Q830687","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q830687","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2002-07-31T13:24:55Z","dateModified":"2023-06-23T15:15:11Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Kernel_Machine.svg","headline":"field of machine learning"}</script><script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Reinforcement learning","url":"https:\/\/en.wikipedia.org\/wiki\/Reinforcement_learning","sameAs":"http:\/\/www.wikidata.org\/entity\/Q830687","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q830687","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2002-07-31T13:24:55Z","dateModified":"2023-06-23T15:15:11Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Kernel_Machine.svg","headline":"field of machine learning"}</script>
</body>
</html>